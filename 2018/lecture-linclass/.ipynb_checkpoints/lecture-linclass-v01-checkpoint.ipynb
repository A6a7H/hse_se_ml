{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"img/logo_hse_black.jpg\"></center>\n",
    "\n",
    "<h1><center>Data Analysis</center></h1>\n",
    "<h3><center>Andrey Shetakov (<a href=\"mailto:avshestakov@hse.ru\">avshestakov@hse.ru</a>)</center></h3>\n",
    "<hr>\n",
    "<h2><center>Linear classification. Logistic Regression<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup></center></h2>\n",
    "\n",
    "\n",
    "\n",
    "<sup id=\"fn1\">1. Some materials are taken from <a href=\"http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5_%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B_%D1%80%D0%B0%D1%81%D0%BF%D0%BE%D0%B7%D0%BD%D0%B0%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F_%D0%BE%D0%B1%D1%80%D0%B0%D0%B7%D0%BE%D0%B2_%28%D0%BA%D1%83%D1%80%D1%81_%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D0%B9%2C_%D0%92.%D0%92.%D0%9A%D0%B8%D1%82%D0%BE%D0%B2%29\">machine learning course of Victor Kitov</a></sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('seaborn-talk')\n",
    "plt.rcParams['figure.figsize'] = (12,8)\n",
    "\n",
    "# Для кириллицы на графиках\n",
    "font = {'family': 'Verdana',\n",
    "        'weight': 'normal'}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "try:\n",
    "    from ipywidgets import interact, IntSlider, fixed, FloatSlider\n",
    "except ImportError:\n",
    "    print u'Так надо'\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's recall previous lecture\n",
    "\n",
    "* Linear regression\n",
    "    * linear dependence between target features and predictors\n",
    "    * predictors themselves can be non-linear\n",
    "* Solution can be found \n",
    "    * analytically \n",
    "    * with gradient descent\n",
    "* Various loss functions options\n",
    "* Robust setting\n",
    "* Regularization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Svm_separating_hyperplanes_%28SVG%29.svg/512px-Svm_separating_hyperplanes_%28SVG%29.svg.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Analytical geometry reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reminder\n",
    "\n",
    "* $a=[a^{1},...a^{D}]^{T},\\,b=[b^{1},...b^{D}]^{T}$\n",
    "* Scalar product $\\langle a,b\\rangle=a^{T}b=\\sum_{d=1}^{D}a_{d}b_{b}$ \n",
    "* $a\\perp b$ means that $\\langle a,b\\rangle=0$\n",
    "* Norm $\\left\\lVert a\\right\\rVert =\\sqrt{\\langle a,a\\rangle}$\n",
    "* Distance $\\rho(a,b)=\\left\\lVert a-b\\right\\rVert =\\sqrt{\\langle a-b,a-b\\rangle}$\n",
    "\n",
    "\n",
    "<center><img src=\"img/projection.png\"></center>\n",
    "\n",
    "* $p=\\langle a,\\frac{b}{\\left\\lVert b\\right\\rVert }\\rangle$\n",
    "* $\\left|p\\right|=\\left|a,\\frac{b}{\\left\\lVert b\\right\\rVert }\\right|$-\n",
    "unsigned projection length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Orthogonal vector to hyperplane\n",
    "\n",
    "### Theorem 1\n",
    "Vector $w$ is orthogonal to hyperplane $w^{T}x+w_{0}=0$\n",
    "\n",
    "**Proof:**\n",
    "Consider arbitrary $x_{A},x_{B}\\in\\{x:\\,w^{T}x+w_{0}=0\\}$:\n",
    "$$\n",
    "\\begin{align}\n",
    "w^{T}x_{A}+w_{0}=0 \\quad \\text{   (1)}\\\\\n",
    "w^{T}x_{B}+w_{0}=0 \\quad \\text{   (2)}\n",
    "\\end{align}\n",
    "$$\n",
    "By substracting (2) from (1), obtain $w^{T}(x_{A}-x_{B})=0$,\n",
    "so $w$ is orthogonal to hyperplane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Distance from point to hyperplane\n",
    "\n",
    "### Theorem 2\n",
    "Distance from point $x$ to hyperplane\n",
    "$w^{T}x+w_{0}=0$ is equal to $\\frac{w^{T}x+w_{0}}{\\left\\lVert w_{0}\\right\\rVert }$.\n",
    "\n",
    "**Proof:** Project $x$ on the hyperplane, let the projection\n",
    "be $p$ and complement $h=x-p$, orthogonal to hyperplane. Then\n",
    "$$\n",
    "x=p+h\n",
    "$$\n",
    "\n",
    "Since $p$ lies on the hyperplane, \n",
    "$$\n",
    "w^{T}p+w_{0}=0\n",
    "$$\n",
    "\n",
    "Since $h$ is orthogonal to hyperplane and according to theorem 1\n",
    "$$\n",
    "h=r\\frac{w}{\\left\\lVert w\\right\\rVert },\\,r\\in\\mathbb{R}\\text{ - distance to hyperplane}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Distance from point to hyperplane\n",
    "\n",
    "$$\n",
    "x=p+r\\frac{w}{\\left\\lVert w\\right\\rVert }\n",
    "$$\n",
    "\n",
    "After multiplication by $w$ and addition of $w_{0}$:\n",
    "$$\n",
    "w^{T}x+w_{0}=w^{T}p+w_{0}+r\\frac{w^{T}w}{\\left\\lVert w\\right\\rVert }=r\\left\\lVert w\\right\\rVert \n",
    "$$\n",
    "\n",
    "because $w^{T}p+w_{0}=0$ and $\\left\\lVert w\\right\\rVert =\\sqrt{w^{T}w}$.\n",
    "So we get, that \n",
    "$$\n",
    "r=\\frac{w^{T}x+w_{0}}{\\left\\lVert w\\right\\rVert }\n",
    "$$\n",
    "\n",
    "**Comments:**\n",
    "* From one side of hyperplane $r>0\\Leftrightarrow w^{T}x+w_{0}>0$\n",
    "* From the other side $r<0\\Leftrightarrow w^{T}x+w_{0}<0$. \n",
    "* Distance from hyperplane to origin 0 is $\\frac{w_{0}}{\\left\\lVert w\\right\\rVert }$.\n",
    "So $w_{0}$ accounts for hyperplane offset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='img\\Linear discriminant function 2.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Binary linear classifier geometric interpretation\n",
    "\n",
    "Binary linear classifier:\n",
    "$$\n",
    "\\widehat{y}(x)= sign\\left(w^{T}x+w_{0}\\right)\n",
    "$$\n",
    "\n",
    "divides feature space by hyperplane $w^{T}x+w_{0}=0$.\n",
    "\n",
    "* Confidence of decision is proportional to distance to hyperplane $\\frac{\\left|w^{T}x+w_{0}\\right|}{\\left\\lVert w\\right\\rVert }$.\n",
    "* $w^{T}x+w_{0}$ is the confidence that class is positive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Multiclass classification with binary classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multiclass classification with binary classifiers\n",
    "* Task - make $C$-class classification using many binary classifiers.\n",
    "* Approaches:\n",
    "\n",
    "    * **one-versus-all**\n",
    "        * for each $c=1,2,...C$ train binary classifier on all objects and output $\\mathbb{I}[y_{n}=c]$, \n",
    "        * assign class, getting the highest score in resulting $C$ classifiers.\n",
    "\n",
    "    * **one-versus-one**\n",
    "        * for each $i,j\\in[1,2,...C],$ $i\\ne j$ learn on objects with $y_{n}\\in\\{i,j\\}$ with output $y_{n}$\n",
    "        * assign class, getting the highest score in resulting $C(C-1)/2$ classifiers.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Binary linear classifier\n",
    "\n",
    "* For two classes $y\\in\\{+1,-1\\}$ classifier becomes\n",
    "$$\n",
    "\\widehat{y}(x)=\\begin{cases}\n",
    "+1, & w_{+1}^{T}x+w_{+1,0}>w_{-1}^{T}x+w_{-1,0}\\\\\n",
    "-1 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "* This decision rule is equivalent to \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\widehat{y}(x) & =sign(w_{+1}^{T}x+w_{+1,0}-w_{-1}^{T}x+w_{-1,0})=\\\\\n",
    " & =sign\\left(\\left(w_{+1}^{T}-w_{-1}^{T}\\right)x+\\left(w_{+1,0}-w_{-1,0}\\right)\\right)\\\\\n",
    " & =sign\\left(w^{T}x+w_{0}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "for $w=w_{+1}-w_{-1},\\,w_{0}=w_{+1,0}-w_{-1,0}$.\n",
    "* Decision boundary $w^{T}x+w_{0}=0$ is linear.\n",
    "* Multiclass case can be solved using multiple binary classifiers with one-vs-all, one-vs-one\n",
    "    * can you imagine faulty situation with those approaches for linear classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='img/one versus all ambiguity.png'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='img/one versus one ambiguity.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear classifier\n",
    "\n",
    "* Classification among classes 1,2,...C. \n",
    "* Use $C$ discriminant functions $g_{c}(x)=w_{c}^{T}x+w_{c0}$\n",
    "* Decision rule:\n",
    "$$\n",
    "\\widehat{y}(x)=\\arg\\max_{c}g_{c}(x)\n",
    "$$\n",
    "* Decision boundary between classes $y=i$ and $y=j$ is linear:\n",
    "$$\n",
    "\\left(w_{i}-w_{j}\\right)^{T}x+\\left(w_{i0}-w_{j0}\\right)=0\n",
    "$$\n",
    "* Decision regions are convex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Margin of binary linear classifier\n",
    "\n",
    "$$\n",
    "M(x,y) =y\\left(w^{T}x+w_{0}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "* Margin = score, how well classifier predicted true $y$ for object $x$.\n",
    "* $M(x,y|w)>0$ <=> object $x$ is correctly classified as $y$\n",
    "    * signs of $w^{T}x+w_{0}$ and $y$ coincide\n",
    "* $|M(x,y|w)|=\\left|w^{T}x+w_{0}\\right|$ - confidence of decision\n",
    "    * proportional to distance from $x$ to hyperplane $w^{T}x+w_{0}=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Margin\n",
    "\n",
    "<center><img src=\"img/Different margin objects.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Weights optimization\n",
    "* Margin = score, how well classifier predicted true $y$ for object $x$.\n",
    "* Task: select such $w$ to increase $M(x_{n},y_{n}|w)$ for all $n$. \n",
    "* Formalization:\n",
    "$$\n",
    "\\frac{1}{N}\\sum_{n=1}^{N}\\mathcal{L}\\left(M(x_{n},y_{n}|w)\\right)\\to\\min_{w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Misclassification rate optimization\n",
    "\n",
    "* Misclassification rate optimization:\n",
    "$$\n",
    "\\frac{1}{N}\\sum_{n=1}^{N}\\mathbb{I}[M(x_{n},y_{n}|w)<0]\\to\\min_{w}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "is not recommended:\n",
    "* discontinious function, can't use numerical optimization!\n",
    "* continous margin is more informative than binary error indicator.\\pause\n",
    "\n",
    "* If we select loss function $\\mathcal{L}(M)$ such that $\\mathbb{I}[M]\\le\\mathcal{L}(M)$\n",
    "then we can optimize upper bound on misclassification rate:\n",
    "$$\n",
    "\\begin{gathered}\\begin{gathered}\\text{MISCLASSIFICATION RATE}\\end{gathered}\n",
    "=\\frac{1}{N}\\sum_{n=1}^{N}\\mathbb{I}[M(x_{n},y_{n}|w)<0]\\\\\n",
    "\\le\\frac{1}{N}\\sum_{n=1}^{N}\\mathcal{L}(M(x_{n},y_{n}|w))=L(w)\n",
    "\\end{gathered}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Common loss functions\n",
    "\n",
    "<center><img src=\"img/Error indicator approximations.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Same story as in linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Same story as in linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='img/overfitting.jpg'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regularization\n",
    "\n",
    "* Insert additional requirement for regularizer $R(\\beta)$ to be small:\n",
    "$$\n",
    "\\sum_{n=1}^{N}\\mathcal{L}\\left(M(x_{n},y_{n}|w\\right)+\\lambda R(\\beta)\\to\\min_{\\beta}\n",
    "$$\n",
    "* $\\lambda>0$ - hyperparameter.\n",
    "* $R(\\beta)$ penalizes complexity of models.\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "R(\\beta)=||\\beta||_{1} & L_{1}\\text{ regularization}\\\\\n",
    "R(\\beta)=||\\beta||_{2}^{2} & L_{2}\\text{ regularization}\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## $L_{1}$ regularization\n",
    "\n",
    "\n",
    "* $||w||_{1}$ regularizer should do feature selection.\n",
    "\n",
    "* Consider \n",
    "$$\n",
    "L(w)=\\sum_{n=1}^{N}\\mathcal{L}\\left(M(x_{n},y_{n}|w)\\right)+\\lambda\\sum_{d=1}^{D}|w_{d}|\n",
    "$$\n",
    "\n",
    "* And gradient updates\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_{i}}L(w)=\\sum_{n=1}^{N}\\frac{\\partial}{\\partial w_{i}}\\mathcal{L}\\left(M(x_{n},y_{n}|w)\\right)+\\lambda sign (w_{i})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda sign (w_{i})\\nrightarrow0\\text{ when }w_{i}\\to0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## $L_{2}$ regularization\n",
    "\n",
    "$$\n",
    "L(w)=\\sum_{n=1}^{N}\\mathcal{L}\\left(M(x_{n},y_{n}|w)\\right)+\\lambda\\sum_{d=1}^{D}w_{d}^{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_{i}}L(w)=\\sum_{n=1}^{N}\\frac{\\partial}{\\partial w_{i}}\\mathcal{L}\\left(M(x_{n},y_{n}|w)\\right)+2\\lambda w_{i}\n",
    "$$\n",
    "$$\n",
    "2\\lambda w_{i}\\to0\\text{ when }w_{i}\\to0\n",
    "$$\n",
    "\n",
    "* Strength of regularization $\\to0$ as weights $\\to0$.\n",
    "* So $L_{2}$ regularization will not set weights exactly to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the foolowing objects\n",
    "\n",
    "| x1 | x2 |\n",
    "|---|---|\n",
    "| 0 | 1 |\n",
    "| 1 | 0 |\n",
    "| 1 | 1 |\n",
    "| 2 | 2 |\n",
    "| 2 | 3 |\n",
    "| 3 | 2 |\n",
    "\n",
    "Find class prediction if $(w_0 = -0.3 , w_1 = 0.1, w_2 = 0.1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Binary classification\n",
    "\n",
    "* Linear classifier:\n",
    "$$\n",
    "score(\\omega_{1}|x)=w^{T}x + w_0 = g(x)\n",
    "$$\n",
    "* +relationship between score and class probability is assumed:\n",
    "$$\n",
    "p(\\omega_{1}|x)=\\sigma(w^{T}x + w_0)\n",
    "$$\n",
    "\n",
    "where $\\sigma(z)=\\frac{1}{1+e^{-z}}$ - sigmoid function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def demo_sigmoid():\n",
    "    z = np.linspace(-10, 10, 100)\n",
    "\n",
    "    y = sigmoid(z)\n",
    "    plt.plot(z, y)\n",
    "    plt.xlabel('$z$')\n",
    "    plt.ylabel('$\\sigma(z)$')\n",
    "    \n",
    "def sigmoid(z): \n",
    "    return 1./(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "demo_sigmoid() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Binary classification: estimation\n",
    "\n",
    "Using the property $1-\\sigma(z)=\\sigma(-z)$ obtain that \n",
    "$$\n",
    "p(y=+1|x)=\\sigma(w^{T}x+w_0)\\Longrightarrow p(y=-1|x)=\\sigma(-w^{T}x - w_0)\n",
    "$$\n",
    "\n",
    "So for $y\\in\\{+1,-1\\}$\n",
    "$$\n",
    "p(y|x)=\\sigma(y(\\langle w,x\\rangle + w_0)) \n",
    "$$\n",
    "\n",
    "Therefore ML estimation can be written as:\n",
    "$$\n",
    "\\prod_{i=1}^{N}\\sigma( y_{i}(\\langle w,x_{i}\\rangle + w_0))\\to\\max_{w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loss function for 2-class logistic regression\n",
    "\n",
    "For binary classification $p(y|x)=\\sigma(y(\\langle w,x\\rangle + w_0))$\n",
    "\n",
    "Estimation with ML:\n",
    "\n",
    "$$\n",
    "\\prod_{i=1}^{n}\\sigma(y_{i}(\\langle w,x_{i}\\rangle + w_0)) = \\prod_{i=1}^{n}\\sigma(y_{i}g(x_{i})) = \\to\\max_{w}\n",
    "$$\n",
    "\n",
    "which is equivalent to \n",
    "$$\n",
    "\\sum_{i}^{n}\\ln(1+e^{-y_{i}g(x_{i})})\\to\\min_{w}\n",
    "$$\n",
    "\n",
    "It follows that logistic regression is linear discriminant\n",
    "estimated with loss function $\\mathcal{L}(M)=\\ln(1+e^{-M})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/Logistic loss function.png\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Another formulation\n",
    "Lets present Likelihood function in another form\n",
    "$$ \\mathcal{L}(w) = \\prod_i^n p(y=+1|x)^{[y^{(i)} = +1]} p(y=-1|x)^{[y^{(i)} = -1]} \\rightarrow \\max_w$$\n",
    "$$ -\\ln{\\mathcal{L}(w)} = - \\sum_i^n [y^{(i)} = +1]\\cdot\\ln{\\sigma(w^{T}x+w_0))} + {[y^{(i)} = -1]}\\cdot\\ln{(1-\\sigma(w^{T}x+w_0))} \\rightarrow \\min_w$$\n",
    "$$L(w) = \\log{\\mathcal{L}(w)} \\rightarrow \\min_w $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='img/prob.png' width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multiple classes\n",
    "\n",
    "Multiple class classification:\n",
    "$$\n",
    "\\begin{cases}\n",
    "score(\\omega_{1}|x)=w_{1}^{T}x + w_{0,1} \\\\\n",
    "score(\\omega_{2}|x)=w_{2}^{T}x + w_{0,2}\\\\\n",
    "\\cdots\\\\\n",
    "score(\\omega_{C}|x)=w_{C}^{T}x + + w_{0,C}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "+relationship between score and class probability is assumed:\n",
    "\n",
    "$$\n",
    "p(\\omega_{c}|x)=softmax(\\omega_c|W, x)=\\frac{exp(w_{c}^{T}x + w_{0,c})}{\\sum_{i}exp(w_{i}^{T}x + w_{0,i})}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Estimation with ML:**\n",
    "$$\n",
    "\\prod_{n=1}^{N}\\prod_{c=1}^{C} softmax(\\omega_c|W, x_i)^{[y_i = w_c]}\n",
    "$$\n",
    "\n",
    "Which would lead us to cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "* Linear classifier - classifier with linear discriminant functions.\n",
    "* Binary linear classifier: $\\widehat{y}(x)=sign(w^{T}x+w_{0})$.\n",
    "* Perceptron, logistic, SVM - linear classifiers estimated with different loss functions.\n",
    "* Weights are selected to minimize total loss on margins.\n",
    "* Gradient descent iteratively optimizes $L(w)$ in the direction of maximum descent.\n",
    "* Stochastic gradient descent approximates $\\nabla_{w}L$ by averaging\n",
    "gradients over small subset of objects.\n",
    "* Regularization gives smooth control over model complexity.\n",
    "* $L_{1}$ regularization automatically selects features."
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/1f8c4751e12938961e423759861e6e5a"
  },
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "gist": {
   "data": {
    "description": "CloudMail/hse-da-course/raw/lecture-intro/lecture-intro-v01.ipynb",
    "public": false
   },
   "id": "1f8c4751e12938961e423759861e6e5a"
  },
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "livereveal": {
   "theme": "serif",
   "transition": "concave",
   "width": "1024px"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "513px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "toc_position": {
   "height": "32px",
   "left": "9px",
   "right": "1379px",
   "top": "33px",
   "width": "212px"
  },
  "widgets": {
   "state": {
    "54e80d57f79b4bfc934a2b84cf5fe7ba": {
     "views": [
      {
       "cell_index": 47
      }
     ]
    },
    "5fb17a3592634a4fba98446dacd6db43": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "6f6f6ce7b81743308b92966f225862a8": {
     "views": [
      {
       "cell_index": 34
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
