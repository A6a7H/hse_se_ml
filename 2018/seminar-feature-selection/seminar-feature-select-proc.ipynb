{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/logo_hse_black.jpg\"></center>\n",
    "\n",
    "<h1><center>Data Analysis</center></h1>\n",
    "<h2><center>Seminar: Feature Engineering and Feature Selection </center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/feature_selection.png' width=700></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is a process of selecting a subset of original features with minimum loss of information related to final task (classification, regression, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why feature selection?\n",
    "\n",
    "* increase predictive accuracy of classifier\n",
    "* improve optimization stability by removing multicollinearity\n",
    "* increase computational efficiency\n",
    "* reduce cost of future data collection\n",
    "* make classifier more interpretable\n",
    "\n",
    "**Not always necessary step**\n",
    "* some methods have implicit feature selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Approaches\n",
    "* Unsupervised methods\n",
    "    * don't use target feature\n",
    "* Filter methdos\n",
    "    * use target feature\n",
    "    * consider each feature independently\n",
    "* Wrapper methods\n",
    "    * uses model quality\n",
    "* Embedded methdos\n",
    "    * embedded inside model\n",
    "\n",
    "### \"Unsupervised\" methods\n",
    "\n",
    "* Determine feature importance regardless of target feature\n",
    "* Your options?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter methods \n",
    "* Features are considered independently of each other\n",
    "* Individual predictive power is measures\n",
    "\n",
    "** Basically **\n",
    "* Order features with respect to feature importances $I(f)$:\n",
    "$$\n",
    "I(f_{1})> I(f_{2})> \\dots\\ge I(f_{D})\n",
    "$$\n",
    "* Select top $m$\n",
    "$$\n",
    "\\hat{F}=\\{f_{1},f_{2},...f_{m}\\}\n",
    "$$\n",
    "\n",
    "\n",
    "* Simple to implement\n",
    "* Usually quite fast\n",
    "* When features are correlated, it will take many redundant features\n",
    "\n",
    "\n",
    "#### Examples\n",
    "\n",
    "* Correlation\n",
    "    * Which kind of relationship does correlation measure?\n",
    "* Mutual Information\n",
    "    * Entropy of variable $Y$: $H(Y) = - \\sum_y p(y)\\ln p(y)$\n",
    "    * Conditional entropy of $Y$ after observing $X$: $H(y|x) = - \\sum_x p(x) \\sum_y p(y|x)\\ln p(y|x) $\n",
    "    * Mutial information: $$MI(Y, X) = \\sum_{x,y} p(x,y) \\ln\\left[\\frac{p(x,y)}{p(x)p(y)}\\right]$$\n",
    "        * Mutual information measures how much $X$ and $Y$ share information between each other\n",
    "        * $MI(Y,X) = H(Y) - H(Y|X)$\n",
    "    * Normalized mutual information: $NMI(X,Y) = \\frac{MI(Y,X)}{H(Y)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/mi.png' width=300></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic = pd.read_csv('data/titanic.csv')\n",
    "df_titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.crosstab(df_titanic.Survived, df_titanic.Sex, normalize=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P = pd.crosstab(df_titanic.Survived, df_titanic.Sex, normalize=True).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px = P.sum(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "py = P.sum(axis=0)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.dot(py.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mutual_info(x, y):\n",
    "    '''\n",
    "    Method should take arrays of values x and y and calculate their mutual information\n",
    "    '''\n",
    "    Pxy = pd.crosstab(x, y, normalize=True).values\n",
    "    Px  = Pxy.sum(axis=1)[:, np.newaxis]\n",
    "    Py = Pxy.sum(axis=0)[:, np.newaxis]\n",
    "    PxPy = Px.dot(Py.T)\n",
    "    MI = (Pxy*np.log(Pxy/(PxPy))).sum()\n",
    "    \n",
    "    return MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_info(df_titanic.Sex, df_titanic.Survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper methods\n",
    "* Selecting suboptimal subset of features\n",
    "* Could be slow\n",
    "* Examples: \n",
    "    * Recursive Feature Elimination\n",
    "        * Consider full set of features\n",
    "        * Fit a model, measure feature importance (based on model)\n",
    "        * Remove least important feature(s)\n",
    "        * Repeat\n",
    "    * [Boruta Algorithm](https://www.google.ru/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=0ahUKEwif5biy-fTWAhXkYJoKHbdxCLAQFgg2MAE&url=https%3A%2F%2Fwww.jstatsoft.org%2Farticle%2Fview%2Fv036i11%2Fv36i11.pdf&usg=AOvVaw3tyiHN0BCe2fkkAA6xEVDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_otp():\n",
    "    # Just data load and some preprocessing\n",
    "    features = pd.read_csv('data/descr.txt', sep='\\t', encoding='cp1251', names=['feature', 'descr'])\n",
    "    \n",
    "    features = features.iloc[3:]\n",
    "    feature_names = features.iloc[:, 0].values\n",
    "    \n",
    "    df_data_x = pd.read_csv('data/data_x.csv', sep=';', header=None, names=feature_names)\n",
    "    df_data_x.loc[:, 'PREVIOUS_CARD_NUM_UTILIZED'] = df_data_x.PREVIOUS_CARD_NUM_UTILIZED.fillna(0)\n",
    "    \n",
    "    features.loc[:, 'uniq_vals'] = df_data_x.apply(lambda c: c.nunique(), axis=0).values\n",
    "    \n",
    "    features = features.reset_index(drop=True)\n",
    "    \n",
    "    df_data_y = pd.read_csv('data/data_y.csv', sep=';', names=['active'])\n",
    "    \n",
    "    idx = np.where(df_data_x.dtypes == 'object')[0]\n",
    "\n",
    "    for i in idx:\n",
    "        df_data_x.iloc[:, i] = df_data_x.iloc[:, i].str.replace(',', '.').astype('float')\n",
    "        \n",
    "    df_data = df_data_x.join(df_data_y)\n",
    "    \n",
    "    return df_data, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_data, features = load_otp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df_data.iloc[:, :-1].values\n",
    "y = df_data.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(5, shuffle=True, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('imputer', Imputer(strategy='median')),\n",
    "    ('scaller', StandardScaler()),\n",
    "    ('clf', RFECV(LogisticRegression(), \n",
    "                  verbose=2, cv=cv, scoring='roc_auc', n_jobs=1))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfe = pipeline.steps[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = rfe.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.feature.values[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,51), rfe.grid_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedded methods\n",
    "* Feature selection process in included in the model\n",
    "* Examples:\n",
    "    * Decision Trees\n",
    "    * Linear model with L1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually dataset is not well formend once the task is provided and you have to \n",
    "* preprocess initial features\n",
    "* make features, based on several sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sberbank Data Science Contest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this ds channenge one have to predict cardholder's gender based on his/her transactional activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets take a look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender = pd.read_csv('data/customers_gender_train.csv')\n",
    "df_gender.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions = pd.read_csv('data/transactions.csv.gz')\n",
    "df_transactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MCC](https://ru.wikipedia.org/wiki/Merchant_Category_Code) codes and transaction type dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = pd.read_csv('data/tr_types.csv', sep=';', encoding='utf8')\n",
    "df_tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mcc = pd.read_csv('data/tr_mcc_codes.csv', sep=';', encoding='utf8')\n",
    "df_mcc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we see strange timestamps and amounts. You can perform some analytical excersises to understand try timestemps and amount values\n",
    "\n",
    "Some magic operations will be executed in the cells below. If you wish, you can try to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import Timestamp, DateOffset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preproc_transactions(df_transactions):\n",
    "    sec_per_day = 86400\n",
    "    sec_per_hour = 3600\n",
    "    \n",
    "    start_date = 1420070400 - 154 * sec_per_day - 3 * sec_per_hour\n",
    "    \n",
    "    df_transactions.loc[:, 'day'] = df_transactions.tr_datetime\\\n",
    "                                               .str.split(' ')\\\n",
    "                                               .str.get(0)\\\n",
    "                                               .astype(int)\n",
    "    df_transactions.loc[:, 'time_raw'] = df_transactions.tr_datetime\\\n",
    "                                                    .str.split(' ')\\\n",
    "                                                    .str.get(1)\n",
    "\n",
    "    # set temp dt\n",
    "    df_transactions.loc[:, 'dt_temp'] = pd.to_datetime(df_transactions.loc[:, 'time_raw'], \n",
    "                                                    format='%H:%M:%S')\\\n",
    "                                        + DateOffset(years=115)\n",
    "    \n",
    "    df_transactions = df_transactions.assign(dt = lambda x: x.dt_temp.astype(np.int64) // 10**9\n",
    "                                             + (x.day - 153) * sec_per_day)\\\n",
    "                                     .assign(weekday = lambda x: ((x.day + 4) % 7 + 1))\n",
    "        \n",
    "    df_transactions.loc[:, 'datetime'] = pd.to_datetime(df_transactions.dt, unit='s')\n",
    "    df_transactions.loc[:, 'date'] = df_transactions.loc[:, 'datetime'].dt.strftime('%Y-%m-%d')\n",
    "    df_transactions.loc[:, 'hour'] = df_transactions.loc[:, 'datetime'].dt.strftime('%H')\n",
    "    \n",
    "    df_transactions = df_transactions.drop(['dt_temp', 'time_raw', 'tr_datetime'], axis=1)\n",
    "    \n",
    "    df_transactions.loc[:, 'amount'] = np.round(df_transactions.loc[:, 'amount']/(np.pi**np.exp(1)))\n",
    "            \n",
    "    return df_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions = df_transactions.pipe(preproc_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets make new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propose your ideas:\n",
    "1. Amounts in mcc_codes\n",
    "2. Timestamp features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And implement them!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mcc =\\\n",
    "df_transactions\\\n",
    ".query('amount >= 0')\\\n",
    ".pivot_table(index='customer_id', fill_value=0.0,\n",
    "             aggfunc='sum',\n",
    "             columns='mcc_code', \n",
    "             values='amount')\\\n",
    ".rename_axis(lambda x: 'mcc_{}'.format(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekday = \\\n",
    "df_transactions.pivot_table(index='customer_id', \n",
    "                            values='amount',\n",
    "                            aggfunc='count', \n",
    "                            columns='weekday')\\\n",
    "               .rename_axis(lambda x: 'weekday_{}'.format(x), axis=1)\n",
    "\n",
    "total = df_weekday.sum(axis=1)\n",
    "\n",
    "df_weekday = ((df_weekday.T)/total.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_features(df_input, df_transactions):\n",
    "    \n",
    "    # Mcc features\n",
    "    df_mcc =\\\n",
    "    df_transactions\\\n",
    "    .query('amount >= 0')\\\n",
    "    .pivot_table(index='customer_id', fill_value=0.0,\n",
    "                 aggfunc='sum',\n",
    "                 columns='mcc_code', \n",
    "                 values='amount')\\\n",
    "    .rename_axis(lambda x: 'mcc_{}'.format(x), axis=1)\n",
    "    \n",
    "    # Weekday features\n",
    "    df_weekday = \\\n",
    "    df_transactions.pivot_table(index='customer_id', fill_value=0.0,\n",
    "                                values='amount',\n",
    "                                aggfunc='count', \n",
    "                                columns='weekday')\\\n",
    "                   .rename_axis(lambda x: 'weekday_{}'.format(x), axis=1)\n",
    "\n",
    "    total = df_weekday.sum(axis=1)\n",
    "\n",
    "    df_weekday = ((df_weekday.T)/total.T).T\n",
    "    \n",
    "    df_features = df_input.join(df_mcc, how='left', on='customer_id')\\\n",
    "                       .join(df_weekday, how='left', on='customer_id')\\\n",
    "                       .fillna(0.0)\n",
    "            \n",
    "    df_features = df_features.drop(['customer_id'], axis=1)\n",
    "    \n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_features = df_gender.pipe(gen_features, df_transactions)\n",
    "\n",
    "label = 'gender'\n",
    "idx_features = df_features.columns != label\n",
    "\n",
    "X = df_features.loc[:, idx_features].values\n",
    "y = df_features.loc[:, ~idx_features].values.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple pipeline with Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import lognorm as sp_lognorm\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RND_SEED = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('clf', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'scaler__with_centering': [False, True],\n",
    "    'clf__penalty': ['l1', 'l2'],\n",
    "    'clf__random_state': [RND_SEED],\n",
    "    'clf__C': sp_lognorm(4)\n",
    "}\n",
    "cv = StratifiedKFold(5, shuffle=True, random_state=123)\n",
    "random_searcher = RandomizedSearchCV(model, param_grid, n_iter=100, \n",
    "                                     random_state=RND_SEED,\n",
    "                                     scoring='roc_auc', \n",
    "                                     n_jobs=-1, cv=cv, \n",
    "                                     verbose=2)\n",
    "\n",
    "random_searcher.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Label encoding\n",
    "* One-hot encoding \n",
    "* Independent separate dataset\n",
    "* Conjunction of two (or more) categorical features = new categorical feature\n",
    "* Target Encoding\n",
    "    * Consider feature $f$ and category `cat_i` in it. Raw values can be encoded via target feature in the following way:\n",
    "$$ cat\\_i\\_meantarget = \\frac{nrows\\_i\\cdot mean\\_i(target) + \\alpha \\cdot global\\_mean}{nrows + \\alpha} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 10\n",
    "cv = StratifiedKFold(5, shuffle=True, random_state=123)\n",
    "\n",
    "def target_encoding(df, col_name, target_name, cv=StratifiedKFold(), alpha=10):\n",
    "    '''\n",
    "    Function takes dataframe, categorical feature name and target feature name \n",
    "    and computes mean target encoding for that feature using cross-validation\n",
    "    '''\n",
    "    \n",
    "    ## Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about test set in this approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbour Features\n",
    "\n",
    "Sometimes features, based on nearest neighour of an object can be helpful.<br/>\n",
    "So you set various $k$ and calculate features like:\n",
    "* Fraction of objects of every class (basically kNN prediction)\n",
    "* Same label streak: the largest number N, such that N nearest neighbors have the same label\n",
    "* Minimum (normalized) distance to objects of each class\n",
    "* Mean distance to neighbors of each class\n",
    "* Mean feature values of neighbours of each class\n",
    "* ...\n",
    "\n",
    "\n",
    "Find more [here](https://github.com/hse-aml/competitive-data-science/blob/master/Programming%20assignment%2C%20week%204:%20KNN%20features/compute_KNN_features.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "216px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
