{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/logo_hse_black.jpg\"></center>\n",
    "\n",
    "<h1><center>Data Analysis</center></h1>\n",
    "<h2><center>Seminar: Neural Networks 1</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Почему *обратное* распространение?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обозначения и затравочка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим такой пример нейронной сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/nn.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $x_i$ - входной слой. Сюда подаются признаки объектов. $i$ - это индексы признаков объекта\n",
    "* $z_j$, $z_k$ - скрытые слои.\n",
    "* $z_m$ - выходной слой. Тут выводятся результаты вычислений сети. $j$, $k$, $m$ - индексы нейронов в этих слоях\n",
    "* $w_{\\cdot,\\cdot}$ - веса, связывающие нейроны соседних слоев\n",
    "\n",
    "Дополнительно введем следующие обозначения:\n",
    "* $net^{(1)}_j = net_j = \\sum_iw_{ij}x_i$ \n",
    "* $net^{(2)}_k = net_k =  \\sum_jw_{jk}z_j$ \n",
    "* $net^{(3)}_m = net_m \\sum_kw_{km}z_k$\n",
    "\n",
    "*Верхние индексы мы пока опустим, но в части про векторизацию к ним вернемся*\n",
    "\n",
    "\n",
    "Так же:\n",
    "* $z_j = f^{(h)}(net_j)$ \n",
    "* $z_k = f^{(k)}(net_k)$\n",
    "* $z_m = f^{(r)}(net_m)$\n",
    "\n",
    "где $f(\\cdot)$ - некоторая функция активации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам так же понадобится функция потерь $L(W)$, которая зависит от весов $w_{\\cdot, \\cdot}$.\n",
    "\n",
    "Если бы мы хотели обучаться с помощью простого градиентного спуска, то нам нужно было на каждом шаге расчитывать\n",
    "частные производные по всем весам: \n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_{ij}}$, $\\frac{\\partial L}{\\partial w_{jk}}$, $\\frac{\\partial L}{\\partial w_{km}}$\n",
    "\n",
    "Достаточно выписать вычисления нейронной сети в виде формулы, чтобы понять, что \"такой футбол нам не нужен\" и хоть и вычислять эти производные можно, это займет много времени. Есть ли способ как-то ускорить расчет производных?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Идея метода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поступим следующим образом - случайно инициализируем начальные веса и пропустим один объект $(x(t), y(t))$ по нейронной сети с прямом направлении (forward pass). Получим ответ нейронной сети на одном из выходов $z_m = \\hat{y}(t)$.\n",
    "\n",
    "Посчитаем $\\frac{\\partial L}{\\partial w_{km}}$, воспользовавшись правилами дифференцирования сложных функций:\n",
    "\n",
    "$$\n",
    "\\begin{align*} \n",
    "\\frac{\\partial L}{\\partial w_{km}} &=  \\underline{\\frac{\\partial L}{\\partial z_m} \\cdot \\frac{\\partial z_m}{\\partial net_m}} \\cdot \\frac{\\partial net_m}{\\partial w_{km}} \\\\ \n",
    " &= \\underline{\\frac{\\partial L}{\\partial z_m} \\cdot f^{(3)`}(net_m)} \\cdot z_k\n",
    "\\end{align*}\n",
    "$$\n",
    "*Первые два члена выделены неслучайно!*\n",
    "\n",
    "Okey, пока ничего особенного. Переходим на слой назад:\n",
    "$$\n",
    "\\begin{align} \n",
    "\\frac{\\partial L}{\\partial w_{jk}} &=  (\\frac{\\partial L}{\\partial z_k}) & \\cdot & \\frac{\\partial z_k}{\\partial net_k} & \\cdot & \\frac{\\partial net_k}{\\partial w_{jk}} \\\\ \n",
    " &= \\left(\\sum_m\\underline{\\frac{\\partial L}{\\partial z_m} \\cdot \\frac{\\partial z_m}{\\partial net_m}} \\cdot \\frac{\\partial net_m}{\\partial z_k}\\right) & \\cdot & f^{(2)`}(net_k) & \\cdot & z_j \\\\\n",
    " &= \\left(\\sum_m\\underline{\\frac{\\partial L}{\\partial z_m} \\cdot f^{(3)`}(net_m)} \\cdot w_{km}\\right) & \\cdot & f^{(2)`}(net_k) & \\cdot & z_j \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "*Надеюсь форматирование вас не запутало*\n",
    "\n",
    "Что же мы видим? Некоторую часть выражения для $\\frac{\\partial L}{\\partial w_{jk}}$ мы уже считали на слое выше! Причем эта часть отвечает именно за производную ошибки по выходу сети! Более того, эта часть входит с весом $ w_{km}$, который как раз связывает эти слои. Таким образом, можно сказать, что ошибка начинает *распростроняться в обратном направлении* по слоям, пропоционально весам между слоями. \n",
    "\n",
    "Отсюда и название метода.\n",
    "\n",
    "Eсли мы продолжим вывод для весов $w_{ij}$, то так же заметим там часть, ранее посчитанную на этапе с весами $w_{jk}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обобщение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Становится ясно, что можно как-то записать вычисление в \"рекурсивном\" виде между слоями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть один объект прошел по сети с результатом $\\hat{y}(t)$.\n",
    "\n",
    "Рассмотрим выражение для нейрона $l$\n",
    "$$ \\delta_l = \\frac{\\partial L}{\\partial net_l} = \\frac{\\partial L}{\\partial \\hat{y}(t)} \\cdot \\frac{\\partial \\hat{y}(t)}{\\partial net_l}$$\n",
    "и назовем это **сигнал об ошибке**, проходящий через нейрон $l$.\n",
    "\n",
    "Если нейрон $l$ находится на выходном слое, то просто считаем $\\frac{\\partial L}{\\partial \\hat{y}(t)}$, а $\\frac{\\partial \\hat{y}(t)}{\\partial net_l} = \\frac{\\partial z_l}{\\partial net_l} = f^{`}(net_l)$\n",
    "\n",
    "Eсли нейрон $l$ находится в промежуточном слое, то $\\frac{\\partial \\hat{y}(t)}{\\partial net_l}$ надо расписать через нейроны следующего (в направлении выхода сети) слоя:\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "\\frac{\\partial \\hat{y}(t)}{\\partial net_l} &= \\frac{\\partial \\hat{y}(t)}{\\partial z_l} & \\cdot \\frac{\\partial z_l}{\\partial net_l} \\\\ \n",
    "&= \\sum_{o \\in out(l)} \\left(\\frac{\\partial \\hat{y}(t)}{\\partial net_o} \\cdot \\frac{\\partial net_o}{\\partial z_l} \\right) & \\cdot f^{`}(net_l) \\\\\n",
    "&= \\sum_{o \\in out(l)} \\left(\\frac{\\partial \\hat{y}(t)}{\\partial net_o} \\cdot w_{lo} \\right) & \\cdot f^{`}(net_l) \n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "Теперь доставим это выражение в \"определение\" $\\delta_l$:\n",
    "$$\n",
    "\\begin{align} \n",
    "\\delta_l &=  \\frac{\\partial L}{\\partial \\hat{y}(t)} \\cdot \\frac{\\partial \\hat{y}(t)}{\\partial net_l} \\\\ \n",
    "&= \\frac{\\partial L}{\\partial \\hat{y}(t)} \\cdot \\sum_{o \\in out(l)} \\left(\\frac{\\partial \\hat{y}(t)}{\\partial net_o} \\cdot w_{lo} \\right) \\cdot f^{`}(net_l) \\\\\n",
    "&=  \\sum_{o \\in out(l)} \\left(\\frac{\\partial L}{\\partial \\hat{y}(t)} \\cdot \\frac{\\partial \\hat{y}(t)}{\\partial net_o} \\cdot w_{lo} \\right) \\cdot f^{`}(net_l) \\\\\n",
    "&=  \\sum_{o \\in out(l)} \\left(\\delta_o \\cdot w_{lo} \\right) \\cdot f^{`}(net_l) \\\\\n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "Красота!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В итоге - шаги алгоритма backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже преставленны шаги алгоритма обратного распространения ошибки с проходом одним случайным объектом. Проход всеми объектами выборки или батчем получается из данной записи очевидным образом.\n",
    "\n",
    "1. Некоторым образом инициализируем все веса сети\n",
    "2. Прогоняем случайный объект $x(t)$ по сети, попутно запоминая все $net_l$ и $z_l$ для всех слоев\n",
    "3. Считаем $\\delta_l$: если слой  $l$ выходной, то \n",
    "$$ \\delta_l = f^{(l)`}(net_l) \\frac{\\partial L}{\\partial \\hat{y}(t)} $$\n",
    "иначе \n",
    "$$ \\delta_l = f^{(l)`}(net_l) \\sum_{o \\in out(l)} \\delta_o w_{lo}$$\n",
    "4. Обновляем веса по следующему правилу\n",
    "$$ w_{jk} = w_{jk} - \\alpha \\cdot \\delta_k \\cdot z_j $$\n",
    "5. Если правило останова не выполнено, перейти к шагу 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Прямой проход"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Понятно, что операции, которые проходят в нейронной сети можно векторизовать.\n",
    "\n",
    "Например, умножение на веса и переход к следующему слою можно просто расписать в виде умножения на матрицу:\n",
    "\n",
    "$$\\left\\{net^{(1)}_j\\right\\}^{J}_{j=1} = \\left\\{ \\sum_iw_{ij}x_i \\right\\}^{J}_{j=1} = W^{i \\rightarrow j}x = net^{(1)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eсли же мы хотим применить какую-то функцию активации $f(\\cdot)$, то смело поэлементно применяем ее к $net^{(1)}$\n",
    "\n",
    "$$ Z^{(1)} = f(net^{(1)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/vect.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Таким образом, все что происходит в полносвязной сети можно реализовать с помощью матричных операций.\n",
    "Например, сеть с двумя слоями с сигмойдными функциями активации записывается как\n",
    "\n",
    "$$y = \\sigma(W_2\\sigma(W_1x)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обратный проход"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем сигнал об ошибке\n",
    "$$ \\left\\{\\delta_l\\right\\}^{L}_{l=1} = \\left\\{ f^{(l)`}(net_l) \\sum_{o \\in out(l)} \\delta_o w_{lo} \\right\\}^{L}_{l=1}$$\n",
    "в векторизованном виде:\n",
    "\n",
    "$$\n",
    "\\Delta^{(1)} = f^{`}(net^{(1)}) \\odot \\Delta^{(2)} W^{(1 \\rightarrow 2)}\n",
    "$$\n",
    "а для выходново слоя\n",
    "$$\n",
    "\\Delta^{(out)} = f^{`}(net^{(out)}) \\odot \\frac{\\partial L}{\\partial \\hat{y}}\n",
    "$$\n",
    "\n",
    "\n",
    "Тогда веса следует обновлять так:\n",
    "\n",
    "$$ W^{(1 \\rightarrow 2)} = W^{(1 \\rightarrow 2)} - \\alpha Z^{{(1)}^\\top} \\Delta^{(2)} $$\n",
    "\n",
    "И так для всех слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/back.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Основано на материале из [cs231n](http://cs231n.github.io/neural-networks-case-study/).\n",
    "\n",
    "Возьмем какой-нибудь простой датасет для классификации и обучим на нем нейронную сеть с одним скрытым слоем с функцией активации ReLU. <br\\>\n",
    "На выходе, расчеты должны пройти через SoftMax.\n",
    "$$z_l = \\frac{\\exp(net_l)}{\\sum_l \\exp(net_l)} \\text{ - это softmax}$$\n",
    "Таким образом, нейронная сеть должна для каждого класса возвращать вероятность принадлежности ему объекта, который через нее прошел.\n",
    "\n",
    "Ошибка в нашем случае будет выражаться как кросс-энтропия. Величину ошибки на одном объекте вычиляем так\n",
    "$$L(\\hat{y}(t)) = - \\sum\\limits_{c \\in classes} y_c\\log(\\hat{y}_c)$$\n",
    "Общая ошибка с регуляризацией:\n",
    "$$L = \\frac{1}{N}\\sum\\limits_{t=1}^N L(\\hat{y}(t)) + \\lambda\\frac{1}{2}\\|\\mathbf{W}\\|^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100 # кол-во объектов\n",
    "D = 2 # кол-во признаков\n",
    "K = 2 # кол-во классов\n",
    "X, y = make_moons(n_samples=N, noise=0.1, random_state=123)\n",
    "C = 0.01 # гиперпараметр регуляризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1108e59d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1107f4ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удобно сделать one-hot encoding для кодирования истинной метки классов объектов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bool = np.zeros((N, K))\n",
    "y_bool[range(N), y] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задаем колиество нейронов в первом слое и веса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 50 \n",
    "# Матрица весов между \n",
    "# входным слоем и промежуточным скрытым\n",
    "W = 0.01 * np.random.randn(D,h) \n",
    "b = np.zeros((1,h)) # св член\n",
    "\n",
    "# Матрица между промежуточным\n",
    "# скрытым слоем и выходным слоем\n",
    "W2 = 0.01 * np.random.randn(h,K) \n",
    "b2 = np.zeros((1,K)) # св член"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 50)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Прямой проход"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dot(W).dot(W2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Прогоняем всю выборку через сеть\n",
    "hidden_layer = np.maximum(0, np.dot(X, W) + b) \n",
    "scores = np.dot(hidden_layer, W2) + b2 # \"Сырой\" выход сети\n",
    "probs = np.exp(scores)/np.exp(scores).sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = -(y_bool * np.log(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.3233240174608\n"
     ]
    }
   ],
   "source": [
    "total_error = errors.sum()\n",
    "print(total_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что на каждом сле у нас есть свободный член b и b2. У них тоже есть веса и их надо учитывать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обратный проход"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начинаем с того, что с считаем $\\Delta^{(out)}$ на выходном слое.\n",
    "\n",
    "Оказывется, что в случае с softmax и кросс-энтропией $\\Delta^{(out)}$ будет выражаться очень просто!\n",
    "\n",
    "$$ \\Delta^{(out)} = \\hat{y} - y $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dscores = probs - y_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dscores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом градиент на веса между выходным и скрытым слоем выражается как \n",
    "\n",
    "$$\\Delta W_2 = Z^{{(1)}^\\top}\\Delta^{(out)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# производные по весам\n",
    "# между выходным и конечным слоем\n",
    "dW2 = np.dot(hidden_layer.T, dscores) + C*W2 # Не забываем регуляризацию\n",
    "\n",
    "# производная по весам для свободного члена\n",
    "db2 = np.sum(dscores, axis=0, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идем дальше\n",
    "\n",
    "$$\\Delta^{(1)} = f^{`}(net^{(1)}) \\odot D^{(2)} W^{(1 \\rightarrow 2)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Это просто пошагово\n",
    "# dhidden_rpart = np.dot(dscores, W2.T) # Правая часть выражения\n",
    "# dhidden_lpart = (hidden_layer >= 0).astype(int) # Производная ReLU\n",
    "# dhidden = dhidden_lpart * dhidden_rpart # Результат\n",
    "\n",
    "dhidden = np.dot(dscores, W2.T)\n",
    "dhidden[hidden_layer <= 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом градитент по весам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW = np.dot(X.T, dhidden) + C*W\n",
    "db = np.sum(dhidden, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот и все, дальше нужно итеративно обновлять веса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34670890694957424\n",
      "0.13316569112579202\n",
      "0.13313604749766544\n",
      "0.1176771746418301\n",
      "0.11370756188679565\n",
      "0.11233204557582853\n",
      "0.11171876084165587\n",
      "0.11146677567860701\n",
      "0.1111198325255528\n",
      "0.11099875856691523\n",
      "0.11086846430935253\n",
      "0.11075978363731448\n",
      "0.11070672967237029\n",
      "0.11064599677000399\n",
      "0.11059059075174744\n",
      "0.11061472982219593\n",
      "0.11054077584964148\n",
      "0.11047769858382966\n",
      "0.11051328248820605\n",
      "0.11048784442368717\n",
      "0.11043912876099675\n",
      "0.11045507770135478\n",
      "0.11045076916170406\n",
      "0.11036405111636492\n",
      "0.11034452535712096\n",
      "0.11031924769150883\n",
      "0.11030099692304451\n",
      "0.110283119184428\n",
      "0.11021615106348934\n",
      "0.11018892160399671\n",
      "0.11020872172639748\n",
      "0.11018030780568931\n",
      "0.11016620412457749\n",
      "0.11015712746505514\n",
      "0.11016803798136524\n",
      "0.1101217214074568\n",
      "0.11014610708954824\n",
      "0.11016670220188195\n",
      "0.11012491438715782\n",
      "0.11011388201133354\n",
      "0.1100864529305519\n",
      "0.11010636586637494\n",
      "0.11012676766890647\n",
      "0.11007548318900633\n",
      "0.11009779677786621\n",
      "0.11007304339979751\n",
      "0.11009627271257613\n",
      "0.11009064068241928\n",
      "0.11008798821329792\n",
      "0.11009386156401142\n",
      "0.11007651314920032\n",
      "0.11006984314326075\n",
      "0.11004347298072752\n",
      "0.11007644167066516\n",
      "0.11008452463662241\n",
      "0.11003797241952736\n",
      "0.11004119289098194\n",
      "0.11003361181431498\n",
      "0.11004332760607945\n",
      "0.11006174362744733\n",
      "0.11002759656736125\n",
      "0.11001606896418803\n",
      "0.11000753940729868\n",
      "0.11003704063353954\n",
      "0.11002880583371188\n",
      "0.11002849373417342\n",
      "0.11002514945024759\n",
      "0.11002545460727499\n",
      "0.11001293802459923\n",
      "0.11000813176935759\n",
      "0.11002161902999141\n",
      "0.1100294360324035\n",
      "0.1099965697604144\n",
      "0.10999391637579775\n",
      "0.10999384060705526\n",
      "0.11000128382819463\n",
      "0.11000904694848078\n",
      "0.1099786367497522\n",
      "0.10999350002605769\n",
      "0.11000665952764717\n",
      "0.1099820689051359\n",
      "0.10998828967894533\n",
      "0.1099942254240161\n",
      "0.10999598762300863\n",
      "0.10997018363552451\n",
      "0.10997241445490272\n",
      "0.10999413124600493\n",
      "0.11000740169745477\n",
      "0.11000674773109456\n",
      "0.10998290537216736\n",
      "0.10998261490623042\n",
      "0.10997074257365433\n",
      "0.10997874233830755\n",
      "0.10999946301150967\n",
      "0.11000466160566588\n",
      "0.10998816837966784\n",
      "0.11000890449332999\n",
      "0.10998641130646304\n",
      "0.1099930301034663\n",
      "0.10998838327150084\n"
     ]
    }
   ],
   "source": [
    "# Your Code Here\n",
    "alpha = 0.01\n",
    "max_iters = 10000\n",
    "\n",
    "h = 50 \n",
    "# Матрица весов между \n",
    "# входным слоем и промежуточным скрытым\n",
    "W = 0.01 * np.random.randn(D,h) \n",
    "b = np.zeros((1,h)) # св член\n",
    "\n",
    "# Матрица между промежуточным\n",
    "# скрытым слоем и выходным слоем\n",
    "W2 = 0.01 * np.random.randn(h,K) \n",
    "b2 = np.zeros((1,K)) # св член\n",
    "\n",
    "for i in range(max_iters):\n",
    "    \n",
    "    # Прямой проход\n",
    "    # Прогоняем всю выборку через сеть\n",
    "    hidden_layer = np.maximum(0, np.dot(X, W) + b) \n",
    "    scores = np.dot(hidden_layer, W2) + b2 # \"Сырой\" выход сети\n",
    "    probs = np.exp(scores)/np.exp(scores).sum(axis=1, keepdims=True)\n",
    "    \n",
    "    errors = -(y_bool * np.log(probs))\n",
    "    total_error = errors.mean()\n",
    "    if (i % 100) == 0:\n",
    "        print(total_error)\n",
    "\n",
    "    \n",
    "    # Обратный проход\n",
    "    dscores = probs - y_bool\n",
    "    \n",
    "    # между выходным и конечным слоем\n",
    "    dW2 = np.dot(hidden_layer.T, dscores) + C*W2 # Не забываем регуляризацию\n",
    "\n",
    "    # производная по весам для свободного члена\n",
    "    db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "    \n",
    "    dhidden = np.dot(dscores, W2.T)\n",
    "    dhidden[hidden_layer <= 0] = 0\n",
    "    \n",
    "    dW = np.dot(X.T, dhidden) + C*W\n",
    "    db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "\n",
    "    \n",
    "    # Обновление весов\n",
    "    W = W - alpha * dW\n",
    "    b = b - alpha * db\n",
    "    W2 = W2 - alpha * dW2\n",
    "    b2 = b2 - alpha * db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.argmax(probs, axis=1) == y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RND_SEED = 7\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "np.random.seed(RND_SEED)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для тренеровки мы будем использовать достаточно известный набор данных [Pima Indians](https://www.kaggle.com/uciml/pima-indians-diabetes-database/data).\n",
    "\n",
    "Признаки такие: <br\\>\n",
    "1. Number of times pregnant\n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "3. Diastolic blood pressure (mm Hg)\n",
    "4. Triceps skin fold thickness (mm)\n",
    "5. 2-Hour serum insulin (mu U/ml)\n",
    "6. Body mass index (weight in kg/(height in m)^2)\n",
    "7. Diabetes pedigree function\n",
    "8. Age (years)\n",
    "9. Class variable (0 or 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/diabetes.csv',\n",
    "                 sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.iloc[:, :8].values, df.iloc[:, 8].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=RND_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Составляем модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим простую 2х (3х) слойную нейронную сеть. Делать это в keras одно удовольствие"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 8)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, init='normal', activation='sigmoid'))\n",
    "model.add(Dense(8, init='normal', activation='sigmoid'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь надо задать функцию [ошибки](https://keras.io/objectives/), способ [оптимизации](https://keras.io/optimizers/) и метрику измерения [качества](https://keras.io/metrics/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(614, 8)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 491 samples, validate on 123 samples\n",
      "Epoch 1/150\n",
      "491/491 [==============================] - 0s - loss: 0.6844 - acc: 0.6273 - val_loss: 0.6733 - val_acc: 0.6504\n",
      "Epoch 2/150\n",
      "491/491 [==============================] - 0s - loss: 0.6629 - acc: 0.6578 - val_loss: 0.6584 - val_acc: 0.6504\n",
      "Epoch 3/150\n",
      "491/491 [==============================] - 0s - loss: 0.6527 - acc: 0.6578 - val_loss: 0.6508 - val_acc: 0.6504\n",
      "Epoch 4/150\n",
      "491/491 [==============================] - 0s - loss: 0.6467 - acc: 0.6578 - val_loss: 0.6487 - val_acc: 0.6504\n",
      "Epoch 5/150\n",
      "491/491 [==============================] - 0s - loss: 0.6442 - acc: 0.6578 - val_loss: 0.6475 - val_acc: 0.6504\n",
      "Epoch 6/150\n",
      "491/491 [==============================] - 0s - loss: 0.6425 - acc: 0.6578 - val_loss: 0.6465 - val_acc: 0.6504\n",
      "Epoch 7/150\n",
      "491/491 [==============================] - 0s - loss: 0.6418 - acc: 0.6578 - val_loss: 0.6459 - val_acc: 0.6504\n",
      "Epoch 8/150\n",
      "491/491 [==============================] - 0s - loss: 0.6413 - acc: 0.6578 - val_loss: 0.6454 - val_acc: 0.6504\n",
      "Epoch 9/150\n",
      "491/491 [==============================] - 0s - loss: 0.6406 - acc: 0.6578 - val_loss: 0.6449 - val_acc: 0.6504\n",
      "Epoch 10/150\n",
      "491/491 [==============================] - 0s - loss: 0.6399 - acc: 0.6578 - val_loss: 0.6444 - val_acc: 0.6504\n",
      "Epoch 11/150\n",
      "491/491 [==============================] - 0s - loss: 0.6393 - acc: 0.6578 - val_loss: 0.6434 - val_acc: 0.6504\n",
      "Epoch 12/150\n",
      "491/491 [==============================] - 0s - loss: 0.6396 - acc: 0.6578 - val_loss: 0.6446 - val_acc: 0.6504\n",
      "Epoch 13/150\n",
      "491/491 [==============================] - 0s - loss: 0.6390 - acc: 0.6578 - val_loss: 0.6420 - val_acc: 0.6504\n",
      "Epoch 14/150\n",
      "491/491 [==============================] - 0s - loss: 0.6381 - acc: 0.6578 - val_loss: 0.6415 - val_acc: 0.6504\n",
      "Epoch 15/150\n",
      "491/491 [==============================] - 0s - loss: 0.6377 - acc: 0.6578 - val_loss: 0.6410 - val_acc: 0.6504\n",
      "Epoch 16/150\n",
      "491/491 [==============================] - 0s - loss: 0.6364 - acc: 0.6578 - val_loss: 0.6409 - val_acc: 0.6504\n",
      "Epoch 17/150\n",
      "491/491 [==============================] - 0s - loss: 0.6357 - acc: 0.6578 - val_loss: 0.6412 - val_acc: 0.6504\n",
      "Epoch 18/150\n",
      "491/491 [==============================] - 0s - loss: 0.6358 - acc: 0.6578 - val_loss: 0.6376 - val_acc: 0.6504\n",
      "Epoch 19/150\n",
      "491/491 [==============================] - 0s - loss: 0.6341 - acc: 0.6578 - val_loss: 0.6370 - val_acc: 0.6504\n",
      "Epoch 20/150\n",
      "491/491 [==============================] - 0s - loss: 0.6330 - acc: 0.6578 - val_loss: 0.6362 - val_acc: 0.6504\n",
      "Epoch 21/150\n",
      "491/491 [==============================] - 0s - loss: 0.6322 - acc: 0.6578 - val_loss: 0.6349 - val_acc: 0.6504\n",
      "Epoch 22/150\n",
      "491/491 [==============================] - 0s - loss: 0.6306 - acc: 0.6578 - val_loss: 0.6338 - val_acc: 0.6504\n",
      "Epoch 23/150\n",
      "491/491 [==============================] - 0s - loss: 0.6297 - acc: 0.6578 - val_loss: 0.6344 - val_acc: 0.6504\n",
      "Epoch 24/150\n",
      "491/491 [==============================] - 0s - loss: 0.6290 - acc: 0.6578 - val_loss: 0.6310 - val_acc: 0.6504\n",
      "Epoch 25/150\n",
      "491/491 [==============================] - 0s - loss: 0.6300 - acc: 0.6578 - val_loss: 0.6283 - val_acc: 0.6504\n",
      "Epoch 26/150\n",
      "491/491 [==============================] - 0s - loss: 0.6262 - acc: 0.6578 - val_loss: 0.6268 - val_acc: 0.6504\n",
      "Epoch 27/150\n",
      "491/491 [==============================] - 0s - loss: 0.6263 - acc: 0.6578 - val_loss: 0.6276 - val_acc: 0.6504\n",
      "Epoch 28/150\n",
      "491/491 [==============================] - 0s - loss: 0.6279 - acc: 0.6578 - val_loss: 0.6250 - val_acc: 0.6504\n",
      "Epoch 29/150\n",
      "491/491 [==============================] - 0s - loss: 0.6247 - acc: 0.6578 - val_loss: 0.6241 - val_acc: 0.6504\n",
      "Epoch 30/150\n",
      "491/491 [==============================] - 0s - loss: 0.6225 - acc: 0.6578 - val_loss: 0.6244 - val_acc: 0.6504\n",
      "Epoch 31/150\n",
      "491/491 [==============================] - 0s - loss: 0.6205 - acc: 0.6578 - val_loss: 0.6208 - val_acc: 0.6504\n",
      "Epoch 32/150\n",
      "491/491 [==============================] - 0s - loss: 0.6205 - acc: 0.6578 - val_loss: 0.6193 - val_acc: 0.6504\n",
      "Epoch 33/150\n",
      "491/491 [==============================] - 0s - loss: 0.6188 - acc: 0.6578 - val_loss: 0.6155 - val_acc: 0.6504\n",
      "Epoch 34/150\n",
      "491/491 [==============================] - 0s - loss: 0.6160 - acc: 0.6578 - val_loss: 0.6176 - val_acc: 0.6504\n",
      "Epoch 35/150\n",
      "491/491 [==============================] - 0s - loss: 0.6128 - acc: 0.6578 - val_loss: 0.6181 - val_acc: 0.6504\n",
      "Epoch 36/150\n",
      "491/491 [==============================] - 0s - loss: 0.6127 - acc: 0.6578 - val_loss: 0.6112 - val_acc: 0.6504\n",
      "Epoch 37/150\n",
      "491/491 [==============================] - 0s - loss: 0.6096 - acc: 0.6578 - val_loss: 0.6104 - val_acc: 0.6504\n",
      "Epoch 38/150\n",
      "491/491 [==============================] - 0s - loss: 0.6098 - acc: 0.6578 - val_loss: 0.6143 - val_acc: 0.6504\n",
      "Epoch 39/150\n",
      "491/491 [==============================] - 0s - loss: 0.6078 - acc: 0.6578 - val_loss: 0.6069 - val_acc: 0.6504\n",
      "Epoch 40/150\n",
      "491/491 [==============================] - 0s - loss: 0.6061 - acc: 0.6578 - val_loss: 0.6115 - val_acc: 0.6504\n",
      "Epoch 41/150\n",
      "491/491 [==============================] - 0s - loss: 0.6046 - acc: 0.6578 - val_loss: 0.6007 - val_acc: 0.6504\n",
      "Epoch 42/150\n",
      "491/491 [==============================] - 0s - loss: 0.6033 - acc: 0.6578 - val_loss: 0.6035 - val_acc: 0.6504\n",
      "Epoch 43/150\n",
      "491/491 [==============================] - 0s - loss: 0.6005 - acc: 0.6578 - val_loss: 0.6043 - val_acc: 0.6504\n",
      "Epoch 44/150\n",
      "491/491 [==============================] - 0s - loss: 0.5975 - acc: 0.6578 - val_loss: 0.5969 - val_acc: 0.6504\n",
      "Epoch 45/150\n",
      "491/491 [==============================] - 0s - loss: 0.5953 - acc: 0.6578 - val_loss: 0.6014 - val_acc: 0.6504\n",
      "Epoch 46/150\n",
      "491/491 [==============================] - 0s - loss: 0.5952 - acc: 0.6578 - val_loss: 0.5911 - val_acc: 0.6504\n",
      "Epoch 47/150\n",
      "491/491 [==============================] - 0s - loss: 0.5919 - acc: 0.6578 - val_loss: 0.5914 - val_acc: 0.6504\n",
      "Epoch 48/150\n",
      "491/491 [==============================] - 0s - loss: 0.5937 - acc: 0.6578 - val_loss: 0.5979 - val_acc: 0.6504\n",
      "Epoch 49/150\n",
      "491/491 [==============================] - 0s - loss: 0.5935 - acc: 0.6578 - val_loss: 0.5915 - val_acc: 0.6504\n",
      "Epoch 50/150\n",
      "491/491 [==============================] - 0s - loss: 0.5901 - acc: 0.6578 - val_loss: 0.5830 - val_acc: 0.6504\n",
      "Epoch 51/150\n",
      "491/491 [==============================] - 0s - loss: 0.5850 - acc: 0.6578 - val_loss: 0.5893 - val_acc: 0.6504\n",
      "Epoch 52/150\n",
      "491/491 [==============================] - 0s - loss: 0.5886 - acc: 0.6578 - val_loss: 0.5880 - val_acc: 0.6504\n",
      "Epoch 53/150\n",
      "491/491 [==============================] - 0s - loss: 0.5862 - acc: 0.6578 - val_loss: 0.5854 - val_acc: 0.6504\n",
      "Epoch 54/150\n",
      "491/491 [==============================] - 0s - loss: 0.5833 - acc: 0.6578 - val_loss: 0.5787 - val_acc: 0.6504\n",
      "Epoch 55/150\n",
      "491/491 [==============================] - 0s - loss: 0.5879 - acc: 0.6578 - val_loss: 0.5797 - val_acc: 0.6504\n",
      "Epoch 56/150\n",
      "491/491 [==============================] - 0s - loss: 0.5804 - acc: 0.6578 - val_loss: 0.5842 - val_acc: 0.6504\n",
      "Epoch 57/150\n",
      "491/491 [==============================] - 0s - loss: 0.5782 - acc: 0.6578 - val_loss: 0.5800 - val_acc: 0.6504\n",
      "Epoch 58/150\n",
      "491/491 [==============================] - 0s - loss: 0.5823 - acc: 0.6578 - val_loss: 0.5819 - val_acc: 0.6504\n",
      "Epoch 59/150\n",
      "491/491 [==============================] - 0s - loss: 0.5780 - acc: 0.6578 - val_loss: 0.5808 - val_acc: 0.6504\n",
      "Epoch 60/150\n",
      "491/491 [==============================] - 0s - loss: 0.5763 - acc: 0.6578 - val_loss: 0.5795 - val_acc: 0.6504\n",
      "Epoch 61/150\n",
      "491/491 [==============================] - 0s - loss: 0.5760 - acc: 0.6578 - val_loss: 0.5781 - val_acc: 0.6504\n",
      "Epoch 62/150\n",
      "491/491 [==============================] - 0s - loss: 0.5736 - acc: 0.6578 - val_loss: 0.5726 - val_acc: 0.6504\n",
      "Epoch 63/150\n",
      "491/491 [==============================] - 0s - loss: 0.5731 - acc: 0.6578 - val_loss: 0.5733 - val_acc: 0.6504\n",
      "Epoch 64/150\n",
      "491/491 [==============================] - 0s - loss: 0.5725 - acc: 0.6578 - val_loss: 0.5788 - val_acc: 0.6504\n",
      "Epoch 65/150\n",
      "491/491 [==============================] - 0s - loss: 0.5730 - acc: 0.6578 - val_loss: 0.5739 - val_acc: 0.6504\n",
      "Epoch 66/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 0s - loss: 0.5682 - acc: 0.6578 - val_loss: 0.5779 - val_acc: 0.6504\n",
      "Epoch 67/150\n",
      "491/491 [==============================] - 0s - loss: 0.5674 - acc: 0.6578 - val_loss: 0.5716 - val_acc: 0.6504\n",
      "Epoch 68/150\n",
      "491/491 [==============================] - 0s - loss: 0.5723 - acc: 0.6578 - val_loss: 0.5745 - val_acc: 0.6504\n",
      "Epoch 69/150\n",
      "491/491 [==============================] - 0s - loss: 0.5740 - acc: 0.6578 - val_loss: 0.5832 - val_acc: 0.6504\n",
      "Epoch 70/150\n",
      "491/491 [==============================] - 0s - loss: 0.5691 - acc: 0.6578 - val_loss: 0.5797 - val_acc: 0.6504\n",
      "Epoch 71/150\n",
      "491/491 [==============================] - 0s - loss: 0.5664 - acc: 0.6578 - val_loss: 0.5738 - val_acc: 0.6585\n",
      "Epoch 72/150\n",
      "491/491 [==============================] - 0s - loss: 0.5653 - acc: 0.6619 - val_loss: 0.5806 - val_acc: 0.6504\n",
      "Epoch 73/150\n",
      "491/491 [==============================] - 0s - loss: 0.5655 - acc: 0.6741 - val_loss: 0.5753 - val_acc: 0.6585\n",
      "Epoch 74/150\n",
      "491/491 [==============================] - 0s - loss: 0.5644 - acc: 0.6864 - val_loss: 0.5751 - val_acc: 0.6504\n",
      "Epoch 75/150\n",
      "491/491 [==============================] - 0s - loss: 0.5621 - acc: 0.6802 - val_loss: 0.5702 - val_acc: 0.6504\n",
      "Epoch 76/150\n",
      "491/491 [==============================] - 0s - loss: 0.5686 - acc: 0.6782 - val_loss: 0.5794 - val_acc: 0.6423\n",
      "Epoch 77/150\n",
      "491/491 [==============================] - 0s - loss: 0.5627 - acc: 0.6782 - val_loss: 0.5882 - val_acc: 0.6585\n",
      "Epoch 78/150\n",
      "491/491 [==============================] - 0s - loss: 0.5689 - acc: 0.6680 - val_loss: 0.5725 - val_acc: 0.6504\n",
      "Epoch 79/150\n",
      "491/491 [==============================] - 0s - loss: 0.5604 - acc: 0.6823 - val_loss: 0.5789 - val_acc: 0.6585\n",
      "Epoch 80/150\n",
      "491/491 [==============================] - 0s - loss: 0.5612 - acc: 0.6823 - val_loss: 0.5704 - val_acc: 0.6504\n",
      "Epoch 81/150\n",
      "491/491 [==============================] - 0s - loss: 0.5630 - acc: 0.6986 - val_loss: 0.5758 - val_acc: 0.6829\n",
      "Epoch 82/150\n",
      "491/491 [==============================] - 0s - loss: 0.5627 - acc: 0.6884 - val_loss: 0.5739 - val_acc: 0.6667\n",
      "Epoch 83/150\n",
      "491/491 [==============================] - 0s - loss: 0.5584 - acc: 0.6986 - val_loss: 0.5836 - val_acc: 0.6667\n",
      "Epoch 84/150\n",
      "491/491 [==============================] - 0s - loss: 0.5590 - acc: 0.7047 - val_loss: 0.5798 - val_acc: 0.6667\n",
      "Epoch 85/150\n",
      "491/491 [==============================] - 0s - loss: 0.5597 - acc: 0.6986 - val_loss: 0.5719 - val_acc: 0.6911\n",
      "Epoch 86/150\n",
      "491/491 [==============================] - 0s - loss: 0.5597 - acc: 0.7047 - val_loss: 0.5743 - val_acc: 0.6585\n",
      "Epoch 87/150\n",
      "491/491 [==============================] - 0s - loss: 0.5576 - acc: 0.7189 - val_loss: 0.5880 - val_acc: 0.6748\n",
      "Epoch 88/150\n",
      "491/491 [==============================] - 0s - loss: 0.5542 - acc: 0.7149 - val_loss: 0.5749 - val_acc: 0.6748\n",
      "Epoch 89/150\n",
      "491/491 [==============================] - 0s - loss: 0.5576 - acc: 0.7006 - val_loss: 0.5757 - val_acc: 0.6748\n",
      "Epoch 90/150\n",
      "491/491 [==============================] - 0s - loss: 0.5558 - acc: 0.7067 - val_loss: 0.5774 - val_acc: 0.6504\n",
      "Epoch 91/150\n",
      "491/491 [==============================] - 0s - loss: 0.5588 - acc: 0.7108 - val_loss: 0.5731 - val_acc: 0.6667\n",
      "Epoch 92/150\n",
      "491/491 [==============================] - 0s - loss: 0.5546 - acc: 0.7169 - val_loss: 0.5801 - val_acc: 0.6748\n",
      "Epoch 93/150\n",
      "491/491 [==============================] - 0s - loss: 0.5551 - acc: 0.7149 - val_loss: 0.5860 - val_acc: 0.6585\n",
      "Epoch 94/150\n",
      "491/491 [==============================] - 0s - loss: 0.5532 - acc: 0.7108 - val_loss: 0.5799 - val_acc: 0.6585\n",
      "Epoch 95/150\n",
      "491/491 [==============================] - 0s - loss: 0.5574 - acc: 0.7251 - val_loss: 0.5869 - val_acc: 0.6667\n",
      "Epoch 96/150\n",
      "491/491 [==============================] - 0s - loss: 0.5606 - acc: 0.7149 - val_loss: 0.5749 - val_acc: 0.6829\n",
      "Epoch 97/150\n",
      "491/491 [==============================] - 0s - loss: 0.5534 - acc: 0.7149 - val_loss: 0.5818 - val_acc: 0.6667\n",
      "Epoch 98/150\n",
      "491/491 [==============================] - 0s - loss: 0.5540 - acc: 0.7128 - val_loss: 0.5783 - val_acc: 0.6667\n",
      "Epoch 99/150\n",
      "491/491 [==============================] - 0s - loss: 0.5557 - acc: 0.7067 - val_loss: 0.5881 - val_acc: 0.6585\n",
      "Epoch 100/150\n",
      "491/491 [==============================] - 0s - loss: 0.5618 - acc: 0.7169 - val_loss: 0.5831 - val_acc: 0.6585\n",
      "Epoch 101/150\n",
      "491/491 [==============================] - 0s - loss: 0.5506 - acc: 0.7230 - val_loss: 0.5783 - val_acc: 0.6748\n",
      "Epoch 102/150\n",
      "491/491 [==============================] - 0s - loss: 0.5541 - acc: 0.7108 - val_loss: 0.5800 - val_acc: 0.6829\n",
      "Epoch 103/150\n",
      "491/491 [==============================] - 0s - loss: 0.5505 - acc: 0.7128 - val_loss: 0.5766 - val_acc: 0.6504\n",
      "Epoch 104/150\n",
      "491/491 [==============================] - 0s - loss: 0.5513 - acc: 0.7108 - val_loss: 0.5829 - val_acc: 0.6829\n",
      "Epoch 105/150\n",
      "491/491 [==============================] - 0s - loss: 0.5496 - acc: 0.7047 - val_loss: 0.5806 - val_acc: 0.6667\n",
      "Epoch 106/150\n",
      "491/491 [==============================] - 0s - loss: 0.5565 - acc: 0.7189 - val_loss: 0.6014 - val_acc: 0.6748\n",
      "Epoch 107/150\n",
      "491/491 [==============================] - 0s - loss: 0.5559 - acc: 0.7088 - val_loss: 0.5988 - val_acc: 0.6829\n",
      "Epoch 108/150\n",
      "491/491 [==============================] - 0s - loss: 0.5502 - acc: 0.7026 - val_loss: 0.5785 - val_acc: 0.6667\n",
      "Epoch 109/150\n",
      "491/491 [==============================] - 0s - loss: 0.5480 - acc: 0.7189 - val_loss: 0.5864 - val_acc: 0.6829\n",
      "Epoch 110/150\n",
      "491/491 [==============================] - 0s - loss: 0.5489 - acc: 0.7067 - val_loss: 0.5868 - val_acc: 0.6748\n",
      "Epoch 111/150\n",
      "491/491 [==============================] - 0s - loss: 0.5480 - acc: 0.7251 - val_loss: 0.5834 - val_acc: 0.6748\n",
      "Epoch 112/150\n",
      "491/491 [==============================] - 0s - loss: 0.5429 - acc: 0.7169 - val_loss: 0.5901 - val_acc: 0.6748\n",
      "Epoch 113/150\n",
      "491/491 [==============================] - 0s - loss: 0.5477 - acc: 0.7291 - val_loss: 0.6133 - val_acc: 0.6667\n",
      "Epoch 114/150\n",
      "491/491 [==============================] - 0s - loss: 0.5485 - acc: 0.7189 - val_loss: 0.5930 - val_acc: 0.6667\n",
      "Epoch 115/150\n",
      "491/491 [==============================] - 0s - loss: 0.5517 - acc: 0.7088 - val_loss: 0.5825 - val_acc: 0.6992\n",
      "Epoch 116/150\n",
      "491/491 [==============================] - 0s - loss: 0.5554 - acc: 0.7108 - val_loss: 0.5915 - val_acc: 0.6911\n",
      "Epoch 117/150\n",
      "491/491 [==============================] - 0s - loss: 0.5447 - acc: 0.7169 - val_loss: 0.5925 - val_acc: 0.6911\n",
      "Epoch 118/150\n",
      "491/491 [==============================] - 0s - loss: 0.5418 - acc: 0.7189 - val_loss: 0.6028 - val_acc: 0.6667\n",
      "Epoch 119/150\n",
      "491/491 [==============================] - 0s - loss: 0.5430 - acc: 0.7108 - val_loss: 0.5958 - val_acc: 0.6423\n",
      "Epoch 120/150\n",
      "491/491 [==============================] - 0s - loss: 0.5606 - acc: 0.7088 - val_loss: 0.5871 - val_acc: 0.6667\n",
      "Epoch 121/150\n",
      "491/491 [==============================] - 0s - loss: 0.5468 - acc: 0.7026 - val_loss: 0.6110 - val_acc: 0.6504\n",
      "Epoch 122/150\n",
      "491/491 [==============================] - 0s - loss: 0.5432 - acc: 0.7149 - val_loss: 0.5916 - val_acc: 0.6829\n",
      "Epoch 123/150\n",
      "491/491 [==============================] - 0s - loss: 0.5520 - acc: 0.7067 - val_loss: 0.5795 - val_acc: 0.6748\n",
      "Epoch 124/150\n",
      "491/491 [==============================] - 0s - loss: 0.5445 - acc: 0.7210 - val_loss: 0.5773 - val_acc: 0.6911\n",
      "Epoch 125/150\n",
      "491/491 [==============================] - 0s - loss: 0.5493 - acc: 0.7189 - val_loss: 0.5864 - val_acc: 0.6585\n",
      "Epoch 126/150\n",
      "491/491 [==============================] - 0s - loss: 0.5471 - acc: 0.7169 - val_loss: 0.5760 - val_acc: 0.7236\n",
      "Epoch 127/150\n",
      "491/491 [==============================] - 0s - loss: 0.5470 - acc: 0.7169 - val_loss: 0.5880 - val_acc: 0.6585\n",
      "Epoch 128/150\n",
      "491/491 [==============================] - 0s - loss: 0.5457 - acc: 0.7230 - val_loss: 0.5971 - val_acc: 0.6829\n",
      "Epoch 129/150\n",
      "491/491 [==============================] - 0s - loss: 0.5381 - acc: 0.7006 - val_loss: 0.5873 - val_acc: 0.6829\n",
      "Epoch 130/150\n",
      "491/491 [==============================] - 0s - loss: 0.5404 - acc: 0.7210 - val_loss: 0.5793 - val_acc: 0.6992\n",
      "Epoch 131/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 0s - loss: 0.5416 - acc: 0.7352 - val_loss: 0.5865 - val_acc: 0.6748\n",
      "Epoch 132/150\n",
      "491/491 [==============================] - 0s - loss: 0.5361 - acc: 0.7108 - val_loss: 0.5939 - val_acc: 0.6667\n",
      "Epoch 133/150\n",
      "491/491 [==============================] - 0s - loss: 0.5420 - acc: 0.7169 - val_loss: 0.6056 - val_acc: 0.6667\n",
      "Epoch 134/150\n",
      "491/491 [==============================] - 0s - loss: 0.5364 - acc: 0.7088 - val_loss: 0.5991 - val_acc: 0.6667\n",
      "Epoch 135/150\n",
      "491/491 [==============================] - 0s - loss: 0.5365 - acc: 0.7108 - val_loss: 0.5952 - val_acc: 0.6585\n",
      "Epoch 136/150\n",
      "491/491 [==============================] - 0s - loss: 0.5393 - acc: 0.7088 - val_loss: 0.5858 - val_acc: 0.6829\n",
      "Epoch 137/150\n",
      "491/491 [==============================] - 0s - loss: 0.5368 - acc: 0.7230 - val_loss: 0.6052 - val_acc: 0.6829\n",
      "Epoch 138/150\n",
      "491/491 [==============================] - 0s - loss: 0.5361 - acc: 0.7067 - val_loss: 0.6041 - val_acc: 0.6423\n",
      "Epoch 139/150\n",
      "491/491 [==============================] - 0s - loss: 0.5341 - acc: 0.7108 - val_loss: 0.5968 - val_acc: 0.6829\n",
      "Epoch 140/150\n",
      "491/491 [==============================] - 0s - loss: 0.5336 - acc: 0.7332 - val_loss: 0.6109 - val_acc: 0.6667\n",
      "Epoch 141/150\n",
      "491/491 [==============================] - 0s - loss: 0.5390 - acc: 0.7210 - val_loss: 0.5909 - val_acc: 0.6829\n",
      "Epoch 142/150\n",
      "491/491 [==============================] - 0s - loss: 0.5368 - acc: 0.7230 - val_loss: 0.6007 - val_acc: 0.6667\n",
      "Epoch 143/150\n",
      "491/491 [==============================] - 0s - loss: 0.5323 - acc: 0.7291 - val_loss: 0.5919 - val_acc: 0.6992\n",
      "Epoch 144/150\n",
      "491/491 [==============================] - 0s - loss: 0.5353 - acc: 0.7271 - val_loss: 0.5916 - val_acc: 0.6748\n",
      "Epoch 145/150\n",
      "491/491 [==============================] - 0s - loss: 0.5277 - acc: 0.7149 - val_loss: 0.6028 - val_acc: 0.6585\n",
      "Epoch 146/150\n",
      "491/491 [==============================] - 0s - loss: 0.5356 - acc: 0.7189 - val_loss: 0.6209 - val_acc: 0.6504\n",
      "Epoch 147/150\n",
      "491/491 [==============================] - 0s - loss: 0.5355 - acc: 0.7352 - val_loss: 0.5869 - val_acc: 0.6911\n",
      "Epoch 148/150\n",
      "491/491 [==============================] - 0s - loss: 0.5389 - acc: 0.7251 - val_loss: 0.6040 - val_acc: 0.6667\n",
      "Epoch 149/150\n",
      "491/491 [==============================] - 0s - loss: 0.5365 - acc: 0.7230 - val_loss: 0.6163 - val_acc: 0.6585\n",
      "Epoch 150/150\n",
      "491/491 [==============================] - 0s - loss: 0.5316 - acc: 0.7230 - val_loss: 0.6192 - val_acc: 0.6829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1d70cf10>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, nb_epoch=150, batch_size=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/154 [=======================>......] - ETA: 0s\n",
      "acc: 75.32%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def gen_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, init='normal', activation='sigmoid'))\n",
    "    model.add(Dense(8, init='normal', activation='sigmoid'))\n",
    "    model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'], )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('nn', KerasClassifier(build_fn=gen_model, nb_epoch=150, batch_size=10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "614/614 [==============================] - 0s - loss: 0.6782 - acc: 0.6498     \n",
      "Epoch 2/150\n",
      "614/614 [==============================] - 0s - loss: 0.6558 - acc: 0.6564     \n",
      "Epoch 3/150\n",
      "614/614 [==============================] - 0s - loss: 0.6442 - acc: 0.6564     \n",
      "Epoch 4/150\n",
      "614/614 [==============================] - 0s - loss: 0.6373 - acc: 0.6564     \n",
      "Epoch 5/150\n",
      "614/614 [==============================] - 0s - loss: 0.6328 - acc: 0.6564     \n",
      "Epoch 6/150\n",
      "614/614 [==============================] - 0s - loss: 0.6300 - acc: 0.6564     \n",
      "Epoch 7/150\n",
      "614/614 [==============================] - 0s - loss: 0.6263 - acc: 0.6564     \n",
      "Epoch 8/150\n",
      "614/614 [==============================] - 0s - loss: 0.6236 - acc: 0.6564     \n",
      "Epoch 9/150\n",
      "614/614 [==============================] - 0s - loss: 0.6213 - acc: 0.6564     \n",
      "Epoch 10/150\n",
      "614/614 [==============================] - 0s - loss: 0.6180 - acc: 0.6564     \n",
      "Epoch 11/150\n",
      "614/614 [==============================] - 0s - loss: 0.6152 - acc: 0.6564     \n",
      "Epoch 12/150\n",
      "614/614 [==============================] - 0s - loss: 0.6124 - acc: 0.6564     \n",
      "Epoch 13/150\n",
      "614/614 [==============================] - 0s - loss: 0.6099 - acc: 0.6564     \n",
      "Epoch 14/150\n",
      "614/614 [==============================] - 0s - loss: 0.6066 - acc: 0.6564     \n",
      "Epoch 15/150\n",
      "614/614 [==============================] - 0s - loss: 0.6035 - acc: 0.6564     \n",
      "Epoch 16/150\n",
      "614/614 [==============================] - 0s - loss: 0.6005 - acc: 0.6564     \n",
      "Epoch 17/150\n",
      "614/614 [==============================] - 0s - loss: 0.5969 - acc: 0.6564     \n",
      "Epoch 18/150\n",
      "614/614 [==============================] - 0s - loss: 0.5939 - acc: 0.6564     \n",
      "Epoch 19/150\n",
      "614/614 [==============================] - 0s - loss: 0.5902 - acc: 0.6564     \n",
      "Epoch 20/150\n",
      "614/614 [==============================] - 0s - loss: 0.5873 - acc: 0.6564     \n",
      "Epoch 21/150\n",
      "614/614 [==============================] - 0s - loss: 0.5834 - acc: 0.6564     \n",
      "Epoch 22/150\n",
      "614/614 [==============================] - 0s - loss: 0.5799 - acc: 0.6564     \n",
      "Epoch 23/150\n",
      "614/614 [==============================] - 0s - loss: 0.5763 - acc: 0.6564     \n",
      "Epoch 24/150\n",
      "614/614 [==============================] - 0s - loss: 0.5727 - acc: 0.6564     \n",
      "Epoch 25/150\n",
      "614/614 [==============================] - 0s - loss: 0.5690 - acc: 0.6564     \n",
      "Epoch 26/150\n",
      "614/614 [==============================] - 0s - loss: 0.5654 - acc: 0.6564     \n",
      "Epoch 27/150\n",
      "614/614 [==============================] - 0s - loss: 0.5616 - acc: 0.6564     \n",
      "Epoch 28/150\n",
      "614/614 [==============================] - 0s - loss: 0.5583 - acc: 0.6564     \n",
      "Epoch 29/150\n",
      "614/614 [==============================] - 0s - loss: 0.5546 - acc: 0.6564     \n",
      "Epoch 30/150\n",
      "614/614 [==============================] - 0s - loss: 0.5513 - acc: 0.6564     \n",
      "Epoch 31/150\n",
      "614/614 [==============================] - 0s - loss: 0.5480 - acc: 0.6564     \n",
      "Epoch 32/150\n",
      "614/614 [==============================] - 0s - loss: 0.5447 - acc: 0.6564     \n",
      "Epoch 33/150\n",
      "614/614 [==============================] - 0s - loss: 0.5415 - acc: 0.6564     \n",
      "Epoch 34/150\n",
      "614/614 [==============================] - 0s - loss: 0.5382 - acc: 0.6564     - E\n",
      "Epoch 35/150\n",
      "614/614 [==============================] - 0s - loss: 0.5352 - acc: 0.6564     \n",
      "Epoch 36/150\n",
      "614/614 [==============================] - 0s - loss: 0.5325 - acc: 0.6564     \n",
      "Epoch 37/150\n",
      "614/614 [==============================] - 0s - loss: 0.5298 - acc: 0.6564     \n",
      "Epoch 38/150\n",
      "614/614 [==============================] - 0s - loss: 0.5274 - acc: 0.6564     \n",
      "Epoch 39/150\n",
      "614/614 [==============================] - 0s - loss: 0.5247 - acc: 0.6564     \n",
      "Epoch 40/150\n",
      "614/614 [==============================] - 0s - loss: 0.5224 - acc: 0.6564     \n",
      "Epoch 41/150\n",
      "614/614 [==============================] - 0s - loss: 0.5202 - acc: 0.6580     \n",
      "Epoch 42/150\n",
      "614/614 [==============================] - 0s - loss: 0.5182 - acc: 0.7329     \n",
      "Epoch 43/150\n",
      "614/614 [==============================] - 0s - loss: 0.5159 - acc: 0.7443     \n",
      "Epoch 44/150\n",
      "614/614 [==============================] - 0s - loss: 0.5140 - acc: 0.7687     \n",
      "Epoch 45/150\n",
      "614/614 [==============================] - 0s - loss: 0.5123 - acc: 0.7655     \n",
      "Epoch 46/150\n",
      "614/614 [==============================] - 0s - loss: 0.5106 - acc: 0.7671     \n",
      "Epoch 47/150\n",
      "614/614 [==============================] - 0s - loss: 0.5087 - acc: 0.7752     \n",
      "Epoch 48/150\n",
      "614/614 [==============================] - 0s - loss: 0.5075 - acc: 0.7655     \n",
      "Epoch 49/150\n",
      "614/614 [==============================] - 0s - loss: 0.5059 - acc: 0.7655     \n",
      "Epoch 50/150\n",
      "614/614 [==============================] - 0s - loss: 0.5043 - acc: 0.7671     \n",
      "Epoch 51/150\n",
      "614/614 [==============================] - 0s - loss: 0.5035 - acc: 0.7622     \n",
      "Epoch 52/150\n",
      "614/614 [==============================] - 0s - loss: 0.5016 - acc: 0.7590     \n",
      "Epoch 53/150\n",
      "614/614 [==============================] - 0s - loss: 0.5006 - acc: 0.7590     \n",
      "Epoch 54/150\n",
      "614/614 [==============================] - 0s - loss: 0.4994 - acc: 0.7557     \n",
      "Epoch 55/150\n",
      "614/614 [==============================] - 0s - loss: 0.4981 - acc: 0.7606     \n",
      "Epoch 56/150\n",
      "614/614 [==============================] - 0s - loss: 0.4972 - acc: 0.7622     \n",
      "Epoch 57/150\n",
      "614/614 [==============================] - 0s - loss: 0.4963 - acc: 0.7622     \n",
      "Epoch 58/150\n",
      "614/614 [==============================] - 0s - loss: 0.4957 - acc: 0.7687     \n",
      "Epoch 59/150\n",
      "614/614 [==============================] - 0s - loss: 0.4947 - acc: 0.7638     \n",
      "Epoch 60/150\n",
      "614/614 [==============================] - 0s - loss: 0.4940 - acc: 0.7671     \n",
      "Epoch 61/150\n",
      "614/614 [==============================] - 0s - loss: 0.4928 - acc: 0.7687     \n",
      "Epoch 62/150\n",
      "614/614 [==============================] - 0s - loss: 0.4922 - acc: 0.7687     \n",
      "Epoch 63/150\n",
      "614/614 [==============================] - 0s - loss: 0.4917 - acc: 0.7622     \n",
      "Epoch 64/150\n",
      "614/614 [==============================] - 0s - loss: 0.4910 - acc: 0.7638     \n",
      "Epoch 65/150\n",
      "614/614 [==============================] - 0s - loss: 0.4912 - acc: 0.7638     \n",
      "Epoch 66/150\n",
      "614/614 [==============================] - 0s - loss: 0.4902 - acc: 0.7622     \n",
      "Epoch 67/150\n",
      "614/614 [==============================] - 0s - loss: 0.4895 - acc: 0.7606     \n",
      "Epoch 68/150\n",
      "614/614 [==============================] - 0s - loss: 0.4889 - acc: 0.7606     \n",
      "Epoch 69/150\n",
      "614/614 [==============================] - 0s - loss: 0.4887 - acc: 0.7606     \n",
      "Epoch 70/150\n",
      "614/614 [==============================] - 0s - loss: 0.4881 - acc: 0.7622     \n",
      "Epoch 71/150\n",
      "614/614 [==============================] - 0s - loss: 0.4879 - acc: 0.7638     \n",
      "Epoch 72/150\n",
      "614/614 [==============================] - 0s - loss: 0.4872 - acc: 0.7622     \n",
      "Epoch 73/150\n",
      "614/614 [==============================] - 0s - loss: 0.4874 - acc: 0.7622     \n",
      "Epoch 74/150\n",
      "614/614 [==============================] - 0s - loss: 0.4867 - acc: 0.7638     \n",
      "Epoch 75/150\n",
      "614/614 [==============================] - 0s - loss: 0.4864 - acc: 0.7573     \n",
      "Epoch 76/150\n",
      "614/614 [==============================] - 0s - loss: 0.4859 - acc: 0.7622     \n",
      "Epoch 77/150\n",
      "614/614 [==============================] - 0s - loss: 0.4858 - acc: 0.7590     \n",
      "Epoch 78/150\n",
      "614/614 [==============================] - 0s - loss: 0.4855 - acc: 0.7557     \n",
      "Epoch 79/150\n",
      "614/614 [==============================] - 0s - loss: 0.4854 - acc: 0.7590     \n",
      "Epoch 80/150\n",
      "614/614 [==============================] - 0s - loss: 0.4849 - acc: 0.7606     \n",
      "Epoch 81/150\n",
      "614/614 [==============================] - 0s - loss: 0.4850 - acc: 0.7590     \n",
      "Epoch 82/150\n",
      "614/614 [==============================] - 0s - loss: 0.4844 - acc: 0.7606     \n",
      "Epoch 83/150\n",
      "614/614 [==============================] - 0s - loss: 0.4847 - acc: 0.7606     \n",
      "Epoch 84/150\n",
      "614/614 [==============================] - 0s - loss: 0.4840 - acc: 0.7557     \n",
      "Epoch 85/150\n",
      "614/614 [==============================] - 0s - loss: 0.4846 - acc: 0.7541     \n",
      "Epoch 86/150\n",
      "614/614 [==============================] - 0s - loss: 0.4835 - acc: 0.7541     \n",
      "Epoch 87/150\n",
      "614/614 [==============================] - 0s - loss: 0.4834 - acc: 0.7557     \n",
      "Epoch 88/150\n",
      "614/614 [==============================] - 0s - loss: 0.4839 - acc: 0.7508     \n",
      "Epoch 89/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614/614 [==============================] - 0s - loss: 0.4831 - acc: 0.7557     \n",
      "Epoch 90/150\n",
      "614/614 [==============================] - 0s - loss: 0.4832 - acc: 0.7557     \n",
      "Epoch 91/150\n",
      "614/614 [==============================] - 0s - loss: 0.4827 - acc: 0.7541     \n",
      "Epoch 92/150\n",
      "614/614 [==============================] - 0s - loss: 0.4826 - acc: 0.7557     \n",
      "Epoch 93/150\n",
      "614/614 [==============================] - 0s - loss: 0.4828 - acc: 0.7557     \n",
      "Epoch 94/150\n",
      "614/614 [==============================] - 0s - loss: 0.4825 - acc: 0.7524     \n",
      "Epoch 95/150\n",
      "614/614 [==============================] - 0s - loss: 0.4822 - acc: 0.7573     \n",
      "Epoch 96/150\n",
      "614/614 [==============================] - 0s - loss: 0.4820 - acc: 0.7590     \n",
      "Epoch 97/150\n",
      "614/614 [==============================] - 0s - loss: 0.4823 - acc: 0.7573     \n",
      "Epoch 98/150\n",
      "614/614 [==============================] - 0s - loss: 0.4818 - acc: 0.7541     \n",
      "Epoch 99/150\n",
      "614/614 [==============================] - 0s - loss: 0.4822 - acc: 0.7557     \n",
      "Epoch 100/150\n",
      "614/614 [==============================] - 0s - loss: 0.4817 - acc: 0.7573     \n",
      "Epoch 101/150\n",
      "614/614 [==============================] - 0s - loss: 0.4816 - acc: 0.7573     \n",
      "Epoch 102/150\n",
      "614/614 [==============================] - 0s - loss: 0.4813 - acc: 0.7557     \n",
      "Epoch 103/150\n",
      "614/614 [==============================] - 0s - loss: 0.4813 - acc: 0.7557     \n",
      "Epoch 104/150\n",
      "614/614 [==============================] - 0s - loss: 0.4811 - acc: 0.7557     \n",
      "Epoch 105/150\n",
      "614/614 [==============================] - 0s - loss: 0.4809 - acc: 0.7590     \n",
      "Epoch 106/150\n",
      "614/614 [==============================] - 0s - loss: 0.4811 - acc: 0.7573     \n",
      "Epoch 107/150\n",
      "614/614 [==============================] - 0s - loss: 0.4811 - acc: 0.7573     \n",
      "Epoch 108/150\n",
      "614/614 [==============================] - 0s - loss: 0.4805 - acc: 0.7573     \n",
      "Epoch 109/150\n",
      "614/614 [==============================] - 0s - loss: 0.4806 - acc: 0.7573     \n",
      "Epoch 110/150\n",
      "614/614 [==============================] - 0s - loss: 0.4804 - acc: 0.7557     \n",
      "Epoch 111/150\n",
      "614/614 [==============================] - 0s - loss: 0.4803 - acc: 0.7557     \n",
      "Epoch 112/150\n",
      "614/614 [==============================] - 0s - loss: 0.4801 - acc: 0.7606     \n",
      "Epoch 113/150\n",
      "614/614 [==============================] - 0s - loss: 0.4802 - acc: 0.7590     \n",
      "Epoch 114/150\n",
      "614/614 [==============================] - 0s - loss: 0.4799 - acc: 0.7573     \n",
      "Epoch 115/150\n",
      "614/614 [==============================] - 0s - loss: 0.4799 - acc: 0.7573     \n",
      "Epoch 116/150\n",
      "614/614 [==============================] - 0s - loss: 0.4797 - acc: 0.7606     \n",
      "Epoch 117/150\n",
      "614/614 [==============================] - 0s - loss: 0.4797 - acc: 0.7573     \n",
      "Epoch 118/150\n",
      "614/614 [==============================] - 0s - loss: 0.4800 - acc: 0.7573     \n",
      "Epoch 119/150\n",
      "614/614 [==============================] - 0s - loss: 0.4794 - acc: 0.7573     \n",
      "Epoch 120/150\n",
      "614/614 [==============================] - 0s - loss: 0.4793 - acc: 0.7573     \n",
      "Epoch 121/150\n",
      "614/614 [==============================] - 0s - loss: 0.4790 - acc: 0.7557     \n",
      "Epoch 122/150\n",
      "614/614 [==============================] - 0s - loss: 0.4789 - acc: 0.7541     \n",
      "Epoch 123/150\n",
      "614/614 [==============================] - 0s - loss: 0.4789 - acc: 0.7557     \n",
      "Epoch 124/150\n",
      "614/614 [==============================] - 0s - loss: 0.4786 - acc: 0.7541     \n",
      "Epoch 125/150\n",
      "614/614 [==============================] - 0s - loss: 0.4786 - acc: 0.7541     \n",
      "Epoch 126/150\n",
      "614/614 [==============================] - 0s - loss: 0.4782 - acc: 0.7557     \n",
      "Epoch 127/150\n",
      "614/614 [==============================] - 0s - loss: 0.4780 - acc: 0.7541     \n",
      "Epoch 128/150\n",
      "614/614 [==============================] - 0s - loss: 0.4783 - acc: 0.7557     \n",
      "Epoch 129/150\n",
      "614/614 [==============================] - 0s - loss: 0.4778 - acc: 0.7557     \n",
      "Epoch 130/150\n",
      "614/614 [==============================] - 0s - loss: 0.4778 - acc: 0.7573     \n",
      "Epoch 131/150\n",
      "614/614 [==============================] - 0s - loss: 0.4776 - acc: 0.7557     \n",
      "Epoch 132/150\n",
      "614/614 [==============================] - 0s - loss: 0.4771 - acc: 0.7573     \n",
      "Epoch 133/150\n",
      "614/614 [==============================] - 0s - loss: 0.4771 - acc: 0.7606     \n",
      "Epoch 134/150\n",
      "614/614 [==============================] - 0s - loss: 0.4770 - acc: 0.7606     \n",
      "Epoch 135/150\n",
      "614/614 [==============================] - 0s - loss: 0.4767 - acc: 0.7590     \n",
      "Epoch 136/150\n",
      "614/614 [==============================] - 0s - loss: 0.4769 - acc: 0.7622     \n",
      "Epoch 137/150\n",
      "614/614 [==============================] - 0s - loss: 0.4763 - acc: 0.7590     \n",
      "Epoch 138/150\n",
      "614/614 [==============================] - 0s - loss: 0.4769 - acc: 0.7622     \n",
      "Epoch 139/150\n",
      "614/614 [==============================] - 0s - loss: 0.4759 - acc: 0.7590     \n",
      "Epoch 140/150\n",
      "614/614 [==============================] - 0s - loss: 0.4758 - acc: 0.7573     \n",
      "Epoch 141/150\n",
      "614/614 [==============================] - 0s - loss: 0.4755 - acc: 0.7590     \n",
      "Epoch 142/150\n",
      "614/614 [==============================] - 0s - loss: 0.4754 - acc: 0.7622     \n",
      "Epoch 143/150\n",
      "614/614 [==============================] - 0s - loss: 0.4753 - acc: 0.7622     \n",
      "Epoch 144/150\n",
      "614/614 [==============================] - 0s - loss: 0.4751 - acc: 0.7606     \n",
      "Epoch 145/150\n",
      "614/614 [==============================] - 0s - loss: 0.4749 - acc: 0.7622     \n",
      "Epoch 146/150\n",
      "614/614 [==============================] - 0s - loss: 0.4743 - acc: 0.7606     \n",
      "Epoch 147/150\n",
      "614/614 [==============================] - 0s - loss: 0.4744 - acc: 0.7606     \n",
      "Epoch 148/150\n",
      "614/614 [==============================] - 0s - loss: 0.4739 - acc: 0.7622     \n",
      "Epoch 149/150\n",
      "614/614 [==============================] - 0s - loss: 0.4738 - acc: 0.7606     \n",
      "Epoch 150/150\n",
      "614/614 [==============================] - 0s - loss: 0.4736 - acc: 0.7606     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('nn', <keras.wrappers.scikit_learn.KerasClassifier object at 0x1a1da81850>)])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y=y_train, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/154 [==========================>...] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "y_hat = model.predict(X_test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7857142857142857"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_test == y_hat).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем строить регрессию на данных [Boston Housing Data Set](https://archive.ics.uci.edu/ml/datasets/Housing)\n",
    "\n",
    "Описание:<br\\>\n",
    "1. CRIM: per capita crime rate by town\n",
    "2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "3. INDUS: proportion of non-retail business acres per town\n",
    "4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "5. NOX: nitric oxides concentration (parts per 10 million)\n",
    "6. RM: average number of rooms per dwelling\n",
    "7. AGE: proportion of owner-occupied units built prior to 1940\n",
    "8. DIS: weighted distances to five Boston employment centres\n",
    "9. RAD: index of accessibility to radial highways\n",
    "10. TAX: full-value property-tax rate per 10,000 USD\n",
    "11. PTRATIO: pupil-teacher ratio by town\n",
    "12. B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "13. LSTAT: lower status of the population\n",
    "14. MEDV: Median value of owner-occupied homes in 1000's USD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Весь pipeline практически повторяется. Ну и повторим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "217px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
