{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/logo_hse_black.jpg\"></center>\n",
    "\n",
    "<h1><center>Data Analysis</center></h1>\n",
    "<h2><center>Seminar: Neural Networks 1</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Почему *обратное* распространение?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обозначения и затравочка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим такой пример нейронной сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/nn.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $x_i$ - входной слой. Сюда подаются признаки объектов. $i$ - это индексы признаков объекта\n",
    "* $z_j$, $z_k$ - скрытые слои.\n",
    "* $z_m$ - выходной слой. Тут выводятся результаты вычислений сети. $j$, $k$, $m$ - индексы нейронов в этих слоях\n",
    "* $w_{\\cdot,\\cdot}$ - веса, связывающие нейроны соседних слоев\n",
    "\n",
    "Дополнительно введем следующие обозначения:\n",
    "* $net^{(1)}_j = net_j = \\sum_iw_{ij}x_i$ \n",
    "* $net^{(2)}_k = net_k =  \\sum_jw_{jk}z_j$ \n",
    "* $net^{(3)}_m = net_m \\sum_kw_{km}z_k$\n",
    "\n",
    "*Верхние индексы мы пока опустим, но в части про векторизацию к ним вернемся*\n",
    "\n",
    "\n",
    "Так же:\n",
    "* $z_j = f^{(h)}(net_j)$ \n",
    "* $z_k = f^{(k)}(net_k)$\n",
    "* $z_m = f^{(r)}(net_m)$\n",
    "\n",
    "где $f(\\cdot)$ - некоторая функция активации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам так же понадобится функция потерь $L(W)$, которая зависит от весов $w_{\\cdot, \\cdot}$.\n",
    "\n",
    "Если бы мы хотели обучаться с помощью простого градиентного спуска, то нам нужно было на каждом шаге расчитывать\n",
    "частные производные по всем весам: \n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_{ij}}$, $\\frac{\\partial L}{\\partial w_{jk}}$, $\\frac{\\partial L}{\\partial w_{km}}$\n",
    "\n",
    "Достаточно выписать вычисления нейронной сети в виде формулы, чтобы понять, что \"такой футбол нам не нужен\" и хоть и вычислять эти производные можно, это займет много времени. Есть ли способ как-то ускорить расчет производных?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Идея метода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поступим следующим образом - случайно инициализируем начальные веса и пропустим один объект $(x(t), y(t))$ по нейронной сети с прямом направлении (forward pass). Получим ответ нейронной сети на одном из выходов $z_m = \\hat{y}(t)$.\n",
    "\n",
    "Посчитаем $\\frac{\\partial L}{\\partial w_{km}}$, воспользовавшись правилами дифференцирования сложных функций:\n",
    "\n",
    "$$\n",
    "\\begin{align*} \n",
    "\\frac{\\partial L}{\\partial w_{km}} &=  \\underline{\\frac{\\partial L}{\\partial z_m} \\cdot \\frac{\\partial z_m}{\\partial net_m}} \\cdot \\frac{\\partial net_m}{\\partial w_{km}} \\\\ \n",
    " &= \\underline{\\frac{\\partial L}{\\partial z_m} \\cdot f^{(3)`}(net_m)} \\cdot z_k\n",
    "\\end{align*}\n",
    "$$\n",
    "*Первые два члена выделены неслучайно!*\n",
    "\n",
    "Okey, пока ничего особенного. Переходим на слой назад:\n",
    "$$\n",
    "\\begin{align} \n",
    "\\frac{\\partial L}{\\partial w_{jk}} &=  (\\frac{\\partial L}{\\partial z_k}) & \\cdot & \\frac{\\partial z_k}{\\partial net_k} & \\cdot & \\frac{\\partial net_k}{\\partial w_{jk}} \\\\ \n",
    " &= \\left(\\sum_m\\underline{\\frac{\\partial L}{\\partial z_m} \\cdot \\frac{\\partial z_m}{\\partial net_m}} \\cdot \\frac{\\partial net_m}{\\partial z_k}\\right) & \\cdot & f^{(2)`}(net_k) & \\cdot & z_j \\\\\n",
    " &= \\left(\\sum_m\\underline{\\frac{\\partial L}{\\partial z_m} \\cdot f^{(3)`}(net_m)} \\cdot w_{km}\\right) & \\cdot & f^{(2)`}(net_k) & \\cdot & z_j \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "*Надеюсь форматирование вас не запутало*\n",
    "\n",
    "Что же мы видим? Некоторую часть выражения для $\\frac{\\partial L}{\\partial w_{jk}}$ мы уже считали на слое выше! Причем эта часть отвечает именно за производную ошибки по выходу сети! Более того, эта часть входит с весом $ w_{km}$, который как раз связывает эти слои. Таким образом, можно сказать, что ошибка начинает *распростроняться в обратном направлении* по слоям, пропоционально весам между слоями. \n",
    "\n",
    "Отсюда и название метода.\n",
    "\n",
    "Eсли мы продолжим вывод для весов $w_{ij}$, то так же заметим там часть, ранее посчитанную на этапе с весами $w_{jk}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обобщение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Становится ясно, что можно как-то записать вычисление в \"рекурсивном\" виде между слоями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть один объект прошел по сети с результатом $\\hat{y}(t)$.\n",
    "\n",
    "Рассмотрим выражение для нейрона $l$\n",
    "$$ \\delta_l = \\frac{\\partial L}{\\partial net_l} = \\frac{\\partial L}{\\partial \\hat{y}(t)} \\cdot \\frac{\\partial \\hat{y}(t)}{\\partial net_l}$$\n",
    "и назовем это **сигнал об ошибке**, проходящий через нейрон $l$.\n",
    "\n",
    "Если нейрон $l$ находится на выходном слое, то просто считаем $\\frac{\\partial L}{\\partial \\hat{y}(t)}$, а $\\frac{\\partial \\hat{y}(t)}{\\partial net_l} = \\frac{\\partial z_l}{\\partial net_l} = f^{`}(net_l)$\n",
    "\n",
    "Eсли нейрон $l$ находится в промежуточном слое, то $\\frac{\\partial \\hat{y}(t)}{\\partial net_l}$ надо расписать через нейроны следующего (в направлении выхода сети) слоя:\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "\\frac{\\partial \\hat{y}(t)}{\\partial net_l} &= \\frac{\\partial \\hat{y}(t)}{\\partial z_l} & \\cdot \\frac{\\partial z_l}{\\partial net_l} \\\\ \n",
    "&= \\sum_{o \\in out(l)} \\left(\\frac{\\partial \\hat{y}(t)}{\\partial net_o} \\cdot \\frac{\\partial net_o}{\\partial z_l} \\right) & \\cdot f^{`}(net_l) \\\\\n",
    "&= \\sum_{o \\in out(l)} \\left(\\frac{\\partial \\hat{y}(t)}{\\partial net_o} \\cdot w_{lo} \\right) & \\cdot f^{`}(net_l) \n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "Теперь доставим это выражение в \"определение\" $\\delta_l$:\n",
    "$$\n",
    "\\begin{align} \n",
    "\\delta_l &=  \\frac{\\partial L}{\\partial \\hat{y}(t)} \\cdot \\frac{\\partial \\hat{y}(t)}{\\partial net_l} \\\\ \n",
    "&= \\frac{\\partial L}{\\partial \\hat{y}(t)} \\cdot \\sum_{o \\in out(l)} \\left(\\frac{\\partial \\hat{y}(t)}{\\partial net_o} \\cdot w_{lo} \\right) \\cdot f^{`}(net_l) \\\\\n",
    "&=  \\sum_{o \\in out(l)} \\left(\\frac{\\partial L}{\\partial \\hat{y}(t)} \\cdot \\frac{\\partial \\hat{y}(t)}{\\partial net_o} \\cdot w_{lo} \\right) \\cdot f^{`}(net_l) \\\\\n",
    "&=  \\sum_{o \\in out(l)} \\left(\\delta_o \\cdot w_{lo} \\right) \\cdot f^{`}(net_l) \\\\\n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "Красота!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В итоге - шаги алгоритма backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже преставленны шаги алгоритма обратного распространения ошибки с проходом одним случайным объектом. Проход всеми объектами выборки или батчем получается из данной записи очевидным образом.\n",
    "\n",
    "1. Некоторым образом инициализируем все веса сети\n",
    "2. Прогоняем случайный объект $x(t)$ по сети, попутно запоминая все $net_l$ и $z_l$ для всех слоев\n",
    "3. Считаем $\\delta_l$: если слой  $l$ выходной, то \n",
    "$$ \\delta_l = f^{(l)`}(net_l) \\frac{\\partial L}{\\partial \\hat{y}(t)} $$\n",
    "иначе \n",
    "$$ \\delta_l = f^{(l)`}(net_l) \\sum_{o \\in out(l)} \\delta_o w_{lo}$$\n",
    "4. Обновляем веса по следующему правилу\n",
    "$$ w_{jk} = w_{jk} - \\alpha \\cdot \\delta_k \\cdot z_j $$\n",
    "5. Если правило останова не выполнено, перейти к шагу 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Прямой проход"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Понятно, что операции, которые проходят в нейронной сети можно векторизовать.\n",
    "\n",
    "Например, умножение на веса и переход к следующему слою можно просто расписать в виде умножения на матрицу:\n",
    "\n",
    "$$\\left\\{net^{(1)}_j\\right\\}^{J}_{j=1} = \\left\\{ \\sum_iw_{ij}x_i \\right\\}^{J}_{j=1} = W^{i \\rightarrow j}x = net^{(1)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eсли же мы хотим применить какую-то функцию активации $f(\\cdot)$, то смело поэлементно применяем ее к $net^{(1)}$\n",
    "\n",
    "$$ Z^{(1)} = f(net^{(1)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/vect.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Таким образом, все что происходит в полносвязной сети можно реализовать с помощью матричных операций.\n",
    "Например, сеть с двумя слоями с сигмойдными функциями активации записывается как\n",
    "\n",
    "$$y = \\sigma(W_2\\sigma(W_1x)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обратный проход"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем сигнал об ошибке\n",
    "$$ \\left\\{\\delta_l\\right\\}^{L}_{l=1} = \\left\\{ f^{(l)`}(net_l) \\sum_{o \\in out(l)} \\delta_o w_{lo} \\right\\}^{L}_{l=1}$$\n",
    "в векторизованном виде:\n",
    "\n",
    "$$\n",
    "\\Delta^{(1)} = f^{`}(net^{(1)}) \\odot \\Delta^{(2)} W^{(1 \\rightarrow 2)}\n",
    "$$\n",
    "а для выходново слоя\n",
    "$$\n",
    "\\Delta^{(out)} = f^{`}(net^{(out)}) \\odot \\frac{\\partial L}{\\partial \\hat{y}}\n",
    "$$\n",
    "\n",
    "\n",
    "Тогда веса следует обновлять так:\n",
    "\n",
    "$$ W^{(1 \\rightarrow 2)} = W^{(1 \\rightarrow 2)} - \\alpha Z^{{(1)}^\\top} \\Delta^{(2)} $$\n",
    "\n",
    "И так для всех слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/back.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Основано на материале из [cs231n](http://cs231n.github.io/neural-networks-case-study/).\n",
    "\n",
    "Возьмем какой-нибудь простой датасет для классификации и обучим на нем нейронную сеть с одним скрытым слоем с функцией активации ReLU. <br\\>\n",
    "На выходе, расчеты должны пройти через SoftMax.\n",
    "$$z_l = \\frac{\\exp(net_l)}{\\sum_l \\exp(net_l)} \\text{ - это softmax}$$\n",
    "Таким образом, нейронная сеть должна для каждого класса возвращать вероятность принадлежности ему объекта, который через нее прошел.\n",
    "\n",
    "Ошибка в нашем случае будет выражаться как кросс-энтропия. Величину ошибки на одном объекте вычиляем так\n",
    "$$L(\\hat{y}(t)) = - \\sum\\limits_{c \\in classes} y_c\\log(\\hat{y}_c)$$\n",
    "Общая ошибка с регуляризацией:\n",
    "$$L = \\frac{1}{N}\\sum\\limits_{t=1}^N L(\\hat{y}(t)) + \\lambda\\frac{1}{2}\\|\\mathbf{W}\\|^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100 # кол-во объектов\n",
    "D = 2 # кол-во признаков\n",
    "K = 2 # кол-во классов\n",
    "X, y = make_moons(n_samples=N, noise=0.1, random_state=123)\n",
    "C = 0.01 # гиперпараметр регуляризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удобно сделать one-hot encoding для кодирования истинной метки классов объектов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bool = np.zeros((N, K))\n",
    "y_bool[range(N), y] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задаем колиество нейронов в первом слое и веса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 50 \n",
    "# Матрица весов между \n",
    "# входным слоем и промежуточным скрытым\n",
    "W = 0.01 * np.random.randn(D,h) \n",
    "b = np.zeros((1,h)) # св член\n",
    "\n",
    "# Матрица между промежуточным\n",
    "# скрытым слоем и выходным слоем\n",
    "W2 = 0.01 * np.random.randn(h,K) \n",
    "b2 = np.zeros((1,K)) # св член"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Прямой проход"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Прогоняем всю выборку через сеть\n",
    "hidden_layer = np.maximum(0, np.dot(X, W) + b) \n",
    "scores = np.dot(hidden_layer, W2) + b2 # \"Сырой\" выход сети\n",
    "probs = np.exp(scores)/np.exp(scores).sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = -(y_bool * np.log(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_error = errors.sum()\n",
    "print(total_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что на каждом сле у нас есть свободный член b и b2. У них тоже есть веса и их надо учитывать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обратный проход"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начинаем с того, что с считаем $\\Delta^{(out)}$ на выходном слое.\n",
    "\n",
    "Оказывется, что в случае с softmax и кросс-энтропией $\\Delta^{(out)}$ будет выражаться очень просто!\n",
    "\n",
    "$$ \\Delta^{(out)} = \\hat{y} - y $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dscores = probs - y_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dscores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом градиент на веса между выходным и скрытым слоем выражается как \n",
    "\n",
    "$$\\Delta W_2 = Z^{{(1)}^\\top}\\Delta^{(out)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# производные по весам\n",
    "# между выходным и конечным слоем\n",
    "dW2 = np.dot(hidden_layer.T, dscores) + C*W2 # Не забываем регуляризацию\n",
    "\n",
    "# производная по весам для свободного члена\n",
    "db2 = np.sum(dscores, axis=0, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идем дальше\n",
    "\n",
    "$$\\Delta^{(1)} = f^{`}(net^{(1)}) \\odot D^{(2)} W^{(1 \\rightarrow 2)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Это просто пошагово\n",
    "# dhidden_rpart = np.dot(dscores, W2.T) # Правая часть выражения\n",
    "# dhidden_lpart = (hidden_layer <= 0).astype(int) # Производная ReLU\n",
    "# dhidden = dhidden_lpart * dhidden_rpart # Результат\n",
    "\n",
    "dhidden = np.dot(dscores, W2.T)\n",
    "dhidden[hidden_layer <= 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом градитент по весам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW = np.dot(X.T, dhidden) + C*W\n",
    "db = np.sum(dhidden, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот и все, дальше нужно итеративно обновлять веса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RND_SEED = 7\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "numpy.random.seed(RND_SEED)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для тренеровки мы будем использовать достаточно известный набор данных [Pima Indians](https://www.kaggle.com/uciml/pima-indians-diabetes-database/data).\n",
    "\n",
    "Признаки такие: <br\\>\n",
    "1. Number of times pregnant\n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "3. Diastolic blood pressure (mm Hg)\n",
    "4. Triceps skin fold thickness (mm)\n",
    "5. 2-Hour serum insulin (mu U/ml)\n",
    "6. Body mass index (weight in kg/(height in m)^2)\n",
    "7. Diabetes pedigree function\n",
    "8. Age (years)\n",
    "9. Class variable (0 or 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/diabetes.csv',\n",
    "                 sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.iloc[:, :8].values, df.iloc[:, 8].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=RND_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Составляем модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим простую 2х (3х) слойную нейронную сеть. Делать это в keras одно удовольствие"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, init='normal', activation='sigmoid'))\n",
    "model.add(Dense(8, init='normal', activation='sigmoid'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь надо задать функцию [ошибки](https://keras.io/objectives/), способ [оптимизации](https://keras.io/optimizers/) и метрику измерения [качества](https://keras.io/metrics/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, nb_epoch=150, batch_size=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем строить регрессию на данных [Boston Housing Data Set](https://archive.ics.uci.edu/ml/datasets/Housing)\n",
    "\n",
    "Описание:<br\\>\n",
    "1. CRIM: per capita crime rate by town\n",
    "2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "3. INDUS: proportion of non-retail business acres per town\n",
    "4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "5. NOX: nitric oxides concentration (parts per 10 million)\n",
    "6. RM: average number of rooms per dwelling\n",
    "7. AGE: proportion of owner-occupied units built prior to 1940\n",
    "8. DIS: weighted distances to five Boston employment centres\n",
    "9. RAD: index of accessibility to radial highways\n",
    "10. TAX: full-value property-tax rate per 10,000 USD\n",
    "11. PTRATIO: pupil-teacher ratio by town\n",
    "12. B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "13. LSTAT: lower status of the population\n",
    "14. MEDV: Median value of owner-occupied homes in 1000's USD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Весь pipeline практически повторяется. Ну и повторим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "217px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
