{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"img/logo_hse_black.jpg\"></center>\n",
    "\n",
    "<h1><center>Data Analysis</center></h1>\n",
    "<h3><center>Andrey Shestakov (<a href=\"mailto:avshestakov@hse.ru\">avshestakov@hse.ru</a>)</center></h3>\n",
    "<hr>\n",
    "<h2><center>Clustering 2: Quality and Mixture models<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup></center></h2>\n",
    "\n",
    "\n",
    "\n",
    "<sup id=\"fn1\">1. Some materials are taken from <a href=\"http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5_%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B_%D1%80%D0%B0%D1%81%D0%BF%D0%BE%D0%B7%D0%BD%D0%B0%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F_%D0%BE%D0%B1%D1%80%D0%B0%D0%B7%D0%BE%D0%B2_%28%D0%BA%D1%83%D1%80%D1%81_%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D0%B9%2C_%D0%92.%D0%92.%D0%9A%D0%B8%D1%82%D0%BE%D0%B2%29\">machine learning course of Victor Kitov</a></sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('seaborn-talk')\n",
    "plt.rcParams['figure.figsize'] = (16,8)\n",
    "\n",
    "# Для кириллицы на графиках\n",
    "font = {'family': 'Verdana',\n",
    "        'weight': 'normal'}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "from ipywidgets import interact, IntSlider, fixed, FloatSlider\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's recall previous lecture\n",
    "\n",
    "## Clustering\n",
    "\n",
    "* Unsupervised learning\n",
    "    * Group similar objects, separate dissimilar\n",
    "* Wide range of methods\n",
    "* How do we know the quality of clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cluster Validity and Quality Measures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## General approaches\n",
    "* Evaluate using \"ground-truth\" clustering (Quality Measure)\n",
    "    * invariant to cluster naming\n",
    "* Unsupervised criterion (Cluster Validity)\n",
    "    * based on intuition:\n",
    "        * objects from same cluster should be similar\n",
    "        * objects from different clusters should be different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Based on ground-truth\n",
    "#### Rand Index\n",
    "\n",
    "$$ \\text{Rand}(\\hat{\\pi},\\pi^*) = \\frac{a + d}{a + b + c + d} \\text{,}$$\n",
    "where \n",
    "* $a$ - number of pairs that are grouped both in $\\hat{\\pi}$ and\n",
    "* $d$ - number of pairs that are separated both in $\\hat{\\pi}$ and $\\pi^*$\n",
    "$\\pi^*$, \n",
    "* $b$ ($c$) - number of pairs that are separated both in $\\hat{\\pi}$ ($\\pi^*$), but grouped in  $\\pi^*$ ($\\hat{\\pi}$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Rand Index\n",
    "\n",
    "$$ \\text{Rand}(\\hat{\\pi},\\pi^*) = \\frac{tp + tn}{tp + fp + fn + tn} \\text{,}$$\n",
    "where \n",
    "* $tp$ - number of pairs that are grouped both in $\\hat{\\pi}$ and\n",
    "* $tn$ - number of pairs that are separated both in $\\hat{\\pi}$ and $\\pi^*$\n",
    "$\\pi^*$, \n",
    "* $fp$ ($fn$) - number of pairs that are separated both in $\\hat{\\pi}$ ($\\pi^*$), but grouped in  $\\pi^*$ ($\\hat{\\pi}$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Adjusted Rand Index**\n",
    "\n",
    "$$\\text{ARI}(\\hat{\\pi},\\pi^*)   = \\frac{\\text{Rand}(\\hat{\\pi},\\pi^*) - \\text{Expected}}{\\text{Max} - \\text{Expected}}$$\n",
    "\n",
    "Check wikipedia =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Precision, Recall, F-measure\n",
    "* $\\text{Precision}(\\hat{\\pi},\\pi^*)  = \\frac{tp}{tp+fn}$\n",
    "* $\\text{Recall}(\\hat{\\pi},\\pi^*)  = \\frac{tp}{tp+fp}$\n",
    "* $\\text{F-measure}(\\hat{\\pi},\\pi^*)  = \\frac{2\\cdot Precision \\cdot Recall}{Precision + Recall}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cluster validity\n",
    "* Intuition\n",
    "    * objects from same cluster should be similar\n",
    "    * objects from different clusters should be different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Silhouette\n",
    "\n",
    "For each object $x_{i}$ define:\n",
    "\n",
    "* $s_{i}$-mean distance to objects in the same cluster\n",
    "* $d_{i}$-mean distance to objects in the next nearest cluster\n",
    "Silhouette coefficient for $x_{i}$:\n",
    "$$\n",
    "Silhouette_{i}=\\frac{d_{i}-s_{i}}{\\max\\{d_{i},s_{i}\\}}\n",
    "$$\n",
    "\n",
    "Silhouette coefficient for $x_{1},...x_{N}$:\n",
    "$$\n",
    "Silhouette=\\frac{1}{N}\\sum_{i=1}^{N}\\frac{d_{i}-s_{i}}{\\max\\{d_{i},s_{i}\\}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='http://scikit-learn.org/stable/_images/sphx_glr_plot_kmeans_silhouette_analysis_003.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Discussion\n",
    "* Advantages\n",
    "    * The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. \n",
    "    * Scores around zero indicate overlapping clusters. \n",
    "    * The score is higher when clusters are dense and well separated.\n",
    "\n",
    "* Disadvantages\n",
    "    * complexity $O(N^{2}D)$ \n",
    "    * favours convex clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### FYI\n",
    "* [Quality measures review 1](http://www.sciencedirect.com/science/article/pii/S003132031200338X)\n",
    "* [Quality measures review 2](https://cran.r-project.org/web/packages/clusterCrit/vignettes/clusterCrit.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mixture models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sample density\n",
    "\n",
    "<center><img src='img/normal_hist.png' width=800></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Parametric density approximation\n",
    "\n",
    "It can be accurately modelled with existing parametric family - Normal\n",
    "\n",
    "$$ \n",
    "p(x | \\theta) = \\mathcal{N}(x|\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left({-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\right)\n",
    "$$\n",
    "or shall we..\n",
    "$$ \n",
    "p(x | \\theta) = \\mathcal{N}(x|\\mu, \\Sigma) = \\frac{1}{(2\\pi\\Sigma)^{1/2}}\\exp\\left(-\\frac{1}{2}(x-\\mu)^\\top\\Sigma^{-1}(x-\\mu)\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "<center><img src='img/normal_hist_fitted.png' width=800></cetner>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ML Method for estimating parameters\n",
    "\n",
    "* Consider (log of) likelihood\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(x) = & \\prod\\limits_{i=1}^B\\mathcal{N}(x_i|\\mu, \\Sigma)\\rightarrow \\max\\limits_{\\mu, \\Sigma}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* Take derivatives and equate them to zero\n",
    "\n",
    "$$\\mu_{ML} = \\frac 1 N \\sum_{i=1}^N x_i, \\quad \\mathbf{\\Sigma}_{ML} = \\frac 1 N \\sum_{i=1}^N (x_i - \\mu_{ML}) (x_i - \\mu_{ML})^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Non-standard sample distribution\n",
    "\n",
    "What to do if no parametric model fits well?\n",
    "\n",
    "<center><img src='img/multi-modal hist.png' width=800></cetner>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mixture models\n",
    "\n",
    "$$\n",
    "p(x)=\\sum_{z=1}^{Z}\\phi_{z}p(x|\\theta_{z})\n",
    "$$\n",
    "\n",
    "* $Z$ - number of components\n",
    "* $\\phi_{z},\\,z=1,2,...Z$ - mixture component probabilities, $\\phi_{z}\\ge0,\\,\\sum_{z=1}^{Z}\\phi_{z}=1$\n",
    "* $p(x|\\theta_{z})$ - component density functions\n",
    "* Parameters of mixture model $\\Theta=\\{\\phi_{z},\\theta_{z},\\,z=1,2,...Z\\}$\n",
    "\n",
    "\n",
    "$p(x|\\theta_{z})$ may be of single or different parametric families.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mixture of Gaussians\n",
    "\n",
    "Gaussians model continious r.v. on $(-\\infty,+\\infty)$.\n",
    "\n",
    "$$p(x|\\theta_{z})=N(x|\\mu_{z},\\Sigma_{z}),\\,\\theta_{z}=\\{\\mu_{z},\\Sigma_{z}\\}$$\n",
    "\n",
    "$$\n",
    "p(x)=\\sum_{z=1}^{Z}\\phi_{z}N(x|\\mu_{z},\\Sigma_{z})\n",
    "$$\n",
    "\n",
    "<center><img src=\"img/multi-modal_hist_fitted.png\" width=800></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Problems\n",
    "\n",
    "* Again consider (log of) likelihood\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(x) = & \\sum_{i=1}^N \\log p(x_i)  = \\sum_{i=1}^N\\log\\left(\\sum_{z=1}^{Z}\\phi_{z}N(x|\\mu_{z},\\Sigma_{z})\\right) \\rightarrow \\max\\limits_{\\mu_z, \\Sigma_z, \\phi_z}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* No closed form solution..\n",
    "* Use EM-algorithm!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Expectation-Maximization\n",
    "\n",
    "* Let's say we know (fixed) all $\\mu_k$-s, $\\Sigma_k$-s and $\\phi_k$-s.\n",
    "* Introduce posteriour probabilty of object $x_i$ belonging to component $z$:\n",
    "$$\n",
    "w_{iz} = p(z | x) = \\frac{p(z) p(x_i | z)}{\\sum_{k=1}^K p(k) p(x_i | k)} = \\frac{\\phi_z \\mathcal{N}(x_i | \\mu_z, \\Sigma_z)}{\\sum_{k=1}^K \\phi_k \\mathcal{N}(x_i | \\mu_k, \\Sigma_k)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_k w_{ik} = 1\n",
    "$$\n",
    "\n",
    "* Now with given soft assignments re-estimate the parameters:\n",
    "$$\n",
    "\\phi_k = \\frac{ \\sum_{i=1}^N w_{ik} }{N}, \\;\\; \\mu_k = \\frac{\\sum_{i=1}^N w_{ik} x_i}{\\sum_{i=1}^N w_{ik}}\n",
    "$$\n",
    "$$\n",
    "\\Sigma_k = \\frac {\\sum_{i=1}^N w_{ik} (x_i - \\mu_k)^\\top (x_i - \\mu_k)}{\\sum_{i=1}^N w_{ik}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Expectation-Maximization\n",
    "\n",
    "<center><img src='img/em-alg.png' width=800></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Does that somehow correspont to k-means algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## EM illustration\n",
    "\n",
    "<center><img src='img/em-illustr.png' width=800></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## K-means versus EM clustering\n",
    "\n",
    "\n",
    "* For each $x_{i}$ EM algorithm gives $w_{iz}=p(z|x_{i})$.\n",
    "* This is soft or probabilistic clustering into $Z$ clusters, having priors $\\phi_{1},...\\phi_{Z}$ and probability distributions $p(x;\\theta_{1}),...p(x;\\theta_{Z})$.\n",
    "* We can make it hard clustering using $z_{i}=\\arg\\max_{z}w_{iz}$.\n",
    "\n",
    "* EM clustering becomes K-means clustering when:\n",
    "    * applied to Gaussians\n",
    "    * with equal priors\n",
    "    * with unity covariance matrices\n",
    "    * with hard clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Initialization for Gaussian mixture EM\n",
    "\n",
    "* Fit K-means to $x_{1},x_{2},...x_{N}$, obtain cluster centers $\\mu_{z},\\,z=1,2,...Z$ and cluster assignments $z_{1},z_{2},...z_{N}$.\n",
    "\\item Initialize mixture probabilities \n",
    "$$\n",
    "\\widehat{\\phi}_{z}\\propto\\sum_{i=1}^{N}\\mathbb{I}[z_{i}=z]\n",
    "$$\n",
    "* Initialize Gaussian means with cluster centers $\\mu_{z},\\,z=1,2,...Z$.\n",
    "* Initialize Gaussian covariance matrices with \n",
    "$$\n",
    "\\widehat{\\Sigma}_{z}=\\frac{1}{\\sum_{i=1}^{N}\\mathbb{I}[z_{i}=z]}\\sum_{n=1}^{N}\\mathbb{I}[z_{i}=z]\\left(x_{i}-\\mu_{z}\\right)\\left(x_{i}-\\mu_{z}\\right)^{T}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Properties of EM\n",
    "\n",
    "* Many local optima exist\n",
    "    * in particular likelihood$\\to\\infty$ when $\\mu_{z}=x_{i}$ and $\\sigma_{z}\\to0$\n",
    "* Only local optimum is found with EM\n",
    "* Results depends on initialization\n",
    "    * It is common to run algorithm multiple times with different initializations and then select the result maximizing the likelihood function.\n",
    "* Number of components may be selected with:\\pause\n",
    "    * cross-validation on the final task\n",
    "    * out-of-sample maximum likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simplifications of Gaussian mixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Simplifications of Gaussian mixtures\n",
    "\n",
    "* $\\Sigma_{z}\\in\\mathbb{R}^{DxD}$ requires $\\frac{D(D+1)}{2}$ parameters.\n",
    "* Covariance matrices for $Z$ components require $Z\\frac{D(D+1)}{2}$ parameters.\n",
    "* Components can be poorly identified when \n",
    "    * $Z\\frac{D(D+1)}{2}$ is large compared to $N$\n",
    "    * when components are not well separated\n",
    "* In these cases we can impose restrictions on covariance matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unrestricted covariance matrices\n",
    "\n",
    "* full covariance matrices $\\Sigma_{z},\\,z=1,2,...Z$.\n",
    "<center><img src=\"img/full covariance matrices.png\" width=800></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Common covariance matrix\n",
    "\n",
    "* $\\Sigma_{1}=\\Sigma_{2}=...=\\Sigma_{Z}$\n",
    "<center><img src=\"img/common covariance matrices.png\" width=800></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Diagonal covariance matrices\n",
    "\n",
    "* $\\Sigma_{z}=\\text{diag}\\{\\sigma_{z,1}^{2},\\sigma_{z,2}^{2}...\\sigma_{z,D}^{2}\\}$\n",
    "<center><img src=\"img/diagonal covariance matrices.png\" width=800></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spherical matrices\n",
    "\n",
    "* $\\Sigma_{z}=\\sigma_{z}^{2}I$, $I\\in\\mathbb{R}^{DxD}$ - identity matrix\n",
    "<center><img src=\"img/spherical covariance matrices.png\" width=800></center>"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/1f8c4751e12938961e423759861e6e5a"
  },
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "gist": {
   "data": {
    "description": "CloudMail/hse-da-course/raw/lecture-intro/lecture-intro-v01.ipynb",
    "public": false
   },
   "id": "1f8c4751e12938961e423759861e6e5a"
  },
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "livereveal": {
   "theme": "serif",
   "transition": "concave",
   "width": "1024px"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "513px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "toc_position": {
   "height": "32px",
   "left": "9px",
   "right": "1379px",
   "top": "33px",
   "width": "212px"
  },
  "widgets": {
   "state": {
    "54e80d57f79b4bfc934a2b84cf5fe7ba": {
     "views": [
      {
       "cell_index": 47
      }
     ]
    },
    "5fb17a3592634a4fba98446dacd6db43": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "6f6f6ce7b81743308b92966f225862a8": {
     "views": [
      {
       "cell_index": 34
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
