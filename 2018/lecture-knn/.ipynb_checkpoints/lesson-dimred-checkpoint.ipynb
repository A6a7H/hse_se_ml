{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Современный анализ данных 13/01/2018\n",
    "\n",
    "##  Методы обучение без учителя\n",
    "### Понижение размерности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,5)\n",
    "\n",
    "# Для кириллицы на графиках\n",
    "font = {'family': 'Verdana',\n",
    "        'weight': 'normal'}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "try:\n",
    "    from ipywidgets import interact, IntSlider, fixed, FloatSlider\n",
    "except ImportError:\n",
    "    print u'Так надо'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Методы уменьшения размерности признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/curse.png' width=800></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Проклятье размерности\n",
    "\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-031e\">$d=2$<img src='https://jeremykun.files.wordpress.com/2016/01/2d-distances.png' width=400></th>\n",
    "    <th class=\"tg-031e\">$d=2 \\dots 100$<img src='https://jeremykun.files.wordpress.com/2016/01/distances-animation.gif' width=400></th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "$$ \\lim_{d \\rightarrow \\infty} \\frac{\\text{dist}_{max} - \\text{dist}_{min}}{\\text{dist}_{min}} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Проклятье размености\n",
    "\n",
    "<center><img src='images/curse_2.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Способы понижения размерности\n",
    "\n",
    "Избавляться от размерности можно методами **отбора признаков (Feature Selection)** и методами **уменьшения размерности (Feature Reduction)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Feature Selection (для Supervised Models)\n",
    "Методы деляться на следующие группы:\n",
    "* Filter methods \n",
    "    * Признаки рассматриваются независимо друг от друга\n",
    "    * Изучается индивидуальный \"вклад\" призника в предсказываемую переменную\n",
    "    * Быстрое вычисление\n",
    "    * *Пример?*\n",
    "* Wrapper methods\n",
    "    * Идет отбор группы признаков\n",
    "    * Может быть оооочень медленным, но качество, обычно, лучше чем у Filter Methods\n",
    "    * Примеры: Stepwise feature selection for regression, [Boruta Algorithm](https://www.google.ru/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=0ahUKEwif5biy-fTWAhXkYJoKHbdxCLAQFgg2MAE&url=https%3A%2F%2Fwww.jstatsoft.org%2Farticle%2Fview%2Fv036i11%2Fv36i11.pdf&usg=AOvVaw3tyiHN0BCe2fkkAA6xEVDE)\n",
    "* Embedded methods\n",
    "    * Отбор признаков \"зашит\" в модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Filter method - Mutual Information\n",
    "$$MI(y,x) = \\sum_{x,y} p(x,y) \\ln\\left[\\frac{p(x,y)}{p(x)p(y)}\\right]$$\n",
    "Сколько информации $x$ сообщает об $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_titanic = pd.read_csv('titanic.csv')\n",
    "df_titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15087048925218183"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "#pd.crosstab(df_titanic.Survived, df_titanic.Sex, normalize=True)\n",
    "\n",
    "#mutual_info_score(df_titanic.Survived, df_titanic.Sex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Wrapper Methods - Recursive Feature Elimination\n",
    "\n",
    "При данном подходе из линейной модели последовательно удаляются признаки с наименьшим коэффициентом\n",
    "\n",
    "Выбор итоговой модели можно производить по одному из следующих критериев, оценивающих сложность и точность модели:\n",
    "* [AIC](https://ru.wikipedia.org/wiki/%D0%98%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D0%BE%D0%BD%D0%BD%D1%8B%D0%B9_%D0%BA%D1%80%D0%B8%D1%82%D0%B5%D1%80%D0%B8%D0%B9_%D0%90%D0%BA%D0%B0%D0%B8%D0%BA%D0%B5)\n",
    "* [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Principal Component Analysis\n",
    "## Метод Главных Компонент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# PCA\n",
    "\n",
    "* Позволяет уменьшить число переменных, выбрав самые изменчивые из них\n",
    "* Новые переменные являются линейной комбинацией старых переменных\n",
    "* Переход к ортономированному базису\n",
    "\n",
    "<center><img src='http://www.visiondummy.com/wp-content/uploads/2014/05/correlated_2d.png' width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Построение PCA\n",
    "*  Пусть $x \\in \\mathbb{R}^d$ - вектор признаков для какого-то объекта. Будем считать, что $x$ - центрировано и отшкалировано. $E[x_i] = 0, V[x_i] = 1, \\quad i=1 \\dots d$\n",
    "* Требуется найти \n",
    "    1. Линейное преобразование, которое задается ортогональной матрицей $A$:\n",
    "$$ pc = A^\\top x $$\n",
    "    2. Максимизирующее дисперсию полученных компонент:\n",
    "$$ V[pc_i] = cov[pc_i, pc_i] \\rightarrow \\max $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Построение PCA\n",
    "\n",
    "* $pc_i = a_i^\\top x = x^\\top a_i$\n",
    "* $E[pc_i] = E[a_i^\\top x] = a_i^\\top E[x]$\n",
    "* $cov[x] = E[(x - E[x])(x - E[x])^\\top] = Exx^\\top = \\Sigma$ -  ковариационная матрица\n",
    "* $cov[pc_i, pc_j] = E[pc_i \\cdot pc_j^\\top] = a_i^\\top \\Sigma a_j $\n",
    "* $\\Sigma$ - симметричная и положительно определенная матрица.\n",
    "    * Собственные числа $\\lambda_i \\in \\mathbb{R}, \\lambda_i \\geq 0$ (Будем считать, что $\\lambda_1 \\geq \\lambda_2  \\geq \\dots \\geq \\lambda_d $)\n",
    "    * Собственные вектора при $\\lambda_i \\neq \\lambda_j $ ортогональны: $v_i^\\top v_j = 0$\n",
    "    * У каждого $\\lambda_i$ есть единственный $v_i$\n",
    "    * Диагонализируемая*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Первая компонента\n",
    "$$ pc_1 = a_1 ^\\top x $$\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "V[pc_1] = a_1^\\top \\Sigma a_1 \\rightarrow \\max_a \\\\\n",
    "a_1^\\top a_1 = 1\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "* Строим функцию лагранжа\n",
    "$$ \\mathcal{L}(a_1, \\nu) = a_1^\\top \\Sigma a_1 - \\nu (a_1^\\top a_1 - 1) \\rightarrow max_{a_1, \\nu}$$\n",
    "* Считаем производую по $a_1$\n",
    "$$ \\frac{\\partial\\mathcal{L}}{\\partial a_1} = 2\\Sigma a_1 - 2\\nu a_1 = 0 $$\n",
    "* Получается, что $a_1$ один из собственных векторов матрицы $\\Sigma$, причем при $\\lambda_1$\n",
    "$$ V[pc_1] = a_1^\\top \\Sigma a_1 = \\lambda_i a_1^\\top a_1 = \\lambda_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Вторая компонента\n",
    "$$ pc_2 = a_2 ^\\top x $$\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "V[pc_2] = a_2^\\top \\Sigma a_2 \\rightarrow \\max_a \\\\\n",
    "a_2^\\top a_2 = 1 \\\\\n",
    "cov[pc_1, pc_2] = a_2^\\top \\Sigma a_1 = \\lambda_1 a_2^\\top a_1 = 0\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "* Строим функцию лагранжа\n",
    "$$ \\mathcal{L}(a_2, \\nu, \\tau) = a_2^\\top \\Sigma a_2 - \\nu (a_2^\\top a_2 - 1) - \\tau a_2^\\top a_1 \\rightarrow max_{a_1, \\nu}$$\n",
    "\n",
    "Аналогичными выкладками приходим к тому, что $a_2$ - собственный вектор $\\Sigma$ при $\\lambda_2$\n",
    "\n",
    "и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Singular Value Decomposition\n",
    "\n",
    "Для любой матрицы $X$ размера $n \\times m$ и ранга $r$ можно найти разложение вида:\n",
    "$$ X = U S V^\\top ,$$\n",
    "где \n",
    "* $U$ - унитарная матрица, состоящая из собственных векторов $XX^\\top$\n",
    "* $V$ - унитарная матрица, состоящая из собственных векторов $X^\\top X$\n",
    "* $S$ - диагональная матрица с сингулярными числами $s_i = \\sqrt{\\lambda_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SVD\n",
    "Матрицы $U$ и $V$ ортогональны и могут быть использованы для перехода к ортогональному базису:\n",
    "$$ XV = US $$\n",
    "\n",
    "Сокращение размерности заключается в том, что вместо того, чтобы умножать $X$ не на всю матрицу $V$, а лишь на первые $k<m$ её столбцов - матрицу $V'$\n",
    "\n",
    "Квадраты сингулярных чисел в $S$ содержат дисперсию, объясненную в главных компонентах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/pca_svd.png' width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MNIST, DIGITS\n",
    "<center><img src='images/digits.png' width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MNIST PCA\n",
    "\n",
    "<center><img src='http://nikhilbuduma.com/img/autoencoder_digit_exp.png' width=1200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Резюме\n",
    "\n",
    "* PCA понижает размерность признакового пространства\n",
    "* Новые компоненты являются линейной комбинацией исходных признаков\n",
    "* Новые компоненты (не признаки) - ортогональны\n",
    "* Можно применять в моделях и для визуализации\n",
    "* [PCA SVD туториал](https://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Можно ли лучше?\n",
    "\n",
    "* Kernel PCA\n",
    "\n",
    "<center><img src='http://scikit-learn.org/stable/_images/sphx_glr_plot_kernel_pca_001.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Многомерное шкалирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Идея\n",
    "\n",
    "* Перейти в пространство меньшей размерности так, чтобы расстояния между объектами в новом пространстве были подобны расстояниям в исходном пространстве.\n",
    "* Дано $X = [x_1,\\dots, x_n]\\in \\mathbb{R}^{N \\times D}$ и/или $\\delta_{ij}$ - мера близости между $(x_i,x_j)$\n",
    "* Надо найти $Y = [y_1,\\dots,y_n] \\in \\mathbb{R}^{N \\times d}$ такие, что $\\delta_{ij} \\approx d(y_i, y_j) = \\|y_i-y_j\\|^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/mds.png' width =1200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Подходы\n",
    "* Классический (cMDS)\n",
    "* Метрический (metric MDS)\n",
    "* Неметрический (non-metric MDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Классический подход\n",
    "\n",
    "* Заметим, что если $Y$ - решение задачи, то и $Y+c$ тоже будет решением задачи (ибо $\\|y_i-y_j\\|^2 = \\|(y_i+с)-(y_j+с)\\|^2$)\n",
    "* Зададим ограничение на центрированное решение:\n",
    "$$ \\sum_i y_{ip} = 0, \\quad \\forall p $$\n",
    "* Заметим, что евклидово расстояние между $y_i$ и $y_j$ можно выразить как:\n",
    "$$ d(y_i, y_j) = \\|y_i-y_j\\|^2 = (y_i-y_j)^\\top(y_i-y_j) = y_i^{\\top} y_i + y_j^{\\top} y_j - 2y_i^{\\top} y_j $$\n",
    "* Обозначим $b_{ij} = y_i^{\\top} y_j$, тогда матрица $B=YY^\\top$ - матрица Грамма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Классический подход\n",
    "$$ d(y_i, y_j) = b_{ii} + b_{jj} - 2b_{ij} $$\n",
    "* Так как $Y$ - центрированы, то \n",
    "$$ \\sum_{i=1}^n b_{ij}=0 $$\n",
    "* Если мы просуммируем $d(y_i, y_j)$ по $i$, по $j$ и по $i,j$, толучим следующие соотношения\n",
    "\n",
    "$$\\frac{1}{N}\\sum_{i=1}^N d_{ij} =  \\frac{1}{N}\\sum_{i=1}^N b_{ii} + b_{jj} = d_{\\bullet j}$$ \t \n",
    "$$\\frac{1}{N}\\sum_{j=1}^N d_{ij} \t=  b_{ii} + \\frac{1}{N}\\sum_{j=1}^n b_{jj} = d_{i \\bullet}$$ \t \n",
    "$$\\frac{1}{N^2}\\sum_{i=1}^N \\sum_{j=1}^N d_{ij} = \\displaystyle \\frac{2}{N} \\sum_{i=1}^N b_{ii} = d_{\\bullet \\bullet}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Классический подход\n",
    "\n",
    "* Подставим эти соотношение в уравнение для $d(y_i, y_j)$ и выразим $b_{ij}$:\n",
    "$$  b_{ij} = -\\frac{1}{2}(d_{ij}-d_{i \\bullet} -d_{\\bullet j}+d_{\\bullet \\bullet})$$\n",
    "* Вспомним, что $B=YY^\\top$\n",
    "* Так же, матрица Грамма - симметричная и положительно определенная матрица. Значит она представима в виде:\n",
    "$$ B = A\\Lambda A^\\top$$\n",
    "* Отсюда $Y = A\\Lambda^{\\frac{1}{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Метрические и неметрические подходы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Метрические и неметрические подходы\n",
    "* Релаксация: приближаем не $d(x_i, x_j)$, а $\\tilde{d}(x_i, x_j) = f(d(x_i, x_j))$, где $f(\\cdot)$ - некоторая монотонная функция\n",
    "* Оптимизируем функцию потерь, которую еще называют стрессом:\n",
    "$$ \\mathcal{L}(d(y_i, y_j)) = \\sqrt{\\frac{\\left(\\tilde{d}(x_i, x_j) - d(y_i, y_j)\\right)^2}{d(y_i, y_j)^2}}$$\n",
    "* Неметрический подход: рассматривать преобразование $f(\\cdot)$, сохраняющее порядок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Диаграмма стресса\n",
    "\n",
    "<center><img src='https://www.mathworks.com/help/stats/mds_nonmetric.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# t-SNE\n",
    "## t-distributed stochastic neighbor embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* t-SNE - практически многомерное шкалирование\n",
    "* Вместо этого мы будем пытаться перенести \"окрестность\" точек из исходного пространства в пространоство меньшей размерности\n",
    "* Полученные расстояния скорее всего не будут соотносится с исходными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Схожесть между объектами в исходном пространстве $\\mathbb{R}^m$\n",
    "$$\n",
    "p(i, j) = \\frac{p(i | j) + p(j | i)}{2n}, \\quad p(j | i) = \\frac{\\exp(-\\|\\mathbf{x}_j-\\mathbf{x}_i\\|^2/{2 \\sigma_i^2})}{\\sum_{k \\neq i}\\exp(-\\|\\mathbf{x}_k-\\mathbf{x}_i\\|^2/{2 \\sigma_i^2})}\n",
    "$$\n",
    "$\\sigma_i$ неявно задается пользователем\n",
    "* Схожесть между объектами в целевом пространстве $\\mathbb{R}^k, k << m$\n",
    "$$\n",
    "q(i, j) = \\frac{g(|\\mathbf{y}_i - \\mathbf{y}_j|)}{\\sum_{k \\neq l} g(|\\mathbf{y}_i - \\mathbf{y}_j|)}\n",
    "$$ \n",
    "где $g(z) = \\frac{1}{1 + z^2}$ - распределение Коши (t-распределение Стьюдента с 1 степенью свободы)\n",
    "* Критерий\n",
    "$$\n",
    "J_{t-SNE}(y) = KL(P \\| Q) = \\sum_i \\sum_j p(i, j) \\log \\frac{p(i, j)}{q(i, j)} \\rightarrow \\min\\limits_{\\mathbf{y}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Дивергенция Кульбака-Лейблера\n",
    "\n",
    "* Насколько распределение $P$ отличается от распределения $Q$?\n",
    "$$\n",
    "KL(P \\| Q) = \\sum_z P(z) \\log \\frac{P(z)}{Q(z)}\n",
    "$$\n",
    "\n",
    "<center><img src='http://www.science-emergence.com/media/images/381.png' width=1200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Оптимизация\n",
    "\n",
    "* Оптимизируем $J_{t-SNE}(y)$ с помощью градиентного спуска\n",
    "\n",
    "$$\\frac{\\partial J_{t-SNE}}{\\partial y_i}=4 \\sum_j(p(i,j)−q(i,j))(y_i−y_j)g(|y_i−y_j|)$$\n",
    "\n",
    "* [Статья](http://jmlr.csail.mit.edu/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)\n",
    "* [Примеры](http://lvdmaaten.github.io/tsne/)\n",
    "* [Демо и советы](http://distill.pub/2016/misread-tsne/)\n",
    "    * t-SNE может быть нестабильным\n",
    "    * Размеры полученных сгустков могут ничего не значить\n",
    "    * Расстояния между кластерами могут ничего не значить\n",
    "    * Полностью шумовые данные могут выдать структуру"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='http://lvdmaaten.github.io/tsne/examples/mnist_tsne.jpg' width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/mainfold.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Литература\n",
    "* [Mohammed J. Zaki, Ch7](https://www.amazon.com/Data-Mining-Analysis-Fundamental-Algorithms/dp/0521766338)\n",
    "* [Мodern MDS](http://www.springer.com/la/book/9780387251509)\n",
    "* [Habrahabr](https://habrahabr.ru/post/267041/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Оставьте анонимный отзыв!\n",
    "\n",
    "### [Туть](https://docs.google.com/forms/d/e/1FAIpQLSf70omriRWE_8rqjOIO6HpD9VkXxKER6TLBW-L_gJ2mMc7Apw/viewform?usp=sf_link)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "livereveal": {
   "theme": "serif",
   "transition": "concave",
   "width": "1024px"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "513px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "none",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "toc_position": {
   "height": "973px",
   "left": "0px",
   "right": "1708px",
   "top": "109px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
