{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/logo_hse_black.jpg\"></center>\n",
    "\n",
    "<h1><center>Data Analysis</center></h1>\n",
    "<h2><center>Seminar: Ensemble Learning. </center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset from here https://archive.ics.uci.edu/ml/datasets/Epileptic+Seizure+Recognition\n",
    "df = pd.read_csv('./data/data_clf.csv').drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is about Epileptic Seizure Recognition. Your data is brain activity (EEG signals) for 26 seconds. Is't really important to learn predict such things like epileptic. Maybe in future or right now machine learning algorithms help doctors make his work better and save more lives. Values which you predict\n",
    "\n",
    "2 - They recorder the EEG from the area where the tumor was located \n",
    "\n",
    "1 - Recording of seizure activity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='y').values\n",
    "y = df['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only two classes\n",
    "X = X[np.where((y == 2) | (y == 1))]\n",
    "y = y[np.where((y == 2) | (y == 1))]\n",
    "y[y==2] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (4600, 178)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  386,   382,   356,   331,   320,   315,   307,   272,   244,\n",
       "          232,   237,   258,   212,     2,  -267,  -605,  -850, -1001,\n",
       "        -1109, -1090,  -967,  -746,  -464,  -152,   118,   318,   427,\n",
       "          473,   485,   447,   397,   339,   312,   314,   326,   335,\n",
       "          332,   324,   310,   312,   309,   309,   303,   297,   295,\n",
       "          295,   293,   286,   279,   283,   301,   308,   285,   252,\n",
       "          215,   194,   169,   111,   -74,  -388,  -679,  -892,  -949,\n",
       "         -972, -1001, -1006,  -949,  -847,  -668,  -432,  -153,    72,\n",
       "          226,   326,   392,   461,   495,   513,   511,   496,   479,\n",
       "          453,   440,   427,   414,   399,   385,   385,   404,   432,\n",
       "          444,   437,   418,   392,   373,   363,   365,   372,   385,\n",
       "          388,   383,   371,   360,   353,   334,   303,   252,   200,\n",
       "          153,   151,   143,    48,  -206,  -548,  -859, -1067, -1069,\n",
       "         -957,  -780,  -597,  -460,  -357,  -276,  -224,  -210,  -350,\n",
       "         -930, -1413, -1716, -1360,  -662,   -96,   243,   323,   241,\n",
       "           29,  -167,  -228,  -136,    27,   146,   229,   269,   297,\n",
       "          307,   303,   305,   306,   307,   280,   231,   159,    85,\n",
       "           51,    43,    62,    63,    63,    69,    89,   123,   136,\n",
       "          127,   102,    95,   105,   131,   163,   168,   164,   150,\n",
       "          146,   152,   157,   156,   154,   143,   129],\n",
       "       [    1,    -2,    -8,   -11,   -12,   -17,   -15,   -16,   -18,\n",
       "          -17,   -19,   -18,   -16,   -15,   -14,   -21,   -19,   -24,\n",
       "          -24,   -24,   -17,   -20,   -23,   -15,   -17,   -20,   -18,\n",
       "          -19,   -20,   -19,   -18,   -20,   -25,   -27,   -24,   -22,\n",
       "          -20,    -9,     0,    12,    18,    25,    23,    20,    17,\n",
       "           12,     6,    -1,    -5,   -10,   -13,   -13,   -17,   -20,\n",
       "          -20,   -19,   -20,   -21,   -22,   -24,   -27,   -29,   -31,\n",
       "          -36,   -45,   -49,   -60,   -71,   -83,   -89,   -97,  -103,\n",
       "         -105,  -103,  -104,   -97,   -99,   -99,  -101,   -96,   -91,\n",
       "          -78,   -64,   -48,   -36,   -23,   -15,   -14,   -17,   -18,\n",
       "          -15,   -14,   -13,   -12,   -17,   -21,   -22,   -23,   -14,\n",
       "          -12,    -9,   -12,   -18,   -16,   -19,   -23,   -21,   -18,\n",
       "          -17,   -15,   -10,    -7,    -9,    -7,    -2,     0,    11,\n",
       "           18,    26,    30,    30,    39,    38,    28,    14,     4,\n",
       "           -8,    -9,    -9,    -8,    -3,     3,     1,    -4,   -12,\n",
       "          -15,   -20,   -25,   -23,   -20,   -26,   -24,   -25,   -35,\n",
       "          -41,   -41,   -53,   -61,   -58,   -59,   -55,   -53,   -65,\n",
       "          -78,   -87,   -97,  -100,  -106,  -104,  -107,  -110,  -110,\n",
       "         -109,  -104,  -118,  -111,  -102,   -80,   -67,   -79,   -91,\n",
       "          -97,   -88,   -76,   -72,   -66,   -57,   -39],\n",
       "       [ -278,  -246,  -215,  -191,  -177,  -167,  -157,  -139,  -118,\n",
       "          -92,   -63,   -39,   -11,    14,    36,    60,    70,    78,\n",
       "           79,    69,    27,   -45,  -123,  -183,  -218,  -242,  -256,\n",
       "         -256,  -236,  -205,  -165,  -125,   -84,   -41,   -10,    12,\n",
       "           35,    58,    71,    85,    98,   107,   106,    97,    77,\n",
       "           46,    -2,   -77,  -130,  -142,  -141,  -120,  -144,  -170,\n",
       "         -189,  -215,  -237,  -242,  -236,  -208,  -163,   -98,   -28,\n",
       "           29,    70,    92,   102,   113,   122,   129,   125,   123,\n",
       "          118,   117,   116,   116,   108,    88,    62,    22,   -27,\n",
       "          -85,   -90,   -71,   -47,   -55,  -107,  -169,  -194,  -210,\n",
       "         -202,  -186,  -145,   -99,   -53,   -17,     1,    15,    23,\n",
       "           33,    44,    56,    77,   100,   123,   144,   155,   158,\n",
       "          154,   151,   152,   146,   143,   131,   119,    93,    39,\n",
       "          -63,  -203,  -315,  -352,  -311,  -254,  -207,  -188,  -180,\n",
       "         -170,  -149,  -120,   -87,   -45,    -3,    29,    45,    52,\n",
       "           50,    51,    59,    64,    74,    79,    81,    76,    65,\n",
       "           63,    62,    65,    67,    70,    63,    45,    20,   -11,\n",
       "          -32,   -26,     3,    40,    85,   124,   182,   248,   349,\n",
       "          418,   419,   291,    73,  -152,  -311,  -386,  -400,  -379,\n",
       "         -336,  -281,  -226,  -174,  -125,   -79,   -40],\n",
       "       [ -167,  -230,  -280,  -315,  -338,  -369,  -405,  -392,  -298,\n",
       "         -140,    27,   146,   211,   223,   214,   187,   167,   166,\n",
       "          179,   192,   190,   168,   129,    85,    43,     4,   -28,\n",
       "          -47,   -43,   -24,    -7,    12,    32,    43,    12,   -70,\n",
       "         -181,  -292,  -374,  -410,  -382,  -335,  -232,  -128,    -6,\n",
       "          106,   233,   312,   423,   550,   695,   816,   839,   769,\n",
       "          661,   525,   383,   292,   267,   339,   451,   537,   564,\n",
       "          534,   444,   305,   160,    27,   -74,  -147,  -205,  -242,\n",
       "         -274,  -304,  -331,  -355,  -372,  -380,  -370,  -341,  -299,\n",
       "         -257,  -235,  -249,  -300,  -381,  -399,  -345,  -183,    17,\n",
       "          178,   274,   288,   265,   229,   193,   160,   106,    34,\n",
       "          -51,  -120,  -166,  -189,  -207,  -225,  -242,  -251,  -255,\n",
       "         -237,  -202,  -120,    19,   186,   340,   441,   465,   410,\n",
       "          288,   130,   -16,  -123,  -194,  -232,  -255,  -272,  -266,\n",
       "         -255,  -209,  -168,  -142,  -148,  -169,  -180,  -174,  -107,\n",
       "           12,   206,   419,   596,   683,   679,   596,   472,   330,\n",
       "          168,    26,   -63,   -73,   -37,    25,    61,    67,    53,\n",
       "           28,    -6,   -44,   -92,  -154,  -211,  -257,  -258,  -168,\n",
       "          -32,   140,   277,   366,   408,   416,   415,   423,   434,\n",
       "          416,   374,   319,   268,   215,   165,   103],\n",
       "       [  -24,   -15,    -5,    -1,     4,     3,     6,    10,    11,\n",
       "            7,     8,    12,    10,    10,     5,    -1,   -11,   -13,\n",
       "          -24,   -39,   -44,   -52,   -50,   -49,   -43,   -41,   -44,\n",
       "          -49,   -49,   -53,   -60,   -58,   -59,   -55,   -49,   -40,\n",
       "          -34,   -29,   -29,   -20,   -15,   -17,   -22,   -31,   -35,\n",
       "          -40,   -38,   -37,   -35,   -33,   -36,   -45,   -53,   -53,\n",
       "          -57,   -55,   -52,   -47,   -47,   -53,   -62,   -70,   -80,\n",
       "          -84,   -92,  -101,  -103,  -104,  -100,   -87,   -70,   -50,\n",
       "          -33,   -13,     0,    14,    23,    26,    26,    18,    13,\n",
       "            6,     8,    10,    16,    20,    21,    18,    15,    15,\n",
       "            9,     6,     2,     2,    -5,   -13,   -25,   -36,   -33,\n",
       "          -32,   -31,   -37,   -52,   -75,   -99,  -109,  -114,  -110,\n",
       "          -99,   -86,   -69,   -47,   -35,   -27,   -21,   -13,    -9,\n",
       "           -9,    -8,    -8,    -9,    -8,    -5,    -5,    -9,   -12,\n",
       "          -17,   -22,   -26,   -27,   -27,   -31,   -30,   -37,   -44,\n",
       "          -52,   -57,   -58,   -58,   -55,   -51,   -49,   -47,   -39,\n",
       "          -41,   -33,   -26,   -24,   -22,   -21,   -19,   -20,   -16,\n",
       "           -7,   -10,   -13,   -19,   -13,   -15,   -13,   -13,    -4,\n",
       "            3,    15,    19,    21,    25,    29,    33,    32,    35,\n",
       "           36,    34,    32,    26,    23,    18,    20]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"X shape: \", X.shape)\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape:  (4600,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"y shape: \", y.shape)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train_test_split function to split the sample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.5,    \n",
    "                                                    random_state=123) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let' s start with small steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First step\n",
    "\n",
    "Implement auxiliary function , which generate random subsample (with replacement) of X, y of size N with m features (m <= X.shape[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_subsample(X, y, N, m):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Second step\n",
    "\n",
    "Implement auxiliary function, which generate K random subsamples (with replacement) of X, y of size N with m random features (m <= X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_subsamples(X, y, K, N, m):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third step\n",
    "\n",
    "Implement auxiliary function which get list of (X,y) with lengh K and fit K base_estimators. Each estimator fit from the corresponding sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_subsamples(subsamples, base_estimator):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(data):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    X, y = data\n",
    "    return LogisticRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "\n",
    "process_count = 4\n",
    "parts = [\n",
    "    (X, y)\n",
    "    for i in range(process_count)\n",
    "]\n",
    "\n",
    "with multiprocessing.Pool(process_count) as p:\n",
    "    # every part is sent to separate processes\n",
    "    clfs = p.map(fit, parts)\n",
    "clfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fourth step \n",
    "\n",
    "Write code to generate two subsamples of data and fit 2 DecisionTreeClassifier parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now combine it all together in one class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 (5 points)\n",
    "\n",
    "Implement Random Forest classifier as it was described in your lectures:\n",
    "\n",
    "**Input**: \n",
    "* training dataset $TDS=\\{(x_{i},y_{i}),\\,1=1,2,...N\\}$; \n",
    "* the number of trees $B$ and the size of feature subsets $m$.\n",
    "\n",
    "for $b=1,2,...B$:\n",
    "\n",
    "1. generate random training dataset $TDS^{b}$ of size $N$ by sampling $(x_{i},y_{i})$ pairs from $TDS$ with replacement (bootstrap)\n",
    "2. build a tree using $TDS^{b}$ training dataset with feature selection for each node from random subset of features of size $m$ (generated **individually for each node**).\n",
    "\n",
    "\n",
    "**Output**: $B$ trees. Classification is done using majority vote and regression using averaging of $B$ outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hints:**\n",
    "- Use decision tree classifier from sklean library. You can import it with the command: `from sklearn.tree import DecisionTreeClassifier`\n",
    "- You can use `numpy.random.choice()` function to generate random subsamples to train decition tree classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 (3 points)\n",
    "\n",
    "Implement parallel learning of trees\n",
    "\n",
    "Compare the running times of parallel and regular versions.\n",
    "\n",
    "Use parameter n_jobs in RandomForestCalssifier init_function. Default value is 1 (if n_jobs = 1 run the regular version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to modify this class #\n",
    "\n",
    "class RandomForestCalssifier(object):\n",
    "    \n",
    "    def __init__(self, n_trees=10, n_subset_features=2, n_jobs=1): # you can add more hyperparameters\n",
    "        \"\"\"\n",
    "        This is your random forest classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_trees : int\n",
    "            Number of decision trees to train.\n",
    "        n_subset_features : int\n",
    "            Number of random features to used to train a decision tree.\n",
    "        n_jobs : int\n",
    "            Number of jobs\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_trees = n_trees\n",
    "        self.n_subset_features = n_subset_features\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.array, shape = (n_objects, n_features)\n",
    "            Matrix of objects that are described by their input features.\n",
    "        y : numpy.array, shape = (n_objects)\n",
    "            1D array with the object labels. \n",
    "            For the classification labels are integers in {0, 1, 2, ...}.\n",
    "        \"\"\"\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        This methods performs probabilities prediction for new objects.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.array, shape = (n_objects, n_features)\n",
    "            Matrix of objects that are described by their input features.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        proba : numpy.array, shape = (n_objects, n_classes)\n",
    "            Array with predicted probabilities. \n",
    "        \"\"\"\n",
    "        \n",
    "        proba = np.random.rand(len(X), 2) # for 2 classes\n",
    "        \n",
    "        return proba\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        This methods performs labels prediction for new objects.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.array, shape = (n_objects, n_features)\n",
    "            Matrix of objects that are described by their input features.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        labels : numpy.array, shape = (n_objects)\n",
    "            1D array with predicted labels. \n",
    "            For the classification labels are integers in {0, 1, 2, ...}.\n",
    "        \"\"\"\n",
    "        \n",
    "        labels = np.random.randint(low=0, high=2, size=len(X)) # for 2 classes\n",
    "        \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 (1 point)\n",
    "\n",
    "Plot ROC curve on the test sample for your random forest classifier. Also, claculate ROC AUC value. Use `RandomForestClassifier.predict_proba()` method.\n",
    "\n",
    "Hints:\n",
    "- You can use `sklearn.metrics.roc_curve` frunction to calculate ROC curve.\n",
    "- `sklearn.metrics.roc_auc_score` function helps you to calculate ROC AUC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 (2 points)\n",
    "\n",
    "Plot dependecy of ROC AUC value from number of trees (`n_trees`) in your random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 (2 points)\n",
    "\n",
    "Plot dependecy of ROC AUC value from `n_subset_features` of your random forest classifier. Use `n_trees=100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read  New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data from here https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD\n",
    "df = pd.read_csv('./data/YearPredictionMSD.txt', sep = ',', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Prediction of the release year of a song from audio features. Songs are mostly western, commercial tracks ranging from 1922 to 2011, with a peak in the year 2000s\n",
    " \n",
    " Why? Because it's cool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = 0).values\n",
    "y = df[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2001, 2001, 2001, ..., 2006, 2006, 2005])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before doing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement function which calculate $z_i$ from gradient boosting alogirithm (minus gradient of loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_objective(r, y, loss = 'mse'):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "        ----------\n",
    "        r : np.array\n",
    "            value of f(x)\n",
    "        y : np.array\n",
    "            target\n",
    "        loss : str\n",
    "            loss function. Possible values : mse, hl, logloss\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement function which make one step of gradient boossting (fit new estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(x, y, estimators, lr, loss = 'mse'):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "        ----------\n",
    "        x : np.array\n",
    "            features\n",
    "        y : np.array\n",
    "            target\n",
    "        estimators : array of estimators\n",
    "            first estimators, who have already been trained\n",
    "        lr : float\n",
    "            learning rate\n",
    "        loss : str\n",
    "            loss function. Possible values : mse, hl, logloss\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now use your knowledge and implement gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6 (5 points)\n",
    "\n",
    "Implement GradientBoostingRegressor as it was described in your lectures:\n",
    "\n",
    "**Input**: training dataset $(x_{i},y_{i}),\\,i=1,2,...N$; loss function $\\mathcal{L}(f,y)$; learning rate $\\nu$ and the number $M$ of successive additive approximations.\n",
    "\n",
    "1. Fit initial approximation $f_{0}(x)$ (might be taken $f_{0}(x)\\equiv0$)\n",
    "2. For each step $m=1,2,...M$:\n",
    "\n",
    "    1. calculate derivatives $z_{i}=-\\frac{\\partial\\mathcal{L}(r,y_{i})}{\\partial r}|_{r=f^{m-1}(x_{i})}$\n",
    "    2. fit $h_{m}$ to $\\{(x_{i},z_{i})\\}_{i=1}^{N}$, for example by solving\n",
    "$$\n",
    "\\sum_{n=1}^{N}(h_{m}(x_{n})-z_{n})^{2}\\to\\min_{h_{m}}\n",
    "$$\n",
    "    4. set $f_{m}(x)=f_{m-1}(x)+\\nu h_{m}(x)$\n",
    "\n",
    "\n",
    "**Output**: approximation function $f_{M}(x)=f_{0}(x)+\\sum_{m=1}^{M}\\nu h_{m}(x)$\n",
    "\n",
    "Implement three loss:\n",
    "\n",
    "    1 MSE\n",
    "    2 Huber loss(https://en.wikipedia.org/wiki/Huber_loss)\n",
    "    3 log_loss (in this case we solve classification task\n",
    "In our case $h_m$ is DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to modify this class #\n",
    "\n",
    "class GradientBoostingRegressor(object):\n",
    "\n",
    "    def __init__(self, n_estimators = 100, loss = 'mse', learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        This is your random forest classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_estimators : int\n",
    "            Number of estimators to train.\n",
    "        learning_rate : float\n",
    "            earning_rate.\n",
    "        loss : str\n",
    "            Loss. Possible values : mse, hl, logloss\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.array, shape = (n_objects, n_features)\n",
    "            Matrix of objects that are described by their input features.\n",
    "        y : numpy.array, shape = (n_objects)\n",
    "            1D array with the object labels. \n",
    "            For the classification labels are integers in {0, 1, 2, ...}.\n",
    "        \"\"\"\n",
    "        \n",
    "        pass\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        This methods performs labels prediction for new objects.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.array, shape = (n_objects, n_features)\n",
    "            Matrix of objects that are described by their input features.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        labels : numpy.array, shape = (n_objects)\n",
    "            1D array with predicted labels. \n",
    "            For the classification labels are integers in {0, 1, 2, ...}.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        This methods performs probabilities prediction for new objects.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.array, shape = (n_objects, n_features)\n",
    "            Matrix of objects that are described by their input features.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        proba : numpy.array, shape = (n_objects, n_classes)\n",
    "            Array with predicted probabilities. \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict_top_n(self, X, n):\n",
    "        \"\"\"\n",
    "        Predict only from top_n estimators \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict_proba_top_n(self, X, n):\n",
    "        \"\"\"\n",
    "        Predict probability only from top_n estimators \n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7 (2 points)\n",
    "\n",
    "Split your data on train, valid sample (fix random_seed = 42). Choose the optimal learning_rate and n_estimators (for logloss use data from first task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8 (2 points)\n",
    "Plot dependecy of loss value (in classification task plor roc-auc score) from `n_estimators` of your bossting. Use `learning_rate=0.1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 9 (2 points)\n",
    "Plot dependecy of loss value (in classification task plor roc-auc score) from `learning_rate` of your bossting. Use `n_estimators=100`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
