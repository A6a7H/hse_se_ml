{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/logo_hse_black.jpg\"></center>\n",
    "\n",
    "<h1><center>Data Analysis</center></h1>\n",
    "<h2><center>Seminar: Ensemble Learning. </center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset from here https://archive.ics.uci.edu/ml/datasets/Epileptic+Seizure+Recognition\n",
    "df = pd.read_csv('./data/data_clf.csv').drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is about Epileptic Seizure Recognition. Your data is brain activity (EEG signals) for 26 seconds. Is't really important to learn predict such things like epileptic. Maybe in future or right now machine learning algorithms help doctors make his work better and save more lives. Values which you predict\n",
    "\n",
    "2 - They recorder the EEG from the area where the tumor was located \n",
    "\n",
    "1 - Recording of seizure activity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='y').values\n",
    "y = df['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only two classes\n",
    "X = X[np.where((y == 2) | (y == 1))]\n",
    "y = y[np.where((y == 2) | (y == 1))]\n",
    "y[y==2] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (4600, 178)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  386,   382,   356,   331,   320,   315,   307,   272,   244,\n",
       "          232,   237,   258,   212,     2,  -267,  -605,  -850, -1001,\n",
       "        -1109, -1090,  -967,  -746,  -464,  -152,   118,   318,   427,\n",
       "          473,   485,   447,   397,   339,   312,   314,   326,   335,\n",
       "          332,   324,   310,   312,   309,   309,   303,   297,   295,\n",
       "          295,   293,   286,   279,   283,   301,   308,   285,   252,\n",
       "          215,   194,   169,   111,   -74,  -388,  -679,  -892,  -949,\n",
       "         -972, -1001, -1006,  -949,  -847,  -668,  -432,  -153,    72,\n",
       "          226,   326,   392,   461,   495,   513,   511,   496,   479,\n",
       "          453,   440,   427,   414,   399,   385,   385,   404,   432,\n",
       "          444,   437,   418,   392,   373,   363,   365,   372,   385,\n",
       "          388,   383,   371,   360,   353,   334,   303,   252,   200,\n",
       "          153,   151,   143,    48,  -206,  -548,  -859, -1067, -1069,\n",
       "         -957,  -780,  -597,  -460,  -357,  -276,  -224,  -210,  -350,\n",
       "         -930, -1413, -1716, -1360,  -662,   -96,   243,   323,   241,\n",
       "           29,  -167,  -228,  -136,    27,   146,   229,   269,   297,\n",
       "          307,   303,   305,   306,   307,   280,   231,   159,    85,\n",
       "           51,    43,    62,    63,    63,    69,    89,   123,   136,\n",
       "          127,   102,    95,   105,   131,   163,   168,   164,   150,\n",
       "          146,   152,   157,   156,   154,   143,   129],\n",
       "       [    1,    -2,    -8,   -11,   -12,   -17,   -15,   -16,   -18,\n",
       "          -17,   -19,   -18,   -16,   -15,   -14,   -21,   -19,   -24,\n",
       "          -24,   -24,   -17,   -20,   -23,   -15,   -17,   -20,   -18,\n",
       "          -19,   -20,   -19,   -18,   -20,   -25,   -27,   -24,   -22,\n",
       "          -20,    -9,     0,    12,    18,    25,    23,    20,    17,\n",
       "           12,     6,    -1,    -5,   -10,   -13,   -13,   -17,   -20,\n",
       "          -20,   -19,   -20,   -21,   -22,   -24,   -27,   -29,   -31,\n",
       "          -36,   -45,   -49,   -60,   -71,   -83,   -89,   -97,  -103,\n",
       "         -105,  -103,  -104,   -97,   -99,   -99,  -101,   -96,   -91,\n",
       "          -78,   -64,   -48,   -36,   -23,   -15,   -14,   -17,   -18,\n",
       "          -15,   -14,   -13,   -12,   -17,   -21,   -22,   -23,   -14,\n",
       "          -12,    -9,   -12,   -18,   -16,   -19,   -23,   -21,   -18,\n",
       "          -17,   -15,   -10,    -7,    -9,    -7,    -2,     0,    11,\n",
       "           18,    26,    30,    30,    39,    38,    28,    14,     4,\n",
       "           -8,    -9,    -9,    -8,    -3,     3,     1,    -4,   -12,\n",
       "          -15,   -20,   -25,   -23,   -20,   -26,   -24,   -25,   -35,\n",
       "          -41,   -41,   -53,   -61,   -58,   -59,   -55,   -53,   -65,\n",
       "          -78,   -87,   -97,  -100,  -106,  -104,  -107,  -110,  -110,\n",
       "         -109,  -104,  -118,  -111,  -102,   -80,   -67,   -79,   -91,\n",
       "          -97,   -88,   -76,   -72,   -66,   -57,   -39]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"X shape: \", X.shape)\n",
    "X[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape:  (4600,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"y shape: \", y.shape)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train_test_split function to split the sample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.5,    \n",
    "                                                    random_state=123) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let' s start with small steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First step\n",
    "\n",
    "Implement auxiliary function, which generate K random subsamples (with replacement) of X, y of size N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_subsamples(X, y, K, N):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second step\n",
    "\n",
    "Implement auxiliary function which get list of (X,y) with lengh K and fit K base_estimators. Each estimator fit from the corresponding sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_subsamples(subsamples, base_estimator):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(params):    \n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    X, y, size = params\n",
    "    subsamples = gen_subsamples(X, y, size, X.shape[0])\n",
    "    return fit_subsamples(subsamples, DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "process_count = 4\n",
    "parts = [\n",
    "    (X, y, 1000 // process_count)\n",
    "    for i in range(process_count)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with multiprocessing.Pool(process_count) as p:\n",
    "    # every part is sent to separate processes\n",
    "    clfs = p.map(fit, parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now combine it all together in one class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 (3 points)\n",
    "\n",
    "Implement Random Forest classifier as it was described in your lectures:\n",
    "\n",
    "**Input**: \n",
    "* training dataset $TDS=\\{(x_{i},y_{i}),\\,1=1,2,...N\\}$; \n",
    "* the number of trees $B$ and the size of feature subsets $m$.\n",
    "\n",
    "for $b=1,2,...B$:\n",
    "\n",
    "1. generate random training dataset $TDS^{b}$ of size $N$ by sampling $(x_{i},y_{i})$ pairs from $TDS$ with replacement (bootstrap)\n",
    "2. build a tree using $TDS^{b}$ training dataset with feature selection for each node from random subset of features of size $m$ (generated **individually for each node**).\n",
    "\n",
    "\n",
    "**Output**: $B$ trees. Classification is done using majority vote and regression using averaging of $B$ outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hints:**\n",
    "- Use decision tree classifier from sklean library. You can import it with the command: `from sklearn.tree import DecisionTreeClassifier`\n",
    "- You can use `numpy.random.choice()` function to generate random subsamples to train decition tree classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 (2 points)\n",
    "\n",
    "Implement parallel learning of trees\n",
    "\n",
    "Use parameter n_jobs in RandomForestCalssifier init_function. Default value is 1 (if n_jobs = 1 run the regular version)\n",
    "\n",
    "Compare the running times of parallel and regular versions (use different n_jobs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to modify this class #\n",
    "\n",
    "class RandomForestCalssifier(object):\n",
    "    \n",
    "    def __init__(self, n_trees=10, n_subset_features=10, n_jobs=1, random_state = 42): # you can add more hyperparameters\n",
    "        \"\"\"\n",
    "        This is your random forest classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_trees : int\n",
    "            Number of decision trees to train.\n",
    "        n_subset_features : int\n",
    "            Number of random features to used to train a decision tree.\n",
    "        n_jobs : int\n",
    "            Number of jobs\n",
    "        random_state : int\n",
    "            random_state for decision tree\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_trees = n_trees\n",
    "        self.n_subset_features = n_subset_features\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.array, shape = (n_objects, n_features)\n",
    "            Matrix of objects that are described by their input features.\n",
    "        y : numpy.array, shape = (n_objects)\n",
    "            1D array with the object labels. \n",
    "            For the classification labels are integers in {0, 1, 2, ...}.\n",
    "        \"\"\"\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        This methods performs probabilities prediction for new objects.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.array, shape = (n_objects, n_features)\n",
    "            Matrix of objects that are described by their input features.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        proba : numpy.array, shape = (n_objects, n_classes)\n",
    "            Array with predicted probabilities. \n",
    "        \"\"\"\n",
    "        \n",
    "        proba = np.random.rand(len(X), 2) # for 2 classes\n",
    "        \n",
    "        return proba\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        This methods performs labels prediction for new objects.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.array, shape = (n_objects, n_features)\n",
    "            Matrix of objects that are described by their input features.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        labels : numpy.array, shape = (n_objects)\n",
    "            1D array with predicted labels. \n",
    "            For the classification labels are integers in {0, 1, 2, ...}.\n",
    "        \"\"\"\n",
    "        \n",
    "        labels = np.random.randint(low=0, high=2, size=len(X)) # for 2 classes\n",
    "        \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 (1 point)\n",
    "\n",
    "Plot ROC curve on the test sample for your random forest classifier. Also, claculate ROC AUC value. Use `RandomForestClassifier.predict_proba()` method.\n",
    "\n",
    "Hints:\n",
    "- You can use `sklearn.metrics.roc_curve` frunction to calculate ROC curve.\n",
    "- `sklearn.metrics.roc_auc_score` function helps you to calculate ROC AUC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 (1 point)\n",
    "\n",
    "Plot dependecy of ROC AUC value from number of trees (`n_trees`) in your random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 (1 point)\n",
    "\n",
    "Plot dependecy of ROC AUC value from `n_subset_features` of your random forest classifier. Use `n_trees=100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read  New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data from here https://www.kaggle.com/c/boston-housing/overview\n",
    "df = pd.read_csv('./data/boston_train.csv').drop(columns = 'ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Prediction of house price in Boston.\n",
    " \n",
    " Why it's important? Because it can help people determine a fair price at home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = 'medv').values\n",
    "y = df['medv'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333, 13)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before doing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement function which calculate $z_i$ from gradient boosting alogirithm (minus gradient of loss function). Implement for all possible loss function (mse, hl, logloss)\n",
    "\n",
    "In hl use $\\delta = 1$, assume that $MSE = \\frac{1}{2}(r - y)^2$ and log_loss use $0$ and $1$ like class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_objective(r, target, loss = 'mse'):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "        ----------\n",
    "        r : np.array\n",
    "            value of f(x)\n",
    "        target : np.array\n",
    "            target\n",
    "        loss : str\n",
    "            loss function. Possible values : mse, hl, logloss\n",
    "    \"\"\"\n",
    "    return np.zeros(y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (333,) (5,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-acaad57db82f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalc_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.00001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mallclose\u001b[0;34m(a, b, rtol, atol, equal_nan)\u001b[0m\n\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m     \"\"\"\n\u001b[0;32m-> 2423\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mequal_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mequal_nan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2424\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36misclose\u001b[0;34m(a, b, rtol, atol, equal_nan)\u001b[0m\n\u001b[1;32m   2522\u001b[0m     \u001b[0myfin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2523\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxfin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2524\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwithin_tol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2525\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m         \u001b[0mfinite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxfin\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0myfin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mwithin_tol\u001b[0;34m(x, y, atol, rtol)\u001b[0m\n\u001b[1;32m   2508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithin_tol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2509\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2510\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mless_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrtol\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2512\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (333,) (5,) "
     ]
    }
   ],
   "source": [
    "#test your code\n",
    "\n",
    "r = np.array([1, 2, 3, 4, 5])\n",
    "target = np.array([10, 9, 8, 7, 6])\n",
    "assert(np.allclose(calc_objective(r, target, 'mse'), np.array([9, 7, 5, 3, 1]), 0.00001))\n",
    "\n",
    "r = np.array([2, 4, 7, 9, 13])\n",
    "target = np.array([2.5, 6, 10, 6, 12.75])\n",
    "assert(np.allclose(calc_objective(r, target, 'hl'), np.array([0.5, -1, -1, 1, -0.25]), 0.00001))\n",
    "\n",
    "r = np.array([0, np.log(2), -np.log(9), np.log(4), np.log(19)])\n",
    "target = np.array([0, 1, 0, 1, 1])\n",
    "assert(np.allclose(calc_objective(r, target, 'logloss'), np.array([-0.5, 1. / 3, -0.1, 0.2, 0.05]), 0.00001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement function which make one step of gradient boossting (fit new estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(X, y, r, base_estimator, loss = 'mse'):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "        ----------\n",
    "        X : numpy.array, shape = (n_objects, n_features)\n",
    "            Matrix of objects that are described by their input features.\n",
    "        y : numpy.array, shape = (n_objects)\n",
    "            1D array with the target variables\n",
    "        r : np.array, shape = (n_ojects)\n",
    "            f_{m-1}(X) (X matrix of features) - prediction of previous m-1 base_estimators\n",
    "        loss : str\n",
    "            loss function. Possible values : mse, hl, logloss\n",
    "            \n",
    "        Return fitted base_estimator\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now use your knowledge and implement gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6 (3 points)\n",
    "\n",
    "Implement GradientBoostingRegressor as it was described in your lectures:\n",
    "\n",
    "**Input**: training dataset $(x_{i},y_{i}),\\,i=1,2,...N$; loss function $\\mathcal{L}(f,y)$; learning rate $\\nu$ and the number $M$ of successive additive approximations.\n",
    "\n",
    "1. Fit initial approximation $f_{0}(x)$ (might be taken $f_{0}(x)\\equiv0$)\n",
    "2. For each step $m=1,2,...M$:\n",
    "\n",
    "    1. calculate derivatives $z_{i}=-\\frac{\\partial\\mathcal{L}(r,y_{i})}{\\partial r}|_{r=f^{m-1}(x_{i})}$\n",
    "    2. fit $h_{m}$ to $\\{(x_{i},z_{i})\\}_{i=1}^{N}$, for example by solving\n",
    "$$\n",
    "\\sum_{n=1}^{N}(h_{m}(x_{n})-z_{n})^{2}\\to\\min_{h_{m}}\n",
    "$$\n",
    "    4. set $f_{m}(x)=f_{m-1}(x)+\\nu h_{m}(x)$\n",
    "\n",
    "\n",
    "**Output**: approximation function $f_{M}(x)=f_{0}(x)+\\sum_{m=1}^{M}\\nu h_{m}(x)$\n",
    "\n",
    "Implement three loss:\n",
    "\n",
    "    1 MSE\n",
    "    2 Huber loss(https://en.wikipedia.org/wiki/Huber_loss)\n",
    "    3 log_loss (in this case we solve classification task\n",
    "In our case $h_m$ is DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to modify this class #\n",
    "\n",
    "class GradientBoostingEstimator(object):\n",
    "\n",
    "    def __init__(self, n_estimators, max_depth = 3, max_leaf_nodes = 8, n_subset_features = 5, random_state = 42,\n",
    "                 loss = 'mse', learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        This is your random forest classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_estimators : int\n",
    "            Number of estimators to train.\n",
    "        max_depth : int\n",
    "            max_depth of DecisionTreeRegressor\n",
    "        max_leaf_nodes:\n",
    "            max_leaf_nodes of DecisionTreeRegressor\n",
    "        n_subset_features : int\n",
    "            Number of random features to used to train a decision tree\n",
    "        random_state : int\n",
    "            random_state for decision tree\n",
    "        loss : str\n",
    "            Loss. Possible values : mse, hl, logloss\n",
    "        learning_rate : float\n",
    "            learning_rate (coef for next estimator on each step)\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.array, shape = (n_objects, n_features)\n",
    "            Matrix of objects that are described by their input features.\n",
    "        y : numpy.array, shape = (n_objects)\n",
    "            1D array with the object labels. \n",
    "            For the classification labels are integers in {0, 1, 2, ...}.\n",
    "        \"\"\"\n",
    "        \n",
    "        pass\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        This methods performs labels prediction for new objects.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.array, shape = (n_objects, n_features)\n",
    "            Matrix of objects that are described by their input features.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        labels : numpy.array, shape = (n_objects)\n",
    "            1D array with predicted labels. \n",
    "            For the classification labels are integers in {0, 1, 2, ...}.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        This methods performs probabilities prediction for new objects.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.array, shape = (n_objects, n_features)\n",
    "            Matrix of objects that are described by their input features.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        proba : numpy.array, shape = (n_objects, n_classes)\n",
    "            Array with predicted probabilities. \n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important\n",
    "When you select `learning_rate` and `n_estimators`, follow the default values of the tree parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7 (2 points)\n",
    "\n",
    "Split your data on train, valid sample (fix random_seed). Choose the optimal `learning_rate` and `n_estimators` for every loss (for logloss use data from first task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8 (1 point)\n",
    "Plot dependecy of loss value (in classification task plor roc-auc score) from `n_estimators` of your bossting. Use `learning_rate=0.1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 9 (1 point)\n",
    "Plot dependecy of loss value (in classification task plor roc-auc score) from `learning_rate` of your bossting. Use `n_estimators=100`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
