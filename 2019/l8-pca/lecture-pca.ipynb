{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"img/logo_hse_black.jpg\"></center>\n",
    "\n",
    "<h1><center>Data Analysis</center></h1>\n",
    "<h3><center>Andrey Shestakov (<a href=\"mailto:avshestakov@hse.ru\">avshestakov@hse.ru</a>)</center></h3>\n",
    "<hr>\n",
    "<h2><center>Feature Selection and Dimention Reduction. PCA.<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup></center></h2>\n",
    "\n",
    "\n",
    "\n",
    "<sup id=\"fn1\">1. Some materials are taken from <a href=\"http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5_%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B_%D1%80%D0%B0%D1%81%D0%BF%D0%BE%D0%B7%D0%BD%D0%B0%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F_%D0%BE%D0%B1%D1%80%D0%B0%D0%B7%D0%BE%D0%B2_%28%D0%BA%D1%83%D1%80%D1%81_%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D0%B9%2C_%D0%92.%D0%92.%D0%9A%D0%B8%D1%82%D0%BE%D0%B2%29\">machine learning course of Victor Kitov</a></sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-talk')\n",
    "plt.rcParams['figure.figsize'] = (12,8)\n",
    "\n",
    "# Для кириллицы на графиках\n",
    "font = {'family': 'Verdana',\n",
    "        'weight': 'normal'}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "from ipywidgets import interact, IntSlider, fixed, FloatSlider\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature Selection vs. Dimension Reduction\n",
    "\n",
    "<center><img src='img/sel_extr.png' width=600></center>\n",
    "\n",
    "* Feature selection is a process of selecting a subset of original features with minimum loss of information related to final task (classification, regression, etc.)\n",
    "* Dimension reduction is a result of some transformation of initial features to (possibly) lower dimension feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why feature selection?\n",
    "\n",
    "* increase predictive accuracy of classifier\n",
    "* improve optimization stability by removing multicollinearity\n",
    "* increase computational efficiency\n",
    "* reduce cost of future data collection\n",
    "* make classifier more interpretable\n",
    "\n",
    "**Not always necessary step**\n",
    "* some methods have implicit feature selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature Selection Approaches\n",
    "* Unsupervised methods\n",
    "    * don't use target feature\n",
    "* Filter methdos\n",
    "    * use target feature\n",
    "    * consider each feature independently\n",
    "* Wrapper methods\n",
    "    * uses model quality\n",
    "* Embedded methdos\n",
    "    * embedded inside model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### \"Unsupervised\" methods\n",
    "\n",
    "* Determine feature importance regardless of target feature\n",
    "* Your ideas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Filter methods \n",
    "* Features are considered independently of each other\n",
    "* Individual predictive power is measures\n",
    "\n",
    "**Basically**\n",
    "* Order features with respect to feature importances $I(f)$:\n",
    "$$\n",
    "I(f_{1})> I(f_{2})> \\dots\\ge I(f_{D})\n",
    "$$\n",
    "* Select top $m$\n",
    "$$\n",
    "\\hat{F}=\\{f_{1},f_{2},...f_{m}\\}\n",
    "$$\n",
    "\n",
    "\n",
    "* Simple to implement\n",
    "* Usually quite fast\n",
    "* When features are correlated, it will take many redundant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Examples\n",
    "\n",
    "* Correlation\n",
    "    * Which kind of relationship does correlation measure?\n",
    "* Mutual Information\n",
    "    * Entropy of variable $Y$: $H(Y) = - \\sum_y p(y)\\ln p(y)$\n",
    "    * Conditional entropy of $Y$ after observing $X$: $H(y|x) = - \\sum_x p(x) \\sum_y p(y|x)\\ln p(y|x) $\n",
    "    * Mutial information: $$MI(Y, X) = \\sum_{x,y} p(x,y) \\ln\\left[\\frac{p(x,y)}{p(x)p(y)}\\right]$$\n",
    "        * Mutual information measures how much $X$ and $Y$ share information between each other\n",
    "        * $MI(Y,X) = H(Y) - H(Y|X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='img/mi.png' width=300></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_titanic = pd.read_csv('data/titanic.csv')\n",
    "df_titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09090909 0.52525253]\n",
      " [0.26150393 0.12233446]]\n"
     ]
    }
   ],
   "source": [
    "P = pd.crosstab(df_titanic.Survived, df_titanic.Sex, normalize=True).values\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.61616162]\n",
      " [0.38383838]]\n",
      "[[0.35241302]\n",
      " [0.64758698]]\n"
     ]
    }
   ],
   "source": [
    "px = P.sum(axis=1)[:, np.newaxis]\n",
    "py = P.sum(axis=0)[:, np.newaxis]\n",
    "print(px)\n",
    "print(py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21714338, 0.39901824],\n",
       "       [0.13526964, 0.24856874]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "px.dot(py.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def mutual_info(x, y):\n",
    "    '''\n",
    "    Method should take arrays of values x and y and calculate their mutual information\n",
    "    '''\n",
    "    Pxy = pd.crosstab(x, y, normalize=True).values\n",
    "    Px  = Pxy.sum(axis=1)[:, np.newaxis]\n",
    "    Py = Pxy.sum(axis=0)[:, np.newaxis]\n",
    "    PxPy = Px.dot(Py.T)\n",
    "    MI = (Pxy*np.log(Pxy/(PxPy))).sum()\n",
    "    \n",
    "    return MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15087048925218172"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutual_info(df_titanic.Sex, df_titanic.Survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Wrapper methods\n",
    "* Selecting suboptimal subset of features\n",
    "* Could be slow\n",
    "* Examples: \n",
    "    * Recursive Feature Elimination\n",
    "        * Consider full set of features\n",
    "        * Fit a model, measure feature importance (based on model)\n",
    "        * Remove least important feature(s)\n",
    "        * Repeat\n",
    "    * [Boruta Algorithm](https://www.google.ru/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=0ahUKEwif5biy-fTWAhXkYJoKHbdxCLAQFgg2MAE&url=https%3A%2F%2Fwww.jstatsoft.org%2Farticle%2Fview%2Fv036i11%2Fv36i11.pdf&usg=AOvVaw3tyiHN0BCe2fkkAA6xEVDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='img/rfecv.png' width=700></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Embedded methods\n",
    "* Feature selection process in included in the model\n",
    "* Examples:\n",
    "    * Decision Trees\n",
    "    * Linear model with L1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dimension Reduction\n",
    "# Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature Selection vs. Feature Extraction\n",
    "\n",
    "\n",
    "<center><img src='img/sel_extr.png' width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why dimension reduction?\n",
    "\n",
    "same as in feature selection +\n",
    "* data visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='http://lvdmaaten.github.io/tsne/examples/mnist_tsne.jpg' width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# PCA\n",
    "\n",
    "* Intuition 1: Find directions along which our datapoints have the greatest variance\n",
    "\n",
    "<center><img src='http://www.visiondummy.com/wp-content/uploads/2014/05/correlated_2d.png' width=400></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='img/pca.png' width=800></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# PCA\n",
    "\n",
    "* Intuition 2: Find a subspace $L$ (of lesser dimention) s.t. the sum of squares of differences between points and their projections is minimized\n",
    "\n",
    "<center><img src='img/pca_example.png' width=800></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## In other words, we can consider PCA as\n",
    "\n",
    "* A transformation of your inital feature axes ...\n",
    "    * New axes are just a linear combination of initial axes\n",
    "    * New axes are orthogonal (orthonormal) to each other\n",
    "    * Variance of data across those axes is maximized\n",
    "* ... which keeps only the most \"informative\" axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How do we project data on new axes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Consider an object $x$ with 3 features: $x=(-0.343, -0.754, 0.241)$\n",
    "* We can say that it is spanned on 3 basis vectors of our feature space:\n",
    "$$ e_1 = \\left( \\begin{array}{c}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{array} \\right) \\quad\n",
    "e_2 = \\left( \\begin{array}{c}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{array} \\right) \\quad\n",
    "e_3 = \\left( \\begin{array}{c}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{array} \\right) \\quad$$\n",
    "\n",
    "$$ x = -0.343 e_1 + -0.754 e_2 + 0.241 e_3 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Consider new basis vectors:\n",
    "\n",
    "$$ a_1 = \\left( \\begin{array}{c}\n",
    "-0.390 \\\\\n",
    "0.089 \\\\\n",
    "-0.916\n",
    "\\end{array} \\right) \\quad\n",
    "a_2 = \\left( \\begin{array}{c}\n",
    "-0.639 \\\\\n",
    "-0.742 \\\\\n",
    "0.200\n",
    "\\end{array} \\right) \\quad\n",
    "a_3 = \\left( \\begin{array}{c}\n",
    "-0.663 \\\\\n",
    "0.664 \\\\\n",
    "0.346\n",
    "\\end{array} \\right) \\quad$$\n",
    "* How do we project $x$ on it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Projecting\n",
    "\n",
    "$$ a_1 = \\left( \\begin{array}{c}\n",
    "-0.390 \\\\\n",
    "0.089 \\\\\n",
    "-0.916\n",
    "\\end{array} \\right) \\quad\n",
    "a_2 = \\left( \\begin{array}{c}\n",
    "-0.639 \\\\\n",
    "-0.742 \\\\\n",
    "0.200\n",
    "\\end{array} \\right) \\quad\n",
    "a_3 = \\left( \\begin{array}{c}\n",
    "-0.663 \\\\\n",
    "0.664 \\\\\n",
    "0.346\n",
    "\\end{array} \\right) \\quad$$\n",
    "\n",
    "\n",
    "$$ z = A^\\top x = \\left( \\begin{array}{ccc}\n",
    "-0.390 & 0.089 & -0.916\\\\\n",
    "-0.639 & -0.742 & 0.200 \\\\\n",
    "-0.663 & 0.664 & 0.346\n",
    "\\end{array} \\right)\n",
    "\\left( \\begin{array}{c}\n",
    "-0.343 \\\\\n",
    "-0.754 \\\\\n",
    "0.241\n",
    "\\end{array} \\right) = \n",
    "\\left( \\begin{array}{c}\n",
    "-1.154 \\\\\n",
    "0.828 \\\\\n",
    "0.190\n",
    "\\end{array} \\right)$$\n",
    "\n",
    "That is: \n",
    "$$ z = -1.154 a_1 + 0.828 a_2 + 0.190 a_3$$\n",
    "\n",
    "(Example from [Mohammed J. Zaki, Ch7](https://www.amazon.com/Data-Mining-Analysis-Fundamental-Algorithms/dp/0521766338) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* So how do we find those $a_i$?\n",
    "    * Orthonormality\n",
    "    * Maximize variance\n",
    "* Vectors $a_i$ are called principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Construction of PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Construction of PCA\n",
    "\n",
    "* Principal components $a_{1},a_{2},...a_{D}\\in\\mathbb{R}^{D}$ are orthonormal: $$\\langle a_{i},a_{j}\\rangle=\\begin{cases}\n",
    "1, & i=j\\\\\n",
    "0 & i\\ne j\n",
    "\\end{cases}$$\n",
    "* Maximize variance\n",
    "    * Datapoints in $X$ assumed to be centralized (and scaled)\n",
    "    * $z_i = a^\\top x_i$ - projection of $x_i$ on $a$\n",
    "    * Variance across principal component $a$ for dataset\n",
    "    $$\n",
    "    \\begin{align} \\sigma^2_a & = \\frac{1}{n}\\sum\\limits_{i=1}^n(z_i - \\mu_z)^2 \\\\\n",
    "    & = \\frac{1}{n}\\sum\\limits_{i=1}^n(a^\\top x_i)^2 \\\\\n",
    "    & = \\frac{1}{n}\\sum\\limits_{i=1}^n a^\\top( x_i x_i^\\top) a \\\\\n",
    "    & = a^\\top \\left(\\frac{1}{n}\\sum\\limits_{i=1}^n x_i x_i^\\top \\right) a \\\\\n",
    "    & = a^\\top C a \\rightarrow \\\\\n",
    "    & = a^\\top X^\\top X a \\rightarrow \\max_w \\\\\n",
    "    \\end{align}\n",
    "    $$\n",
    "    * $C = X^\\top X$ - convariance (correlation in case of scaled dataset) matrix\n",
    "\n",
    "    * [Interpreting Covariance Matrix](http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Construction of PCA\n",
    "\n",
    "\n",
    "1. $a_{1}$ is selected to maximize $a_1^\\top X^\\top X a_1$\n",
    "subject to $\\langle a_{1},a_{1}\\rangle=1$\n",
    "2. $a_{2}$ is selected to maximize $a_2^\\top X^\\top X a_2$\n",
    "subject to $\\langle a_{2},a_{2}\\rangle=1$, $\\langle a_{2},a_{1}\\rangle=0$\n",
    "3. $a_{3}$ is selected to maximize $a_3^\\top X^\\top X a_3$\n",
    "subject to $\\langle a_{3},a_{3}\\rangle=1$, $\\langle a_{3},a_{1}\\rangle=\\langle a_{3},a_{2}\\rangle=0$\n",
    "\n",
    "\n",
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Derivation of 1st component\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "a_1^\\top X^\\top X a_1 \\rightarrow \\max_{a_1} \\\\\n",
    "a_1^\\top a_1 = 1\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "* Lagrangian of optimization problem\n",
    "$$ \\mathcal{L}(a_1, \\nu) = a_1^\\top X^\\top X a_1 - \\nu (a_1^\\top a_1 - 1) \\rightarrow max_{a_1, \\nu}$$\n",
    "* Derivative w.r.t. $a_1$\n",
    "$$ \\frac{\\partial\\mathcal{L}}{\\partial a_1} = 2X^\\top X a_1 - 2\\nu a_1 = 0 $$\n",
    "* so $a_1$ is selected from a set of eigenvectors of  $X^\\top X$. But which one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Useful properties of $X^\\top X$\n",
    "\n",
    "* $X^\\top X$ - symmetric and positive semi-definite\n",
    "    * $(X^\\top X)^\\top = X^\\top X$\n",
    "    * $\\forall w \\in \\mathbb{R}^D:\\ w^\\top (X^\\top X) w \\geq 0$\n",
    "* Properties\n",
    "    * All eigenvalues $\\lambda_i \\in \\mathbb{R}, \\lambda_i \\geq 0$ (we also assume $\\lambda_1 \\geq \\lambda_2  \\geq \\dots \\geq \\lambda_d $)\n",
    "    * Eigenvectors for $\\lambda_i \\neq \\lambda_j $ are orthogonal: $v_i^\\top v_j = 0$\n",
    "    * For each unique eigenvalue $\\lambda_i$ there is a unique $v_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Back to component 1\n",
    "\n",
    "Initially, our objective was \n",
    "$$a_1^\\top X^\\top X a_1  \\rightarrow \\max_{a_1}$$\n",
    "\n",
    "From lagrangian we derived that \n",
    "$$X^\\top X a_1 = \\nu a_1$$\n",
    "\n",
    "Putting one in to another:\n",
    "$$ a_1^\\top X^\\top X a_1 = \\nu a_1^\\top a_1 = \\nu \\rightarrow \\max$$\n",
    "\n",
    "That means:\n",
    "* $\\nu$ should be the greatest eigenvalue of matrix $X^\\top X$, which is $\\lambda_1$\n",
    "* $a_1$ is eigenvector, correspondent to $\\lambda_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Derivation of 2nd component\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "a_2^\\top X^\\top X a_2 \\rightarrow \\max_{a_2} \\\\\n",
    "a_2^\\top a_2 = 1 \\\\\n",
    "a_2^\\top a_1 = 0\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "* Lagrangian of optimization problem\n",
    "$$ \\mathcal{L}(a_2, \\nu,\\alpha) = a_2^\\top X^\\top X a_2 - \\nu (a_2^\\top a_2 - 1) - \\alpha (a_1^\\top a_2) \\rightarrow max_{a_2, \\nu,\\alpha}$$\n",
    "* Derivative w.r.t. $a_2$\n",
    "$$ \\frac{\\partial\\mathcal{L}}{\\partial a_2} = 2X^\\top X a_2 - 2\\nu a_2 - \\alpha a_1 = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Derivation of 2nd component\n",
    "* By multiplying by $a_1^\\top$ :\n",
    "$$ a_1^\\top\\frac{\\partial\\mathcal{L}}{\\partial a_2} = 2a_1^\\top X^\\top X a_2 - 2\\nu a_1^\\top a_2 - \\alpha a_1^\\top a_1 = 0 $$\n",
    "\n",
    "* It follows that $\\alpha a_1^\\top a_1 = \\alpha = 0$, which means that \n",
    "$$ \\frac{\\partial\\mathcal{L}}{\\partial a_2} = 2X^\\top X a_2 - \\nu a_2 = 0 $$\n",
    "And $a_2$ is selected from a set of eigenvectors of $X^\\top X$. Again, which one?\n",
    "\n",
    "Derivations of other components proceeds in the same manner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## PCA algorithm\n",
    "\n",
    "1. Center (and scale) dataset\n",
    "2. Calculate covariance matrix $С=X^\\top X$\n",
    "3. Find first $k$ eigenvalues and eigenvectors\n",
    "$$A = \n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "    \\mid & \\mid & & \\mid\\\\\n",
    "    a_{1} & a_{2} & \\ldots & a_{k} \\\\\n",
    "    \\mid & \\mid & & \\mid \n",
    "  \\end{array}\n",
    "\\right]\n",
    "$$\n",
    "4. Perform projection:\n",
    "$$ Z = XA $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Explained variance or why do we neeed $\\lambda$?\n",
    "\n",
    "* As we remember\n",
    "$$a_i^\\top X^\\top X a_i = \\lambda_i,$$\n",
    "which means that $\\lambda_i$ shows variance of data \"explained\" by $a_i$\n",
    "* We can calculate explained variance ratio for $a_i$:\n",
    "$$\n",
    "\\frac{\\lambda_{i}}{\\sum_{d=1}^{D}\\lambda_{d}}\n",
    "$$\n",
    "\n",
    "<center><img src='img/cumul_rat.png' width=1900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# (Supplementary) alternative view on PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Best hyperplane fit\n",
    "\n",
    "For point $x$ and subspace $L$ denote: \n",
    "\n",
    "* $p$: the projection of $x$ on $L$ \n",
    "* $h$: orthogonal complement \n",
    "* $x=p+h$, $\\langle p,h\\rangle=0$, \n",
    "\n",
    "#### Proposition\n",
    "$\\|x\\|^2 = \\|p\\|^2 + \\|h\\|^2$\n",
    "\n",
    "For training set $x_{1},x_{2},...x_{N}$ and subspace $L$  we can find:\n",
    "\n",
    "* projections: $p_{1},p_{2},...p_{N}$\n",
    "* orthogonal complements: $h_{1},h_{2},...h_{N}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Best subspace fit\n",
    "#### Definition\n",
    "Best-fit $k$-dimentional subspace for a set of points $x_1 \\dots x_N$ is a subspace, spanned by $k$ vectors $v_1$, $v_2$, $\\dots$, $v_k$, solving\n",
    "\n",
    "$$ \\sum_{n=1}^N \\| h_n \\| ^2 \\rightarrow \\min\\limits_{v_1, v_2,\\dots,v_k}$$\n",
    "\n",
    "or\n",
    "\n",
    "$$ \\sum_{n=1}^N \\| p_n \\| ^2 \\rightarrow \\max\\limits_{v_1, v_2,\\dots,v_k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Definition of PCA\n",
    "\n",
    "Principal components $a_1, a_2, \\dots, a_k$ are vectors, forming orthonormal basis in the k-dimentional subspace of best fit\n",
    "\n",
    "#### Properties\n",
    "* Not invariant to translation:\n",
    "    * center data before PCA: \n",
    "$$\n",
    "x\\leftarrow x-\\mu\\text{ where }\\mu=\\frac{1}{N}\\sum_{n=1}^{N}x_{n}\n",
    "$$\n",
    "* Not invariant to scaling:\n",
    "    * scale features to have unit variance before PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: line of best fit\n",
    "\n",
    "* In PCA the sum of squared perpendicular distances to line is minimized:\n",
    "\n",
    "<center><img src='img/line_best_fit.png' width=500></center>\n",
    "\n",
    "* What is the difference with least squares minimization in regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: plane of best fit\n",
    "\n",
    "<center><img src='img/plane_best_fit.png' width=500></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Construction of PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Construction of PCA\n",
    "\n",
    "* Datapoints in $X$ assumed to be centralized and scaled (!)\n",
    "* Principal components $a_{1},a_{2},...a_{D}\\in\\mathbb{R}^{D}$ are\n",
    "found such that $\\langle a_{i},a_{j}\\rangle=\\begin{cases}\n",
    "1, & i=j\\\\\n",
    "0 & i\\ne j\n",
    "\\end{cases}$\n",
    "* $Xa_{i}$ is a vector of projections of all objects onto the $i$-th principal component.\n",
    "* For any object $x$ its projections onto principal components are\n",
    "equal to:\n",
    "$$\n",
    "p=A^{T}x=[\\langle a_{1},x\\rangle,...\\langle a_{D},x\\rangle]^{T}\n",
    "$$\n",
    "where $A=[a_{1};a_{2};...a_{D}]\\in\\mathbb{R}^{DxD}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Construction of PCA\n",
    "\n",
    "\n",
    "1. $a_{1}$ is selected to maximize $\\left\\lVert Xa_{1}\\right\\rVert $\n",
    "subject to $\\langle a_{1},a_{1}\\rangle=1$\n",
    "2. $a_{2}$ is selected to maximize $\\left\\lVert Xa_{2}\\right\\rVert $\n",
    "subject to $\\langle a_{2},a_{2}\\rangle=1$, $\\langle a_{2},a_{1}\\rangle=0$\n",
    "3. $a_{3}$ is selected to maximize $\\left\\lVert Xa_{3}\\right\\rVert $\n",
    "subject to $\\langle a_{3},a_{3}\\rangle=1$, $\\langle a_{3},a_{1}\\rangle=\\langle a_{3},a_{2}\\rangle=0$\n",
    "\n",
    "\n",
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA summary\n",
    "\n",
    "* Dimensionality reduction - common preprocessing step for efficiency and numerical stability.\n",
    "* Solution vectors $a_1,\\dots,a_k$ are called top $k$ principal components.\n",
    "* Principal component analysis - expression of $x$ in terms of first $k$ principal components.\n",
    "* It is unsupervised linear dimensionality reduction.\n",
    "* Solution is achieved on top $k$ eigenvectors $a_{1},...a_{k}$ of covariance matrix.\n",
    "* [PCA tutorial](https://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# t-SNE\n",
    "## t-distributed stochastic neighbor embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multidimentional scaling - intuinition\n",
    "\n",
    "Find feature space with lesser dimentions s.t. **distances** in initial space are conserved in the new one. A bit more formally:\n",
    "\n",
    "* Given $X = [x_1,\\dots, x_n]\\in \\mathbb{R}^{N \\times D}$ and/or $\\delta_{ij}$ - proximity measure between $(x_i,x_j)$ in initial feature space.\n",
    "* Find $Y = [y_1,\\dots,y_n] \\in \\mathbb{R}^{N \\times d}$ such that $\\delta_{ij} \\approx d(y_i, y_j) = \\|y_i-y_j\\|^2$\n",
    "\n",
    "<center><img src='img/mds.png' width =1200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multidimentional scaling\n",
    "\n",
    "It is clear, that most of the times distances won't be conserved completely:\n",
    "<center><img src='img/sphere_example.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multidimentional scaling approaches\n",
    "\n",
    "* classical MDS (essentially PCA)\n",
    "* metric MDS\n",
    "* non-metric MDS\n",
    "\n",
    "\n",
    "* But what if we want to conserve not the distances themselves, but the structure of inital dataset? Like neighbourhood of each point? \n",
    "* Here comes t-SNE !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## T-SNE\n",
    "\n",
    "* t-SNE - is not multidimentional scaling, but the goal is somewhat similar\n",
    "* There are going to be 3 types of similarities:\n",
    "    * Similarity between points in initial feature space\n",
    "    * Similarity between points in derived feature space\n",
    "    * Similarity between feature spaces!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## T-SNE\n",
    "\n",
    "* Similarity between points in initial feature space $\\mathbb{R}^D$\n",
    "$$\n",
    "p(i, j) = \\frac{p(i | j) + p(j | i)}{2N}, \\quad p(j | i) = \\frac{\\exp(-\\|\\mathbf{x}_j-\\mathbf{x}_i\\|^2/{2 \\sigma_i^2})}{\\sum_{k \\neq i}\\exp(-\\|\\mathbf{x}_k-\\mathbf{x}_i\\|^2/{2 \\sigma_i^2})}\n",
    "$$\n",
    "$\\sigma_i$ is set by user (implicitly)\n",
    "* Similarity between points in derived feature space $\\mathbb{R}^d, d << D$\n",
    "$$\n",
    "q(i, j) = \\frac{g(|\\mathbf{y}_i - \\mathbf{y}_j|)}{\\sum_{k \\neq l} g(|\\mathbf{y}_i - \\mathbf{y}_j|)}\n",
    "$$ \n",
    "where $g(z) = \\frac{1}{1 + z^2}$ - is student t-distribution with dof=$1$\n",
    "* Similarity between feature spaces (Kullback–Leibler divergence)\n",
    "$$\n",
    "J_{t-SNE}(y) = KL(P \\| Q) = \\sum_i \\sum_j p(i, j) \\log \\frac{p(i, j)}{q(i, j)} \\rightarrow \\min\\limits_{\\mathbf{y}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## KL-Divergence\n",
    "$$\n",
    "KL(P \\| Q) = \\sum_z P(z) \\log \\frac{P(z)}{Q(z)}\n",
    "$$\n",
    "\n",
    "<center><img src='img/kld.png' width=1000></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optimization\n",
    "\n",
    "* Optimize $J_{t-SNE}(y)$ with SGD\n",
    "\n",
    "$$\\frac{\\partial J_{t-SNE}}{\\partial y_i}=4 \\sum_j(p(i,j)−q(i,j))(y_i−y_j)g(|y_i−y_j|)$$\n",
    "\n",
    "* [Article](http://jmlr.csail.mit.edu/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)\n",
    "* [Examples](http://lvdmaaten.github.io/tsne/)\n",
    "* [Demo and advises](http://distill.pub/2016/misread-tsne/)\n",
    "    * t-SNE is unstable\n",
    "    * Size of clusters means anything\n",
    "    * Distance means anything\n",
    "    * Random data can provide structure"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/1f8c4751e12938961e423759861e6e5a"
  },
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "gist": {
   "data": {
    "description": "CloudMail/hse-da-course/raw/lecture-intro/lecture-intro-v01.ipynb",
    "public": false
   },
   "id": "1f8c4751e12938961e423759861e6e5a"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "livereveal": {
   "theme": "serif",
   "transition": "concave"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "513px",
    "width": "253px"
   },
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "toc_position": {
   "height": "32px",
   "left": "9px",
   "right": "1379px",
   "top": "33px",
   "width": "212px"
  },
  "widgets": {
   "state": {
    "54e80d57f79b4bfc934a2b84cf5fe7ba": {
     "views": [
      {
       "cell_index": 47
      }
     ]
    },
    "5fb17a3592634a4fba98446dacd6db43": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "6f6f6ce7b81743308b92966f225862a8": {
     "views": [
      {
       "cell_index": 34
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
