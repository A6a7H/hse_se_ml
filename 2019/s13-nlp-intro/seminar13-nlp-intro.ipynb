{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/logo_hse_black.jpg\"></center>\n",
    "\n",
    "<h1><center>Data Analysis</center></h1>\n",
    "<h2><center>Seminar Introduction to Natural Language Processing<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup></center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis in Russian (from https://www.kaggle.com/c/sentiment-analysis-in-russian/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('Data/train.json') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/r.britkov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "word_tokenizer = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    texts = []\n",
    "    targets = []\n",
    "\n",
    "    for item in data:\n",
    "        if item['sentiment'] == 'negative':\n",
    "            targets.append(0)\n",
    "        else:\n",
    "            targets.append(1)\n",
    "        \n",
    "        tokens = word_tokenizer.tokenize(item['text'].lower())\n",
    "        \n",
    "        #delete punct and stop words\n",
    "        tokens = [word for word in tokens if (word not in string.punctuation and word not in stop_words)]\n",
    "        \n",
    "        texts.append(tokens)\n",
    "    \n",
    "    return texts, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, y = process_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize words. 2 Ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1)Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer \n",
    "\n",
    "stemmer = SnowballStemmer(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a32f4d61b84fd9bb7650b9fba9ce96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "texts_copy = texts[:100]\n",
    "for i in tqdm_notebook(range(len(texts_copy))):\n",
    "    texts_copy[i] = ' '.join(list(map(stemmer.stem, texts_copy[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "досудебн расследован факт покупк енпф пакет облигац то бузгул аурум начат инициатив национальн банк рк сообщ директор департамент защит прав потребител финансов услуг нацбанк казахста александр терент основан досудебн расследован стал обращен национальн банк письм 25 ноябр 2016 год обращен национальн банк правоохранительн орга нам эт сделк показа сомнительн недостаточн корректн поэт нацбанк 25 ноябр 2016 год обрат правоохранительн орга эт мог озвуч сегодн идет следств провод проверк \", – сказа терент 28 декабр нацбанк заяв знают стал основан проверк енпф 23 декабр факт проведен проверк а един накопительн пенсион фонд подтверд пресс служб национальн банк сообщ проверк провод операц совершен а енпф отношен инвестирован собствен актив такж финрегулятор сообща сделк енпф сумм пят млрд завед уголовн дел нацбанк заверя все происходя затрагива пенсион накоплен казахстанц нашл ошибк текст выдел мыш нажм ctrl enter\n"
     ]
    }
   ],
   "source": [
    "print(texts_copy[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages: fast\n",
    "\n",
    "Disadvantages: not very intellectual normalization\n",
    "\n",
    "Alternative: lemmatization (but this method slomly than stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2)Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e573ed746400490fabf7d41ff39fcb7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "texts_copy = texts[:100]\n",
    "for i in tqdm_notebook(range(len(texts_copy))):\n",
    "    texts_copy[i] = ' '.join([morph.parse(word)[0].normal_form for word in texts_copy[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "досудебный расследование факт покупка енпф пакет облигация тоо бузгул аурум начать инициатива национальный банка рк сообщить директор департамент защита право потребитель финансовый услуга нацбанк казахстан александр терентьев основание досудебный расследование стать обращение национальный банка письмо 25 ноябрь 2016 год обращение национальный банка правоохранительный орган мы этот сделка показаться сомнительный недостаточно корректный поэтому нацбанк 25 ноябрь 2016 год обратиться правоохранительный орган это мочь озвучить сегодня идти следствие проводиться проверка \", – сказать терентьев 28 декабрь нацбанк заявить знать стать основание проверка енпф 23 декабрь факт проведение проверка ао единый накопительный пенсионный фонд подтвердиться пресс служба национальный банка сообщить проверка проводить операция совершить ао енпф отношение инвестирование собственный актив также финрегулятор сообщать сделка енпф сумма пять миллиард завести уголовный дело нацбанк заверять весь происходить затрагивать пенсионный накопление казахстанец найти ошибка текст выделить мыший нажать ctrl enter\n"
     ]
    }
   ],
   "source": [
    "print(texts_copy[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use stemmer because it is enough for our task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c34fa658cee48f59d816c3fbbe1003e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8263), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm_notebook(range(len(texts))):\n",
    "    texts[i] = ' '.join(list(map(stemmer.stem, texts[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, test_texts, train_y, test_y = train_test_split(texts, y, test_size=0.33, random_state=42, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF: method to measure importance of word in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "TF-IDF(t,d) = tf(t,d) \\cdot idf(t, D)\n",
    "$$\n",
    "\n",
    "where $tf(t,d)$ (in simple case) raw count of a term t in a document d.\n",
    "$$\n",
    "idf(t, D) = \\log \\frac{N}{N(t)}\n",
    "$$\n",
    "where $N$ - total number of documents, $N(t)$ - total number of documents with term t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example on small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '006',\n",
       " '01',\n",
       " '013',\n",
       " '018',\n",
       " '02',\n",
       " '025',\n",
       " '03',\n",
       " '04',\n",
       " '05',\n",
       " '06',\n",
       " '07',\n",
       " '08',\n",
       " '080',\n",
       " '089',\n",
       " '09',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '100er',\n",
       " '102',\n",
       " '103',\n",
       " '104',\n",
       " '105',\n",
       " '106',\n",
       " '107',\n",
       " '1089',\n",
       " '11',\n",
       " '113',\n",
       " '114',\n",
       " '115',\n",
       " '116',\n",
       " '117',\n",
       " '118',\n",
       " '119',\n",
       " '12',\n",
       " '120',\n",
       " '1200',\n",
       " '121',\n",
       " '122',\n",
       " '125',\n",
       " '127',\n",
       " '13',\n",
       " '130',\n",
       " '137',\n",
       " '1380',\n",
       " '139',\n",
       " '14',\n",
       " '140',\n",
       " '141',\n",
       " '1418',\n",
       " '142',\n",
       " '1446',\n",
       " '146',\n",
       " '148',\n",
       " '1497',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '1503',\n",
       " '153',\n",
       " '1560',\n",
       " '159',\n",
       " '16',\n",
       " '160',\n",
       " '161',\n",
       " '162',\n",
       " '1640',\n",
       " '165',\n",
       " '168',\n",
       " '16г',\n",
       " '17',\n",
       " '170',\n",
       " '1719',\n",
       " '1725',\n",
       " '173',\n",
       " '1739',\n",
       " '1759',\n",
       " '176',\n",
       " '177',\n",
       " '1775',\n",
       " '178',\n",
       " '179',\n",
       " '18',\n",
       " '180',\n",
       " '1800',\n",
       " '181',\n",
       " '1831',\n",
       " '185',\n",
       " '187',\n",
       " '188',\n",
       " '189',\n",
       " '1899',\n",
       " '19',\n",
       " '190',\n",
       " '1900',\n",
       " '1902',\n",
       " '1913',\n",
       " '1932',\n",
       " '1934',\n",
       " '1936',\n",
       " '1938',\n",
       " '1939',\n",
       " '1940',\n",
       " '1943',\n",
       " '1946',\n",
       " '1947',\n",
       " '1949',\n",
       " '195',\n",
       " '1950',\n",
       " '1952',\n",
       " '1953',\n",
       " '1954',\n",
       " '1955',\n",
       " '1956',\n",
       " '1957',\n",
       " '1958',\n",
       " '1959',\n",
       " '1960',\n",
       " '1962',\n",
       " '1964',\n",
       " '1965',\n",
       " '1970',\n",
       " '1971',\n",
       " '1972',\n",
       " '1973',\n",
       " '1976',\n",
       " '1977',\n",
       " '1978',\n",
       " '1979',\n",
       " '198',\n",
       " '1981',\n",
       " '1982',\n",
       " '1983',\n",
       " '1984',\n",
       " '1985',\n",
       " '1986',\n",
       " '1987',\n",
       " '1988',\n",
       " '1989',\n",
       " '1990',\n",
       " '1991',\n",
       " '1992',\n",
       " '1993',\n",
       " '1994',\n",
       " '1995',\n",
       " '1996',\n",
       " '1997',\n",
       " '1998',\n",
       " '1999',\n",
       " '1й',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '2001',\n",
       " '2002',\n",
       " '2003',\n",
       " '2004',\n",
       " '2005',\n",
       " '2006',\n",
       " '200614',\n",
       " '2007',\n",
       " '2008',\n",
       " '2008г',\n",
       " '2009',\n",
       " '2009г',\n",
       " '2010',\n",
       " '2011',\n",
       " '2012',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '2016',\n",
       " '2016г',\n",
       " '2017',\n",
       " '2018',\n",
       " '2019',\n",
       " '2020',\n",
       " '2021',\n",
       " '2025',\n",
       " '2027',\n",
       " '2030',\n",
       " '2031',\n",
       " '2036',\n",
       " '2050',\n",
       " '206',\n",
       " '209',\n",
       " '21',\n",
       " '2121',\n",
       " '213',\n",
       " '215',\n",
       " '22',\n",
       " '220',\n",
       " '221',\n",
       " '2269',\n",
       " '229',\n",
       " '23',\n",
       " '236',\n",
       " '237',\n",
       " '2372',\n",
       " '24',\n",
       " '2400',\n",
       " '242',\n",
       " '247',\n",
       " '25',\n",
       " '250',\n",
       " '257',\n",
       " '26',\n",
       " '260',\n",
       " '266',\n",
       " '268',\n",
       " '27',\n",
       " '270',\n",
       " '271',\n",
       " '274',\n",
       " '28',\n",
       " '280',\n",
       " '2851',\n",
       " '29',\n",
       " '292',\n",
       " '293',\n",
       " '2а',\n",
       " '30',\n",
       " '300',\n",
       " '3000',\n",
       " '301',\n",
       " '31',\n",
       " '312',\n",
       " '3143',\n",
       " '32',\n",
       " '320',\n",
       " '3220',\n",
       " '3259',\n",
       " '33',\n",
       " '330',\n",
       " '34',\n",
       " '3417',\n",
       " '3426',\n",
       " '345',\n",
       " '346',\n",
       " '348',\n",
       " '35',\n",
       " '350',\n",
       " '35992',\n",
       " '36',\n",
       " '367',\n",
       " '369',\n",
       " '37',\n",
       " '374',\n",
       " '388',\n",
       " '38а',\n",
       " '39',\n",
       " '391',\n",
       " '397',\n",
       " '399',\n",
       " '3d',\n",
       " '3x3',\n",
       " '3й',\n",
       " '3х3',\n",
       " '40',\n",
       " '400',\n",
       " '403',\n",
       " '408',\n",
       " '41',\n",
       " '42',\n",
       " '420',\n",
       " '423',\n",
       " '428',\n",
       " '43',\n",
       " '434',\n",
       " '435',\n",
       " '44',\n",
       " '443',\n",
       " '446',\n",
       " '45',\n",
       " '459',\n",
       " '46',\n",
       " '47',\n",
       " '473',\n",
       " '4785',\n",
       " '479',\n",
       " '48',\n",
       " '483',\n",
       " '484',\n",
       " '4842144',\n",
       " '489',\n",
       " '49',\n",
       " '4b02',\n",
       " '50',\n",
       " '500',\n",
       " '51',\n",
       " '52',\n",
       " '520',\n",
       " '53',\n",
       " '530',\n",
       " '53090',\n",
       " '535',\n",
       " '536',\n",
       " '54',\n",
       " '543',\n",
       " '54а',\n",
       " '55',\n",
       " '553',\n",
       " '56',\n",
       " '569',\n",
       " '57',\n",
       " '58',\n",
       " '580',\n",
       " '581',\n",
       " '582',\n",
       " '583',\n",
       " '589',\n",
       " '59',\n",
       " '60',\n",
       " '600',\n",
       " '605',\n",
       " '61',\n",
       " '614',\n",
       " '619',\n",
       " '61а',\n",
       " '62',\n",
       " '622',\n",
       " '626',\n",
       " '627',\n",
       " '63',\n",
       " '64',\n",
       " '640',\n",
       " '642',\n",
       " '647',\n",
       " '65',\n",
       " '6506',\n",
       " '651',\n",
       " '652',\n",
       " '653',\n",
       " '656',\n",
       " '66',\n",
       " '67',\n",
       " '68',\n",
       " '680',\n",
       " '69',\n",
       " '690',\n",
       " '691',\n",
       " '6937',\n",
       " '70',\n",
       " '700',\n",
       " '718',\n",
       " '72',\n",
       " '73',\n",
       " '7303',\n",
       " '734',\n",
       " '74',\n",
       " '742',\n",
       " '744',\n",
       " '75',\n",
       " '76',\n",
       " '766',\n",
       " '77',\n",
       " '78',\n",
       " '780',\n",
       " '79',\n",
       " '80',\n",
       " '800',\n",
       " '802',\n",
       " '807',\n",
       " '809',\n",
       " '81',\n",
       " '810',\n",
       " '8120',\n",
       " '815',\n",
       " '82',\n",
       " '824',\n",
       " '83',\n",
       " '8340',\n",
       " '84',\n",
       " '841',\n",
       " '845',\n",
       " '84а',\n",
       " '85',\n",
       " '850',\n",
       " '853',\n",
       " '86',\n",
       " '869',\n",
       " '87',\n",
       " '875',\n",
       " '88',\n",
       " '883',\n",
       " '89',\n",
       " '90',\n",
       " '900',\n",
       " '906',\n",
       " '908',\n",
       " '91',\n",
       " '92',\n",
       " '93',\n",
       " '932',\n",
       " '94',\n",
       " '945',\n",
       " '946',\n",
       " '948',\n",
       " '95',\n",
       " '953',\n",
       " '96',\n",
       " '960',\n",
       " '961',\n",
       " '97',\n",
       " '977',\n",
       " '98',\n",
       " '981',\n",
       " '99',\n",
       " 'ab',\n",
       " 'ads',\n",
       " 'aic',\n",
       " 'akcijj',\n",
       " 'alata',\n",
       " 'almat',\n",
       " 'alst',\n",
       " 'anylogic',\n",
       " 'appl',\n",
       " 'aqu',\n",
       " 'ar',\n",
       " 'artspac',\n",
       " 'as',\n",
       " 'asta',\n",
       " 'at',\n",
       " 'aud',\n",
       " 'baiterek',\n",
       " 'bank',\n",
       " 'banker',\n",
       " 'batl',\n",
       " 'benz',\n",
       " 'best',\n",
       " 'bfg',\n",
       " 'big',\n",
       " 'billing',\n",
       " 'bmw',\n",
       " 'bombardier',\n",
       " 'brent',\n",
       " 'broadcasting',\n",
       " 'business',\n",
       " 'by',\n",
       " 'call',\n",
       " 'camr',\n",
       " 'capita',\n",
       " 'car',\n",
       " 'carlton',\n",
       " 'cbonds',\n",
       " 'cd',\n",
       " 'chamoionships',\n",
       " 'chevrolet',\n",
       " 'cib',\n",
       " 'cisc',\n",
       " 'com',\n",
       " 'comments',\n",
       " 'communit',\n",
       " 'computer',\n",
       " 'consulting',\n",
       " 'corad',\n",
       " 'coroll',\n",
       " 'corporation',\n",
       " 'crj',\n",
       " 'ctrl',\n",
       " 'cредств',\n",
       " 'daewo',\n",
       " 'dailynews',\n",
       " 'datsun',\n",
       " 'deloitt',\n",
       " 'di',\n",
       " 'disqus',\n",
       " 'doing',\n",
       " 'economist',\n",
       " 'ema',\n",
       " 'enabl',\n",
       " 'energ',\n",
       " 'energypr',\n",
       " 'eng',\n",
       " 'enter',\n",
       " 'erg',\n",
       " 'ex',\n",
       " 'exp',\n",
       " 'facebook',\n",
       " 'fib',\n",
       " 'financia',\n",
       " 'finpr',\n",
       " 'fis',\n",
       " 'footprints',\n",
       " 'for',\n",
       " 'forbes',\n",
       " 'fortebank',\n",
       " 'fund',\n",
       " 'gaz',\n",
       " 'generalelectric',\n",
       " 'globa',\n",
       " 'gmbh',\n",
       " 'gobain',\n",
       " 'gold',\n",
       " 'googl',\n",
       " 'grant',\n",
       " 'graphics',\n",
       " 'group',\n",
       " 'halyk',\n",
       " 'hollywood',\n",
       " 'html',\n",
       " 'http',\n",
       " 'hyunda',\n",
       " 'ifc',\n",
       " 'ilint',\n",
       " 'immersiv',\n",
       " 'in',\n",
       " 'infinit',\n",
       " 'informbur',\n",
       " 'innovation',\n",
       " 'inoventic',\n",
       " 'intelligenc',\n",
       " 'internationa',\n",
       " 'invest',\n",
       " 'investment',\n",
       " 'ip',\n",
       " 'isunplast',\n",
       " 'isuz',\n",
       " 'it',\n",
       " 'iv',\n",
       " 'ivec',\n",
       " 'jac',\n",
       " 'javascript',\n",
       " 'kamaz',\n",
       " 'kapita',\n",
       " 'kas',\n",
       " 'kaz',\n",
       " 'kazakhsta',\n",
       " 'kaznex',\n",
       " 'kaznexinvest',\n",
       " 'kazpravd',\n",
       " 'kazsport',\n",
       " 'kazyn',\n",
       " 'kegoc',\n",
       " 'ki',\n",
       " 'kids',\n",
       " 'kljuchevojj',\n",
       " 'kn',\n",
       " 'kookmin',\n",
       " 'kpmg',\n",
       " 'kulansh',\n",
       " 'kz',\n",
       " 'laborator',\n",
       " 'lad',\n",
       " 'largus',\n",
       " 'lexus',\n",
       " 'lifa',\n",
       " 'liter',\n",
       " 'lpi',\n",
       " 'lrt',\n",
       " 'ls',\n",
       " 'ma',\n",
       " 'mad',\n",
       " 'mercedes',\n",
       " 'min',\n",
       " 'motor',\n",
       " 'motors',\n",
       " 'nationa',\n",
       " 'ncoc',\n",
       " 'nedc',\n",
       " 'news',\n",
       " 'nissa',\n",
       " 'nomad',\n",
       " 'otyrar',\n",
       " 'pacific',\n",
       " 'pages',\n",
       " 'paket',\n",
       " 'panoram',\n",
       " 'pennenerg',\n",
       " 'pentagon',\n",
       " 'pentagonon',\n",
       " 'peugeot',\n",
       " 'pleas',\n",
       " 'plm',\n",
       " 'pol',\n",
       " 'porsch',\n",
       " 'powered',\n",
       " 'pravd',\n",
       " 'prior',\n",
       " 'ra',\n",
       " 'raex',\n",
       " 'renault',\n",
       " 'resourc',\n",
       " 'ri',\n",
       " 'ritz',\n",
       " 'road',\n",
       " 'ru',\n",
       " 'rur',\n",
       " 'russ',\n",
       " 'rx',\n",
       " 'rx1e',\n",
       " 'saint',\n",
       " 'samruk',\n",
       " 'samsung',\n",
       " 'samura',\n",
       " 'sberbank',\n",
       " 'scat',\n",
       " 'seme',\n",
       " 'shell',\n",
       " 'show',\n",
       " 'siemens',\n",
       " 'sim',\n",
       " 'skod',\n",
       " 'skyp',\n",
       " 'slk',\n",
       " 'smart',\n",
       " 'snowkidz',\n",
       " 'soft',\n",
       " 'softwar',\n",
       " 'solaris',\n",
       " 'sos',\n",
       " 'specia',\n",
       " 'sputnik',\n",
       " 'sputniknews',\n",
       " 'su',\n",
       " 'systems',\n",
       " 'telegr',\n",
       " 'tengrinews',\n",
       " 'the',\n",
       " 'to',\n",
       " 'tobolinf',\n",
       " 'toda',\n",
       " 'top',\n",
       " 'tota',\n",
       " 'toyot',\n",
       " 'tre',\n",
       " 'trucks',\n",
       " 'trust',\n",
       " 'twitter',\n",
       " 'u18',\n",
       " 'uaz',\n",
       " 'un',\n",
       " 'union',\n",
       " 'universit',\n",
       " 'ventur',\n",
       " 'vest',\n",
       " 'vi',\n",
       " 'view',\n",
       " 'vip',\n",
       " 'vist',\n",
       " 'volkswag',\n",
       " 'vp',\n",
       " 'welt',\n",
       " 'whatsapp',\n",
       " 'world',\n",
       " 'www',\n",
       " 'xi',\n",
       " 'xix',\n",
       " 'xra',\n",
       " 'xv',\n",
       " 'xvi',\n",
       " 'ya',\n",
       " 'yellow',\n",
       " 'yons',\n",
       " 'youtub',\n",
       " 'zakon',\n",
       " 'а320',\n",
       " 'а75',\n",
       " 'аб',\n",
       " 'абайск',\n",
       " 'абден',\n",
       " 'абдибек',\n",
       " 'абдибеков',\n",
       " 'абдикан',\n",
       " 'абдирович',\n",
       " 'абдразаков',\n",
       " 'абдрахман',\n",
       " 'абен',\n",
       " 'абенов',\n",
       " 'абердин',\n",
       " 'абза',\n",
       " 'абильфаизович',\n",
       " 'абонент',\n",
       " 'абсолютн',\n",
       " 'абудж',\n",
       " 'абулхайыр',\n",
       " 'абулхас',\n",
       " 'абхазск',\n",
       " 'абылха',\n",
       " 'ав',\n",
       " 'авар',\n",
       " 'аварийн',\n",
       " 'аватар',\n",
       " 'август',\n",
       " 'авиакомпан',\n",
       " 'авиапромышлен',\n",
       " 'авиацион',\n",
       " 'авив',\n",
       " 'австр',\n",
       " 'австрал',\n",
       " 'авт',\n",
       " 'автобизнес',\n",
       " 'автобус',\n",
       " 'автоваз',\n",
       " 'автовладельц',\n",
       " 'автограф',\n",
       " 'автодел',\n",
       " 'автодилер',\n",
       " 'автодорог',\n",
       " 'автозаправк',\n",
       " 'автозаправочн',\n",
       " 'автоконцерн',\n",
       " 'автокредитован',\n",
       " 'автомат',\n",
       " 'автоматизац',\n",
       " 'автоматическ',\n",
       " 'автомашин',\n",
       " 'автомоб',\n",
       " 'автомобил',\n",
       " 'автомобильн',\n",
       " 'автопарк',\n",
       " 'автопр',\n",
       " 'автопроизводител',\n",
       " 'автопром',\n",
       " 'автопромышлен',\n",
       " 'автор',\n",
       " 'авторетейл',\n",
       " 'авторизова',\n",
       " 'авторитет',\n",
       " 'авторск',\n",
       " 'авторынк',\n",
       " 'автосборочн',\n",
       " 'автосектор',\n",
       " 'автостат',\n",
       " 'автостоянк',\n",
       " 'автотехобслуживан',\n",
       " 'автотранспорт',\n",
       " 'автотранспортн',\n",
       " 'агент',\n",
       " 'агентств',\n",
       " 'агитац',\n",
       " 'аграрн',\n",
       " 'агрессор',\n",
       " 'агропромышлен',\n",
       " 'адаптирова',\n",
       " 'адвокат',\n",
       " 'адил',\n",
       " 'административн',\n",
       " 'администратор',\n",
       " 'администрац',\n",
       " 'администрирован',\n",
       " 'адрес',\n",
       " 'адресн',\n",
       " 'аз',\n",
       " 'азамат',\n",
       " 'азат',\n",
       " 'азевед',\n",
       " 'азербайджа',\n",
       " 'азиатск',\n",
       " 'азирбек',\n",
       " 'азот',\n",
       " 'азс',\n",
       " 'айбек',\n",
       " 'айбын',\n",
       " 'айгер',\n",
       " 'айдар',\n",
       " 'айдарбек',\n",
       " 'айдаркул',\n",
       " 'аймак',\n",
       " 'айманов',\n",
       " 'аймақ',\n",
       " 'айсул',\n",
       " 'айт',\n",
       " 'айтба',\n",
       " 'айтжан',\n",
       " 'айтмагамбет',\n",
       " 'ак',\n",
       " 'ака',\n",
       " 'акаб',\n",
       " 'академ',\n",
       " 'академик',\n",
       " 'академическ',\n",
       " 'акжа',\n",
       " 'акжайык',\n",
       " 'акжар',\n",
       " 'аким',\n",
       " 'акимат',\n",
       " 'акиф',\n",
       " 'акиш',\n",
       " 'акишев',\n",
       " 'аккайынск',\n",
       " 'аккольск',\n",
       " 'аккредитац',\n",
       " 'аккредитова',\n",
       " 'аккумулятор',\n",
       " 'акмол',\n",
       " 'акмолинск',\n",
       " 'акмолинц',\n",
       " 'акорд',\n",
       " 'акса',\n",
       " 'акт',\n",
       " 'акта',\n",
       " 'актауск',\n",
       " 'актер',\n",
       " 'актерск',\n",
       " 'актив',\n",
       " 'активизац',\n",
       " 'активизирова',\n",
       " 'активн',\n",
       " 'актоб',\n",
       " 'актуальн',\n",
       " 'актюбинск',\n",
       " 'акц',\n",
       " 'акцент',\n",
       " 'акционер',\n",
       " 'акционерн',\n",
       " 'ал',\n",
       " 'алата',\n",
       " 'алатаумунайалтын',\n",
       " 'алатауск',\n",
       " 'алаш',\n",
       " 'алашорд',\n",
       " 'албан',\n",
       " 'алгоритм',\n",
       " 'алдажар',\n",
       " 'алдангар',\n",
       " 'алевтин',\n",
       " 'алекеш',\n",
       " 'алекс',\n",
       " 'александр',\n",
       " 'александрович',\n",
       " 'александровн',\n",
       " 'алиба',\n",
       " 'алибек',\n",
       " 'алимбет',\n",
       " 'алимбетов',\n",
       " 'алкогол',\n",
       " 'алкогольн',\n",
       " 'алм',\n",
       " 'алмалинск',\n",
       " 'алмасад',\n",
       " 'алмат',\n",
       " 'алматинск',\n",
       " 'алт',\n",
       " 'алта',\n",
       " 'алтаналмас',\n",
       " 'алтын',\n",
       " 'алтыналмас',\n",
       " 'алфав',\n",
       " 'алфавит',\n",
       " 'алфер',\n",
       " 'алчинбаев',\n",
       " 'альберт',\n",
       " 'альбом',\n",
       " 'альпар',\n",
       " 'альтернативн',\n",
       " 'альфараб',\n",
       " 'аманбек',\n",
       " 'амангельд',\n",
       " 'аманжол',\n",
       " 'амбициозн',\n",
       " 'америк',\n",
       " 'американск',\n",
       " 'амет',\n",
       " 'амирбек',\n",
       " 'ан',\n",
       " 'анализ',\n",
       " 'анализир',\n",
       " 'аналитик',\n",
       " 'аналитическ',\n",
       " 'аналог',\n",
       " 'аналогичн',\n",
       " 'анатол',\n",
       " 'анвар',\n",
       " 'ангарит',\n",
       " 'ангел',\n",
       " 'ангелин',\n",
       " 'англ',\n",
       " 'английск',\n",
       " 'андорр',\n",
       " 'анжелес',\n",
       " 'анк',\n",
       " 'аннотац',\n",
       " 'аннуитет',\n",
       " 'анонсирова',\n",
       " 'анпз',\n",
       " 'ансага',\n",
       " 'антарктид',\n",
       " 'антиконкурентн',\n",
       " 'антимонопольн',\n",
       " 'антитеррористическ',\n",
       " 'антолог',\n",
       " 'антонин',\n",
       " 'анттин',\n",
       " 'апелляцион',\n",
       " 'апенов',\n",
       " 'апк',\n",
       " 'аппарат',\n",
       " 'аппаратн',\n",
       " 'апрел',\n",
       " 'ар',\n",
       " 'арабск',\n",
       " 'арав',\n",
       " 'арафат',\n",
       " 'аргентин',\n",
       " 'арен',\n",
       " 'аренд',\n",
       " 'арендн',\n",
       " 'арест',\n",
       " 'арестова',\n",
       " 'арзымбетов',\n",
       " 'аристократическ',\n",
       " 'арк',\n",
       " 'арм',\n",
       " 'арма',\n",
       " 'армен',\n",
       " 'арт',\n",
       " 'артер',\n",
       " 'артефакт',\n",
       " 'артист',\n",
       " 'артистическ',\n",
       " 'аружа',\n",
       " 'археологическ',\n",
       " 'архив',\n",
       " 'архитектор',\n",
       " 'архитектур',\n",
       " 'архитектурн',\n",
       " 'аршалынск',\n",
       " 'арыстанбек',\n",
       " 'ас',\n",
       " 'асан',\n",
       " 'асар',\n",
       " 'асет',\n",
       " 'аскар',\n",
       " 'аскаров',\n",
       " 'аскарович',\n",
       " 'асла',\n",
       " 'аспект',\n",
       " 'аспирантур',\n",
       " 'ассамбл',\n",
       " 'ассамбле',\n",
       " 'ассоциац',\n",
       " 'аста',\n",
       " 'астан',\n",
       " 'астананың',\n",
       " 'астанинск',\n",
       " 'астык',\n",
       " 'ат',\n",
       " 'атамек',\n",
       " 'атинск',\n",
       " 'атлант',\n",
       " 'атмосфер',\n",
       " 'атмосферн',\n",
       " 'атомн',\n",
       " 'атташ',\n",
       " 'атфбанк',\n",
       " 'атыра',\n",
       " 'атыраумуна',\n",
       " 'атырауск',\n",
       " 'аудиоверс',\n",
       " 'аудит',\n",
       " 'аудиторск',\n",
       " 'аукцион',\n",
       " 'аурум',\n",
       " 'ауэз',\n",
       " 'ауэзов',\n",
       " 'ауэзовск',\n",
       " 'аф',\n",
       " 'афганиста',\n",
       " 'афганистан',\n",
       " 'афер',\n",
       " 'афк',\n",
       " 'африк',\n",
       " 'африканск',\n",
       " 'аффилирова',\n",
       " 'аханов',\n",
       " 'ахимов',\n",
       " 'ахметбек',\n",
       " 'ахунгалиевич',\n",
       " 'ашимба',\n",
       " 'ашимов',\n",
       " 'аэропорт',\n",
       " 'аэрофлот',\n",
       " 'аэс',\n",
       " 'аягузск',\n",
       " 'аязба',\n",
       " 'бабушк',\n",
       " 'багаж',\n",
       " 'бад',\n",
       " 'баз',\n",
       " 'базарбек',\n",
       " 'базархан',\n",
       " 'базов',\n",
       " 'байгазин',\n",
       " 'баймаханов',\n",
       " 'баймолдин',\n",
       " 'байсеит',\n",
       " 'байсеитов',\n",
       " 'байтерек',\n",
       " 'байтурсынов',\n",
       " 'байшагир',\n",
       " 'бакад',\n",
       " 'бакауов',\n",
       " 'бакиев',\n",
       " 'бакиш',\n",
       " 'бакытжа',\n",
       " 'бал',\n",
       " 'балабек',\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_head = train_texts[:100]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts_head)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How you see many useless words (numbers, english words). Let's use filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)Filter by min_df. When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10',\n",
       " '12',\n",
       " '14',\n",
       " '20',\n",
       " '2015',\n",
       " '2016',\n",
       " '2017',\n",
       " '30',\n",
       " 'kz',\n",
       " 'алмат',\n",
       " 'аста',\n",
       " 'банк',\n",
       " 'больш',\n",
       " 'будут',\n",
       " 'вид',\n",
       " 'возможн',\n",
       " 'вопрос',\n",
       " 'врем',\n",
       " 'глав',\n",
       " 'год',\n",
       " 'государств',\n",
       " 'государствен',\n",
       " 'дан',\n",
       " 'декабр',\n",
       " 'дел',\n",
       " 'ден',\n",
       " 'деятельн',\n",
       " 'директор',\n",
       " 'должн',\n",
       " 'друг',\n",
       " 'един',\n",
       " 'занима',\n",
       " 'информац',\n",
       " 'итог',\n",
       " 'казахста',\n",
       " 'казахстан',\n",
       " 'казахстанск',\n",
       " 'компан',\n",
       " 'котор',\n",
       " 'кром',\n",
       " 'крупн',\n",
       " 'лет',\n",
       " 'лиц',\n",
       " 'международн',\n",
       " 'мер',\n",
       " 'мест',\n",
       " 'месяц',\n",
       " 'министерств',\n",
       " 'министр',\n",
       " 'млн',\n",
       " 'млрд',\n",
       " 'направлен',\n",
       " 'населен',\n",
       " 'наход',\n",
       " 'национальн',\n",
       " 'нача',\n",
       " 'наш',\n",
       " 'необходим',\n",
       " 'нов',\n",
       " 'обеспечен',\n",
       " 'област',\n",
       " 'общ',\n",
       " 'обь',\n",
       " 'одн',\n",
       " 'отмет',\n",
       " 'перв',\n",
       " 'переда',\n",
       " 'период',\n",
       " 'планир',\n",
       " 'получ',\n",
       " 'порядк',\n",
       " 'правительств',\n",
       " 'предприят',\n",
       " 'председател',\n",
       " 'представител',\n",
       " 'пресс',\n",
       " 'провод',\n",
       " 'программ',\n",
       " 'проект',\n",
       " 'производств',\n",
       " 'прошл',\n",
       " 'работ',\n",
       " 'работа',\n",
       " 'развит',\n",
       " 'размер',\n",
       " 'рамк',\n",
       " 'ран',\n",
       " 'реализац',\n",
       " 'результат',\n",
       " 'республик',\n",
       " 'решен',\n",
       " 'рк',\n",
       " 'рынк',\n",
       " 'сайт',\n",
       " 'сам',\n",
       " 'сво',\n",
       " 'связ',\n",
       " 'сегодн',\n",
       " 'сегодняшн',\n",
       " 'ситуац',\n",
       " 'сказа',\n",
       " 'след',\n",
       " 'слов',\n",
       " 'служб',\n",
       " 'совет',\n",
       " 'созда',\n",
       " 'сообщ',\n",
       " 'сообща',\n",
       " 'сообщен',\n",
       " 'состав',\n",
       " 'социальн',\n",
       " 'сред',\n",
       " 'средств',\n",
       " 'стал',\n",
       " 'стран',\n",
       " 'сфер',\n",
       " 'счет',\n",
       " 'так',\n",
       " 'такж',\n",
       " 'тенг',\n",
       " 'управлен',\n",
       " 'уровн',\n",
       " 'услов',\n",
       " 'участ',\n",
       " 'финансов',\n",
       " 'фонд',\n",
       " 'цел',\n",
       " 'цен',\n",
       " 'центр',\n",
       " 'част',\n",
       " 'человек',\n",
       " 'числ',\n",
       " 'экономик',\n",
       " 'эт',\n",
       " 'явля',\n",
       " 'январ']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 0.2) #filter all word which appear less than 20% of documents\n",
    "X = vectorizer.fit_transform(texts_head)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)Filter by max_df. When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10',\n",
       " '12',\n",
       " '14',\n",
       " '20',\n",
       " '2015',\n",
       " '2016',\n",
       " '2017',\n",
       " '30',\n",
       " 'kz',\n",
       " 'алмат',\n",
       " 'аста',\n",
       " 'банк',\n",
       " 'больш',\n",
       " 'будут',\n",
       " 'вид',\n",
       " 'возможн',\n",
       " 'вопрос',\n",
       " 'врем',\n",
       " 'глав',\n",
       " 'год',\n",
       " 'государств',\n",
       " 'государствен',\n",
       " 'дан',\n",
       " 'декабр',\n",
       " 'дел',\n",
       " 'ден',\n",
       " 'деятельн',\n",
       " 'директор',\n",
       " 'должн',\n",
       " 'друг',\n",
       " 'един',\n",
       " 'занима',\n",
       " 'информац',\n",
       " 'итог',\n",
       " 'казахста',\n",
       " 'казахстан',\n",
       " 'казахстанск',\n",
       " 'компан',\n",
       " 'котор',\n",
       " 'кром',\n",
       " 'крупн',\n",
       " 'лет',\n",
       " 'лиц',\n",
       " 'международн',\n",
       " 'мер',\n",
       " 'мест',\n",
       " 'месяц',\n",
       " 'министерств',\n",
       " 'министр',\n",
       " 'млн',\n",
       " 'млрд',\n",
       " 'направлен',\n",
       " 'населен',\n",
       " 'наход',\n",
       " 'национальн',\n",
       " 'нача',\n",
       " 'наш',\n",
       " 'необходим',\n",
       " 'нов',\n",
       " 'обеспечен',\n",
       " 'област',\n",
       " 'общ',\n",
       " 'обь',\n",
       " 'одн',\n",
       " 'отмет',\n",
       " 'перв',\n",
       " 'переда',\n",
       " 'период',\n",
       " 'планир',\n",
       " 'получ',\n",
       " 'порядк',\n",
       " 'правительств',\n",
       " 'предприят',\n",
       " 'председател',\n",
       " 'представител',\n",
       " 'пресс',\n",
       " 'провод',\n",
       " 'программ',\n",
       " 'проект',\n",
       " 'производств',\n",
       " 'прошл',\n",
       " 'работ',\n",
       " 'работа',\n",
       " 'развит',\n",
       " 'размер',\n",
       " 'рамк',\n",
       " 'ран',\n",
       " 'реализац',\n",
       " 'результат',\n",
       " 'республик',\n",
       " 'решен',\n",
       " 'рк',\n",
       " 'рынк',\n",
       " 'сайт',\n",
       " 'сам',\n",
       " 'сво',\n",
       " 'связ',\n",
       " 'сегодн',\n",
       " 'сегодняшн',\n",
       " 'ситуац',\n",
       " 'сказа',\n",
       " 'след',\n",
       " 'слов',\n",
       " 'служб',\n",
       " 'совет',\n",
       " 'созда',\n",
       " 'сообщ',\n",
       " 'сообща',\n",
       " 'сообщен',\n",
       " 'состав',\n",
       " 'социальн',\n",
       " 'сред',\n",
       " 'средств',\n",
       " 'стал',\n",
       " 'стран',\n",
       " 'сфер',\n",
       " 'счет',\n",
       " 'так',\n",
       " 'такж',\n",
       " 'тенг',\n",
       " 'управлен',\n",
       " 'уровн',\n",
       " 'услов',\n",
       " 'участ',\n",
       " 'финансов',\n",
       " 'фонд',\n",
       " 'цел',\n",
       " 'цен',\n",
       " 'центр',\n",
       " 'част',\n",
       " 'человек',\n",
       " 'числ',\n",
       " 'экономик',\n",
       " 'эт',\n",
       " 'явля',\n",
       " 'январ']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 0.2, max_df = 0.95) #filter all word which appear less than 20% and great than 95% of documents\n",
    "X = vectorizer.fit_transform(texts_head)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)max_features build a vocabulary that only consider the top max_features ordered by term frequency across the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2016',\n",
       " '2017',\n",
       " 'вопрос',\n",
       " 'год',\n",
       " 'казахста',\n",
       " 'казахстанск',\n",
       " 'компан',\n",
       " 'котор',\n",
       " 'лет',\n",
       " 'назад',\n",
       " 'национальн',\n",
       " 'необходим',\n",
       " 'нов',\n",
       " 'област',\n",
       " 'работ',\n",
       " 'развит',\n",
       " 'район',\n",
       " 'республик',\n",
       " 'рк',\n",
       " 'стран',\n",
       " 'суд',\n",
       " 'такж',\n",
       " 'тенг',\n",
       " 'трудов',\n",
       " 'эт']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features = 25) #only top-25 words ordered by tf\n",
    "X = vectorizer.fit_transform(texts_head)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to get the column number associated with the token, you can use mapping from vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'котор': 7,\n",
       " 'лет': 8,\n",
       " 'год': 3,\n",
       " 'такж': 21,\n",
       " 'эт': 24,\n",
       " 'тенг': 22,\n",
       " 'казахстанск': 5,\n",
       " 'компан': 6,\n",
       " '2016': 0,\n",
       " 'вопрос': 2,\n",
       " 'рк': 18,\n",
       " 'област': 13,\n",
       " 'республик': 17,\n",
       " 'казахста': 4,\n",
       " 'развит': 15,\n",
       " 'работ': 14,\n",
       " 'стран': 19,\n",
       " 'нов': 12,\n",
       " '2017': 1,\n",
       " 'национальн': 10,\n",
       " 'необходим': 11,\n",
       " 'район': 16,\n",
       " 'суд': 20,\n",
       " 'назад': 9,\n",
       " 'трудов': 23}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case we will use simple max_features filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features = 40000)\n",
    "X = vectorizer.fit_transform(train_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is matrix with shape = (number of sentences $\\times$ number of words). Each coordinate is tf-idf for corresponding words. If in coordinate will be count of word in sentence this is call \"Bag-of-words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5536, 40000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit log-reg model, but 40000 is too large for model. Use LSA to reduce dimension. In sklearn LCA is TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41 s, sys: 3.63 s, total: 44.6 s\n",
      "Wall time: 33.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_small = svd.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5536, 1000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_small, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(X_small)\n",
    "proba = model.predict_proba(X_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY = 0.8876445086705202\n",
      "ROC-AUC = 0.9494297265485065\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "print(\"ACCURACY = {}\".format(accuracy_score(train_y, predict)))\n",
    "print(\"ROC-AUC = {}\".format(roc_auc_score(train_y, proba[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = vectorizer.transform(test_texts).toarray()\n",
    "test_X = svd.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY = 0.8720205353868721\n",
      "ROC-AUC = 0.9200978856475028\n"
     ]
    }
   ],
   "source": [
    "predict = model.predict(test_X)\n",
    "proba = model.predict_proba(test_X)\n",
    "print(\"ACCURACY = {}\".format(accuracy_score(test_y, predict)))\n",
    "print(\"ROC-AUC = {}\".format(roc_auc_score(test_y, proba[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use any models and algorithms that were told at a lecture or a seminar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/train.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>category_id</th>\n",
       "      <th>city</th>\n",
       "      <th>date_created</th>\n",
       "      <th>delivery_available</th>\n",
       "      <th>desc_text</th>\n",
       "      <th>img_num</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>name_text</th>\n",
       "      <th>owner_id</th>\n",
       "      <th>payment_available</th>\n",
       "      <th>price</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_type</th>\n",
       "      <th>properties</th>\n",
       "      <th>region</th>\n",
       "      <th>sold_mode</th>\n",
       "      <th>subcategory_id</th>\n",
       "      <th>sold_fast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Краснодар</td>\n",
       "      <td>2018-10-08</td>\n",
       "      <td>False</td>\n",
       "      <td>Продаю стол раскладной, деревянный, советский ...</td>\n",
       "      <td>3</td>\n",
       "      <td>45.0686</td>\n",
       "      <td>38.9518</td>\n",
       "      <td>Стол</td>\n",
       "      <td>4ce583fe8231a0cc4a3c7d241c7d0289</td>\n",
       "      <td>True</td>\n",
       "      <td>500.0</td>\n",
       "      <td>8cb80c05c65c210275f5500779d6b593</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'slug_id': 'stoly_stulya_tip', 'slug_name': ...</td>\n",
       "      <td>Краснодарский край</td>\n",
       "      <td>1</td>\n",
       "      <td>410</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Тюмень</td>\n",
       "      <td>2018-06-18</td>\n",
       "      <td>False</td>\n",
       "      <td>Тарелки глубокие 6 шт. Блюдца, чашки по 6 шт. ...</td>\n",
       "      <td>2</td>\n",
       "      <td>57.1840</td>\n",
       "      <td>65.5674</td>\n",
       "      <td>Посуда</td>\n",
       "      <td>e58be2c8f143c17246dc2243b5d3b98f</td>\n",
       "      <td>False</td>\n",
       "      <td>300.0</td>\n",
       "      <td>3b7a9f8b27a53b63525f95bc8070abb2</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'slug_id': 'dom_dacha_posuda_tip', 'slug_nam...</td>\n",
       "      <td>Тюменская область</td>\n",
       "      <td>1</td>\n",
       "      <td>405</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>Омск</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>True</td>\n",
       "      <td>Новый,с этикеткой. Размер L. Не подошёл по раз...</td>\n",
       "      <td>1</td>\n",
       "      <td>54.9889</td>\n",
       "      <td>73.4312</td>\n",
       "      <td>Костюм</td>\n",
       "      <td>51b408796027214232532b7e478e2159</td>\n",
       "      <td>True</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>c97dd9c5a3e938c52cf5d7822bc0eb7b</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'slug_id': 'zhenskaya_odezhda_pidzhaki_kosty...</td>\n",
       "      <td>Омская область</td>\n",
       "      <td>1</td>\n",
       "      <td>908</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>Санкт-Петербург</td>\n",
       "      <td>2018-04-17</td>\n",
       "      <td>False</td>\n",
       "      <td>Складывается тростью, все колеса вниз. Сплошна...</td>\n",
       "      <td>4</td>\n",
       "      <td>59.9590</td>\n",
       "      <td>30.4877</td>\n",
       "      <td>Коляска</td>\n",
       "      <td>6544b83acbbf04439a7ba983093cafb4</td>\n",
       "      <td>True</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>3e5d0286b25fd7f62f88bc436a59ae4e</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'slug_id': 'waggon_type', 'slug_name': 'Тип'...</td>\n",
       "      <td>Ленинградская область</td>\n",
       "      <td>1</td>\n",
       "      <td>312</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>Москва</td>\n",
       "      <td>2018-02-09</td>\n",
       "      <td>False</td>\n",
       "      <td>Неразлучники, птичкам по 1,5 года. Продаю с бо...</td>\n",
       "      <td>2</td>\n",
       "      <td>55.6473</td>\n",
       "      <td>37.4118</td>\n",
       "      <td>Волнистые попугаи</td>\n",
       "      <td>ea575e28daf1f47bfce63015cd3ce5cf</td>\n",
       "      <td>True</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>57b4a8679d0d3eb1e31367b57221098f</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>Московская область</td>\n",
       "      <td>1</td>\n",
       "      <td>504</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  category_id             city date_created  delivery_available  \\\n",
       "0           1            4        Краснодар   2018-10-08               False   \n",
       "1           2            4           Тюмень   2018-06-18               False   \n",
       "2           4            9             Омск   2018-07-31                True   \n",
       "3           6            3  Санкт-Петербург   2018-04-17               False   \n",
       "4          10            5           Москва   2018-02-09               False   \n",
       "\n",
       "                                           desc_text  img_num      lat  \\\n",
       "0  Продаю стол раскладной, деревянный, советский ...        3  45.0686   \n",
       "1  Тарелки глубокие 6 шт. Блюдца, чашки по 6 шт. ...        2  57.1840   \n",
       "2  Новый,с этикеткой. Размер L. Не подошёл по раз...        1  54.9889   \n",
       "3  Складывается тростью, все колеса вниз. Сплошна...        4  59.9590   \n",
       "4  Неразлучники, птичкам по 1,5 года. Продаю с бо...        2  55.6473   \n",
       "\n",
       "      long          name_text                          owner_id  \\\n",
       "0  38.9518               Стол  4ce583fe8231a0cc4a3c7d241c7d0289   \n",
       "1  65.5674             Посуда  e58be2c8f143c17246dc2243b5d3b98f   \n",
       "2  73.4312             Костюм  51b408796027214232532b7e478e2159   \n",
       "3  30.4877            Коляска  6544b83acbbf04439a7ba983093cafb4   \n",
       "4  37.4118  Волнистые попугаи  ea575e28daf1f47bfce63015cd3ce5cf   \n",
       "\n",
       "   payment_available   price                        product_id  product_type  \\\n",
       "0               True   500.0  8cb80c05c65c210275f5500779d6b593             1   \n",
       "1              False   300.0  3b7a9f8b27a53b63525f95bc8070abb2             1   \n",
       "2               True  1100.0  c97dd9c5a3e938c52cf5d7822bc0eb7b             1   \n",
       "3               True  5000.0  3e5d0286b25fd7f62f88bc436a59ae4e             1   \n",
       "4               True  2000.0  57b4a8679d0d3eb1e31367b57221098f             1   \n",
       "\n",
       "                                          properties                 region  \\\n",
       "0  [{'slug_id': 'stoly_stulya_tip', 'slug_name': ...     Краснодарский край   \n",
       "1  [{'slug_id': 'dom_dacha_posuda_tip', 'slug_nam...      Тюменская область   \n",
       "2  [{'slug_id': 'zhenskaya_odezhda_pidzhaki_kosty...         Омская область   \n",
       "3  [{'slug_id': 'waggon_type', 'slug_name': 'Тип'...  Ленинградская область   \n",
       "4                                                 []     Московская область   \n",
       "\n",
       "   sold_mode  subcategory_id  sold_fast  \n",
       "0          1             410          1  \n",
       "1          1             405          0  \n",
       "2          1             908          0  \n",
       "3          1             312          0  \n",
       "4          1             504          0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "def target_encoding(features, targets):\n",
    "    values = defaultdict(int)\n",
    "    counts = Counter()\n",
    "    for val, target in zip(features, targets):\n",
    "        values[val] += target\n",
    "        counts[val] += 1\n",
    "    \n",
    "    mean_values = dict()\n",
    "    for val in values:\n",
    "        mean_values[val] = values[val] / counts[val]\n",
    "    \n",
    "    return mean_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cat_features(data, cat_features, target):\n",
    "    cat_features_dict = dict()\n",
    "    for feature in cat_features:\n",
    "        cat_features_dict[feature] = target_encoding(data[feature].values, data[target].values)\n",
    "    return cat_features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use real features\n",
    "X = data[['lat', 'long', 'price']].values\n",
    "y = data['sold_fast']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['category_id', 'city', 'delivery_available', 'img_num', 'region']\n",
    "cat_features_dict = preprocess_cat_features(data, cat_features, 'sold_fast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in cat_features:\n",
    "    res = [0] * len(data)\n",
    "    for i, val in enumerate(data[feature].values):\n",
    "        res[i] = cat_features_dict[feature][val]\n",
    "    X = np.c_[X, np.array(res)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  We dont'use text data, because don't have enough time on seminar for preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always shuffle your data and don't forget fix random_seed and random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = shuffle(X, y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test_data and save file for submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/test_nolabel.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['lat', 'long', 'price']].values\n",
    "for feature in cat_features:\n",
    "    res = [0] * len(data)\n",
    "    for i, val in enumerate(data[feature].values):\n",
    "        res[i] = cat_features_dict[feature].get(val, 0)\n",
    "    X = np.c_[X, np.array(res)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_id = data['product_id'].values\n",
    "data = pd.DataFrame.from_dict({'product_id' : product_id, 'score' : proba[:, 1]})\n",
    "data.to_csv('./to_submit', sep = ',', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
