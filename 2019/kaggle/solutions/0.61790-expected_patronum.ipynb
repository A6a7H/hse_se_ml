{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle\n",
    "##### https://www.kaggle.com/c/postsold-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('./data/train.tsv', sep = '\\t')\n",
    "preprocessed_data = raw_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_columns = [raw_data.columns[0], 'date_created', 'name_text', 'owner_id', 'product_id', 'lat', 'long']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature = 'sold_fast'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_features = ['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['category_id', 'img_num', 'city'\n",
    "                'product_type', 'region', 'sold_mode', 'subcategory_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = ['desc_text', 'properties']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = ['date_created']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_features = ['delivery_available', 'payment_available']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_encode = ['region', 'category_id', 'img_num', 'city', 'subcategory_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swapColumns():\n",
    "    cols = list(data.columns[data.columns != target_feature])\n",
    "    if target_feature in data:\n",
    "        cols.append(target_feature)\n",
    "    return data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resetData():   \n",
    "    global data\n",
    "    data = preprocessed_data.drop(useless_columns, axis = 1)\n",
    "#     data = data.drop('properties', axis = 1)\n",
    "    data = data.drop(text_features, axis = 1)\n",
    "    data = swapColumns()\n",
    "    \n",
    "resetData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessRealFeatures():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessCatFeatures():\n",
    "    preprocessCategoryID()\n",
    "    preprocessProductType()\n",
    "    for feature in features_to_encode:\n",
    "        encodeCatFeature(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessCategoryID():\n",
    "    col = preprocessed_data['category_id'].values\n",
    "    col[col == 100] = 0 \n",
    "    \n",
    "def preprocessProductType():\n",
    "    col = preprocessed_data['product_type'].values\n",
    "    col[col != 1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# def createDictionary(feature):\n",
    "#     col = raw_data[feature].values\n",
    "#     labels = raw_data[target_feature].values\n",
    "#     cat_dict = defaultdict(int)\n",
    "#     for aval in np.unique(col):\n",
    "#         labels_for_val = labels[col == aval]\n",
    "#         n = len(labels_for_val)\n",
    "#         n_1 = len(labels_for_val[labels_for_val == 1])\n",
    "#         p = 1.0 * n_1 / n\n",
    "#         cat_dict[aval] = p\n",
    "#     return cat_dict\n",
    "\n",
    "def createDictionary(feature):\n",
    "    values = defaultdict(int)\n",
    "    counts = Counter()\n",
    "    col = preprocessed_data[feature].values\n",
    "    target = preprocessed_data[target_feature].values\n",
    "    \n",
    "\n",
    "    X_train, _, y_train, _ = train_test_split(col, target, \n",
    "                                                    test_size=0.5,    \n",
    "                                                    random_state=4010)\n",
    "\n",
    "    avg = len(target[target == 1]) / len(target)\n",
    "     \n",
    "    for i in range(len(X_train)):\n",
    "        values[X_train[i]] += y_train[i]\n",
    "        counts[X_train[i]] += 1\n",
    "    \n",
    "    \n",
    "    mean_values = dict()\n",
    "    for val in values:\n",
    "        if(counts[val] > 10):\n",
    "            mean_values[val] = values[val] / counts[val]\n",
    "        else:\n",
    "            mean_values[val] = avg\n",
    "    return mean_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = data.columns[data.columns != target_feature]\n",
    "X = data[X_cols].values\n",
    "y = data[target_feature].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = dict()\n",
    "for feature in features_to_encode:\n",
    "    feature_dict = createDictionary(feature)\n",
    "    dicts[feature] = feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encodeCatFeature(feature): \n",
    "#     cat_dict = dicts[feature]\n",
    "#     col = preprocessed_data[feature].values\n",
    "    \n",
    "#     for i in range(len(col)):\n",
    "#         aval = col[i]\n",
    "#         if aval in cat_dict:\n",
    "#             col[i] = cat_dict[aval] * 100\n",
    "#         else:\n",
    "#             col[i] = 0.0\n",
    "\n",
    "def encodeCatFeature(feature):         \n",
    "    mean_values = dicts[feature]\n",
    "    col = preprocessed_data[feature].values\n",
    "    \n",
    "    minKey = min(mean_values, key = mean_values.get)\n",
    "    minVal = mean_values[minKey]\n",
    "    maxKey = max(mean_values, key = mean_values.get)\n",
    "    maxVal = mean_values[maxKey]\n",
    "    \n",
    "    for val in mean_values:\n",
    "        normVal = (mean_values[val] - minVal) / (maxVal - minVal)\n",
    "        mean_values[val] = round(normVal / 0.01)\n",
    "        \n",
    "    for i in range(len(col)):\n",
    "        if(col[i] in mean_values.keys()):\n",
    "            col[i] = mean_values[col[i]]\n",
    "        else:\n",
    "            col[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "\n",
    "word_tokenizer = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('russian')\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "    \n",
    "def _processText(data):\n",
    "    texts = []\n",
    "    for item in data['desc_text']:\n",
    "        tokens = word_tokenizer.tokenize(item.lower())\n",
    "        tokens = [word for word in tokens if (word not in string.punctuation and word not in stop_words)]\n",
    "        texts.append(tokens)\n",
    "    return texts\n",
    "\n",
    "def preprocessTextFeatures():\n",
    "    texts = _processText(raw_data)\n",
    "        \n",
    "    for i in tqdm_notebook(range(len(texts))):\n",
    "        texts[i] = ' '.join(list(map(stemmer.stem, texts[i])))\n",
    "        \n",
    "    texts_head = texts[:100]\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     X = vectorizer.fit_transform(texts_head)\n",
    "#     vectorizer.get_feature_names()\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(min_df = 0.2) #filter all word which appear less than 20% of documents\n",
    "    X = vectorizer.fit_transform(texts_head)\n",
    "    vectorizer.get_feature_names()\n",
    "    \n",
    "#     vectorizer = TfidfVectorizer(min_df = 0.2, max_df = 0.95) #filter all word which appear less than 20% and great than 95% of documents\n",
    "#     X = vectorizer.fit_transform(texts_head)\n",
    "#     vectorizer.get_feature_names()\n",
    "\n",
    "#     vectorizer = TfidfVectorizer(max_features = 25) #only top-25 words ordered by tf\n",
    "#     X = vectorizer.fit_transform(texts_head)\n",
    "#     vectorizer.get_feature_names()\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features = 40000)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "        \n",
    "    svd = TruncatedSVD(n_components = 1000)\n",
    "    X_small = svd.fit_transform(X)\n",
    "#     preprocessed_data['desc_text'] = _processText(raw_data)\n",
    "    return X_small\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def preprocessDataFeatures():\n",
    "    col = raw_data['date_created'].values\n",
    "    years = []\n",
    "    monthes = []\n",
    "    days = []\n",
    "    isWeekend = []\n",
    "    \n",
    "    dates = list(col)\n",
    "    for i in range(len(dates)):\n",
    "        date = dates[i]\n",
    "        date_splitted = date.split('-')\n",
    "        \n",
    "        month = int(date_splitted[1])\n",
    "        monthes.append(month)\n",
    "        day = int(date_splitted[2])\n",
    "        days.append(day)\n",
    "    \n",
    "        dayNum = datetime.datetime.strptime(date, '%Y-%m-%d').weekday()\n",
    "        isWeekend.append(int(dayNum >= 4))\n",
    "        \n",
    "    preprocessed_data['month'] = monthes\n",
    "    preprocessed_data['day'] = days\n",
    "    preprocessed_data['is_weekend'] = isWeekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessBooleanFeatures():\n",
    "    for feature in bool_features:\n",
    "        preprocessed_data[feature] = preprocessed_data[feature].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    preprocessRealFeatures()\n",
    "    preprocessCatFeatures()\n",
    "#     preprocessTextFeatures()\n",
    "    preprocessDataFeatures()\n",
    "    preprocessBooleanFeatures()\n",
    "    \n",
    "preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resetData()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe_category_id = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe_region_id = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe_img_num_id = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "ohe_category_id.fit(data['category_id'].values.reshape(-1, 1))\n",
    "ohe_region_id.fit(data['region'].values.reshape(-1, 1))\n",
    "ohe_img_num_id.fit(data['img_num'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_cols = ['lat', 'long', 'category_id']\n",
    "extra_data = raw_data[extra_cols].values\n",
    "\n",
    "category_id = ohe_category_id.transform(raw_data['category_id'].values.reshape(-1, 1)).toarray()\n",
    "region = ohe_category_id.transform(raw_data['region'].values.reshape(-1, 1)).toarray()\n",
    "img_num = ohe_category_id.transform(raw_data['img_num'].values.reshape(-1, 1)).toarray()\n",
    "\n",
    "ignore_features = [target_feature]\n",
    "X_cols = []\n",
    "for col in data.columns:\n",
    "    if (not col in ignore_features):\n",
    "        X_cols.append(col)\n",
    "\n",
    "X = data[X_cols].values\n",
    "# X = np.concatenate(tuple([X, extra_data, category_id, region, img_num]), axis=1)\n",
    "X = np.concatenate(tuple([X]), axis=1)\n",
    "y = data[target_feature].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit-Predict\n",
    "\n",
    "Always shuffle your data and don't forget fix random_seed and random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.7,      # 20% for test, 80% for train\n",
    "                                                    random_state=72)  # shuffle objects before split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.DataFrame(data=X_train[0:,0:]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-ml approach\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# model = GaussianNB()\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier(n_estimators=800, learning_rate=0.03, max_depth=3, n_jobs=14, \n",
    "                        colsample_bytree=0.6, scale_pos_weight=3.33, silent=0)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Важность фич кэтбустом\n",
    "# from catboost import CatBoostClassifier, Pool, cv\n",
    "\n",
    "# X_train2, X_validation2, y_train2, y_validation2 = train_test_split(X_train, y_train, train_size=0.75, random_state=42)\n",
    "\n",
    "# params = {\n",
    "#     'iterations': 500,\n",
    "#     'learning_rate': 0.1,\n",
    "#     'eval_metric': 'auc',\n",
    "#     'random_seed': 42,\n",
    "#     'logging_level': 'Silent',\n",
    "#     'use_best_model': False,\n",
    "#     'task_type' : 'GPU'\n",
    "# }\n",
    "# train_pool = Pool(X_train2, y_train2)\n",
    "# validate_pool = Pool(X_validation2, y_validation2)\n",
    "\n",
    "# model = CatBoostClassifier(iterations=50, random_seed=42, logging_level='Silent').fit(train_pool)\n",
    "# feature_importances = model.get_feature_importance(train_pool)\n",
    "# feature_names = data.columns\n",
    "# for score, name in sorted(zip(feature_importances, feature_names), reverse=True):\n",
    "#     print('{}: {}'.format(name, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "train_proba = model.predict_proba(X_train)[:, 1]\n",
    "test_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auc_train = roc_auc_score(y_train, train_proba)\n",
    "auc_test = roc_auc_score(y_test, test_proba)\n",
    "\n",
    "print(\"Train AUC: \", auc_train)\n",
    "print(\"Test AUC:  \", auc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawFeatureHist(features):\n",
    "    # Define size of the figure\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    for feature in features:\n",
    "\n",
    "        # Create subplot for each column\n",
    "        plt.subplot(4, 3, features.index(feature)+1)\n",
    "\n",
    "        # Get column and label values\n",
    "        x_col = data[feature].values\n",
    "        y_col = data[target_feature].values\n",
    "        \n",
    "        # Plot histograms\n",
    "        bins = 25\n",
    "        plt.hist(x_col[y_col == 0], bins=bins, color = 'r', alpha=0.5, label='0')\n",
    "        plt.hist(x_col[y_col == 1], bins=bins, color = 'b', alpha=0.5, label='1')\n",
    "\n",
    "        # Labels and legend\n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel('Counts')\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# drawFeatureHist(list(data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('./data/test_nolabel.tsv', sep = '\\t')\n",
    "preprocessed_data = raw_data.copy()\n",
    "preprocess()\n",
    "resetData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predict = data.values\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = model.predict_proba(X_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save file for submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_id = raw_data['product_id'].values\n",
    "data = pd.DataFrame.from_dict({'product_id' : product_id, 'score' : proba[:, 1]})\n",
    "data.to_csv('./Submission/to_submit', sep = ',', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
