{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тут просто препроцессим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_data(data):\n",
    "    data.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "    data[\"delivery_available\"] = [1 if v is \"True\" else 0 for v in data[\"delivery_available\"]]\n",
    "    data[\"payment_available\"] = [1 if v is \"True\" else 0 for v in data[\"payment_available\"]]\n",
    "    data.sort_values(\"date_created\", inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preproc_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тут выделяем разные типы фич для того, чтобы напрямую потом по-разному их обрабатывать и доставать новые фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"img_num\", \"lat\", \"long\", \"price\"]\n",
    "binary_features = [\"delivery_available\", \"payment_available\"]\n",
    "categorical_features = [\"category_id\", \"city\", \"product_type\", \"region\", \"sold_mode\", \"subcategory_id\"]\n",
    "for_num_unique_features = [\"category_id\", \"city\", \"date_created\", \"owner_id\",\n",
    "                           \"product_id\", \"product_type\", \"region\", \"subcategory_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# эта функция для доставания разных статистих по числовым колонкам\n",
    "def get_all_stats(arr):\n",
    "    return [\n",
    "        np.min(arr),\n",
    "        np.max(arr),\n",
    "        np.mean(arr),\n",
    "        np.median(arr),\n",
    "        np.std(arr),\n",
    "        np.percentile(arr, 25),\n",
    "        np.percentile(arr, 75),\n",
    "        np.unique(arr).shape[0]\n",
    "    ]\n",
    "\n",
    "# эта по бинарным\n",
    "def get_stats_from_bin_column(arr):\n",
    "    return [\n",
    "        np.mean(arr),\n",
    "        np.sum(arr),\n",
    "        np.std(arr),\n",
    "        len(arr) - np.sum(arr)\n",
    "    ]\n",
    "\n",
    "# здесь достаются фичи по дате\n",
    "def get_date_feautures(date):\n",
    "    date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    \n",
    "    return [\n",
    "        date.year,\n",
    "        date.month,\n",
    "        date.day,\n",
    "        date.weekday(),\n",
    "        1 if date.weekday() in (5, 6) else 0,\n",
    "        date.isocalendar()[1]\n",
    "    ]\n",
    "\n",
    "# а тут комплексная функция, как мы обрабатываем категориальные колонки\n",
    "def get_categorical_features(data_subset):\n",
    "    data_for_extracting_features = data_subset\n",
    "    \n",
    "    data_last_day = datetime.strptime(data_for_extracting_features[\"date_created\"].values[-1], \"%Y-%m-%d\")\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # для каждого значения каждой категориальной фичи считаем разные фичи по числовым и бинарным колонкам\n",
    "    for cat_feature in categorical_features:\n",
    "        features[cat_feature] = {}\n",
    "        \n",
    "        for current_cat_feature_value, current_cat_feature_value_subset in data_for_extracting_features \\\n",
    "                                                                                .groupby(cat_feature):\n",
    "            current_feauture_stats = []\n",
    "\n",
    "            # по числовым достаём из соответствующей функции\n",
    "            for num_feature in numeric_features:\n",
    "                current_feauture_stats += get_all_stats(current_cat_feature_value_subset[num_feature])\n",
    "\n",
    "            # по бинарным тоже\n",
    "            for bin_feature in binary_featues:\n",
    "                current_feauture_stats += get_stats_from_bin_column(current_cat_feature_value_subset[bin_feature])\n",
    "            \n",
    "            # ну и ещё по другим категориальным считаем количество уникальных\n",
    "            for nu_feature in for_num_unique_features:\n",
    "                current_feauture_stats += [current_cat_feature_value_subset[nu_feature].unique().shape[0]]\n",
    "\n",
    "            # + долю конкретного значения в фиче и общее количество\n",
    "            current_feauture_stats += [current_cat_feature_value_subset.shape[0] / data_subset.shape[0]]\n",
    "            current_feauture_stats += [current_cat_feature_value_subset.shape[0]]\n",
    "            \n",
    "            # + сколько это значение существует и когда в последний раз использовалось\n",
    "            first_date = datetime.strptime(current_cat_feature_value_subset[\"date_created\"].values[0], \"%Y-%m-%d\")\n",
    "            last_date = datetime.strptime(current_cat_feature_value_subset[\"date_created\"].values[0], \"%Y-%m-%d\")\n",
    "            current_feauture_stats += [(last_date - first_date).days, (data_last_day - last_date).days]\n",
    "\n",
    "            # ну и этот большой вектор и будет оцифровкой категориальной фичи - добавим его как отдельные колонки\n",
    "            features[cat_feature][current_cat_feature_value] = current_feauture_stats\n",
    "            features[cat_feature][current_cat_feature_value] += [data_for_extracting_features[cat_feature]\\\n",
    "                                                                     .unique().shape[0]]\n",
    "        \n",
    "    return features, len(features[cat_feature][current_cat_feature_value])\n",
    "\n",
    "# эта функция чтобы достать tf-idf из текстовых колонок - названия и описания\n",
    "def get_tf_idfs(data_subset):\n",
    "    name_text = data_subset['name_text'].values\n",
    "    vectorizer_name_text = TfidfVectorizer(max_features=100, decode_error='ignore')\n",
    "    vectorizer_name_text.fit(name_text)\n",
    "\n",
    "    desc_text = data_subset['desc_text'].values\n",
    "    vectorizer_desc_text = TfidfVectorizer(max_features=100, decode_error='ignore')\n",
    "    vectorizer_desc_text.fit(desc_text)\n",
    "    \n",
    "    return vectorizer_name_text, vectorizer_desc_text\n",
    "\n",
    "# а тут просто всё соединияем из того, что выше накатали\n",
    "def get_all_features_from_row(row, calc_categorical_features, len_features,\n",
    "                              vectorizer_name_text, vectorizer_desc_text):\n",
    "    row_features = []\n",
    "\n",
    "    for cat_feature in categorical_features:\n",
    "        cat_value = row[cat_feature]\n",
    "        try:\n",
    "            row_features += calc_categorical_features[cat_feature][cat_value]\n",
    "        except:\n",
    "            row_features += np.ones(len_features).tolist()\n",
    "\n",
    "        for num_feature in numeric_features:\n",
    "            row_features += [row[num_feature]]\n",
    "\n",
    "        for bin_feature in binary_featues:\n",
    "            row_features += [row[bin_feature]]\n",
    "\n",
    "        row_features += get_date_feautures(row[\"date_created\"])\n",
    "        \n",
    "        row_features += vectorizer_name_text.transform([row[\"name_text\"]]).toarray()[0].tolist()\n",
    "        row_features += vectorizer_desc_text.transform([row[\"desc_text\"]]).toarray()[0].tolist()\n",
    "\n",
    "    return row_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# для таких данных хорошо проверять точность на кросс-валидации по времени, поэтому поделим на фолды по 30к элементов (цифра из головы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folds = []\n",
    "current_fold_rows = []\n",
    "\n",
    "for index, row in tqdm_notebook(data.iterrows()):    \n",
    "    current_fold_rows.append(row)\n",
    "    \n",
    "    if index % 30000 == 0 and index != 0:\n",
    "        folds.append(pd.concat(current_fold_rows, axis=1).T)\n",
    "        current_fold_rows = []\n",
    "        \n",
    "folds.append(pd.concat(current_fold_rows, axis=1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## а теперь предлагается учиться на каждом префиксе по фолдам [:i], а проверять точность на i-ом фолде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_features_and_answers = []\n",
    "\n",
    "for i in tqdm_notebook(range(1, len(folds))):\n",
    "    y = folds[i][\"sold_fast\"]\n",
    "    x = folds[i].drop(columns=[\"sold_fast\"])\n",
    "    \n",
    "    historical_data = pd.concat(folds[:i])\n",
    "    \n",
    "    calc_categorical_features, len_features = get_categorical_features(historical_data)\n",
    "    vectorizer_name_text, vectorizer_desc_text = get_tf_idfs(historical_data)\n",
    "    \n",
    "    x_features = []\n",
    "    for index, row in x.iterrows():\n",
    "        x_features.append(get_all_features_from_row(row,\n",
    "                                                    calc_categorical_features,\n",
    "                                                    len_features,\n",
    "                                                    vectorizer_name_text,\n",
    "                                                    vectorizer_desc_text\n",
    "                                                   ))\n",
    "\n",
    "    fold_features_and_answers.append((np.array(x_features), np.array(y.values.tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### поверх этого делаем grid-search по параметрам lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "import lightgbm\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"boosting_type\": [\"dart\", \"gbdt\"],\n",
    "    \"num_leaves\": [31, 50, 100],\n",
    "    \"learning_rate\": [0.05, 0.1],\n",
    "    \"n_estimators\": [100, 500, 1000],\n",
    "    \"max_depth\": [None, 10, 50, 100]\n",
    "}\n",
    "\n",
    "params = list(ParameterGrid(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluation = []\n",
    "\n",
    "for param in tqdm_notebook(params):\n",
    "    scores = []\n",
    "    \n",
    "    for i in range(1, len(fold_features_and_answers)):\n",
    "        train_data = np.concatenate([fold[0] for fold in fold_features_and_answers[:i]])\n",
    "        train_ans = np.concatenate([fold[1] for fold in fold_features_and_answers[:i]])\n",
    "\n",
    "        current_model = lightgbm.LGBMClassifier(**param)\n",
    "        current_model.fit(train_data, train_ans)\n",
    "        prediction = current_model.predict_proba(fold_features_and_answers[i][0])\n",
    "        prediction = prediction[:, 1]\n",
    "\n",
    "        score = roc_auc_score(fold_features_and_answers[i][1], prediction)\n",
    "        scores.append(score)\n",
    "        \n",
    "    evaluation.append((np.mean(scores), param, current_model))\n",
    "    print(np.mean(scores), scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# теперь предсказываем ответ на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"test_nolabel.tsv\", sep='\\t')\n",
    "test_data = preproc_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_features = []\n",
    "\n",
    "# берём за исторические данные все данные из train\n",
    "historical_data = np.concatenate(folds)\n",
    "\n",
    "calc_categorical_features, len_features = get_categorical_features(historical_data)\n",
    "vectorizer_name_text, vectorizer_desc_text = get_tf_idfs(historical_data)\n",
    "\n",
    "for index, row in tqdm_notebook(test_data.iterrows()):\n",
    "    ans_features.append(get_all_features_from_row(row,\n",
    "                                                calc_categorical_features,\n",
    "                                                len_features,\n",
    "                                                vectorizer_name_text,\n",
    "                                                vectorizer_desc_text\n",
    "                                               ))\n",
    "    \n",
    "ans_features = np.array(ans_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# берём лучшую модель и предсказываем вероятности\n",
    "prediction = list(sorted(evaluation, key=lambda x: x[0]))[-1][2].predict_proba(ans_features)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = test_data[[\"product_id\"]]\n",
    "answer[\"score\"] = prediction\n",
    "answer.to_csv(\"answer.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
