{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import string\n",
    "import nltk\n",
    "import multiprocessing as mp\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2V embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://shestakoff.github.io/hse_se_ml/2020/l09-nlp-w2v/img/w2v_window.png\" width=\"480\">\n",
    "\n",
    "Ideally, we would like to have following probabilities (multi-class classification problem):\n",
    "\n",
    "$$p_{\\theta}(v|w) = \\frac{\\exp(in_w^T out_v)}{\\sum_{v'}\\exp(in_w^T out_{v'})}$$\n",
    "\n",
    "However, it's hard to compute the sum in the denominator. So, instead of multi-class problem, we solve binary classification problem (is there a context or not). We minimize\n",
    "\n",
    "$$L(w,v) = - \\log(\\sigma(in_w^T out_v)) - \\hat{\\sum_{v'}}\\log(\\sigma(-in_w^T out_{v'}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As on the previous seminar, we are going to use a <i>sentiment analysis</i> problem for demonstration, but with slightly different dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus, sentiment = pd.read_csv('IMDB Dataset.csv').values.T\n",
    "sentiment = (sentiment=='positive').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Of all the films I have seen, this one, The Rage, has got to be one of the worst yet. The direction, LOGIC, continuity, changes in plot-script and dialog made me cry out in pain. \"How could ANYONE come up with something so crappy\"? Gary Busey is know for his \"B\" movies, but this is a sure \"W\" movie. (W=waste).<br /><br />Take for example: about two dozen FBI & local law officers surround a trailer house with a jeep wagoneer. Inside the jeep is MA and is \"confused\" as to why all the cops are about. Within seconds a huge gun battle ensues, MA being killed straight off. The cops blast away at the jeep with gary and company blasting away at them. The cops fall like dominoes and the jeep with Gary drives around in circles and are not hit by one single bullet/pellet. MA is killed and gary seems to not to have noticed-damn that guy is tough. Truly a miracle, not since the six-shooter held 300 bullets has there been such a miracle.',\n",
       " 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_corpus[42], sentiment[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**we would like to drop stop-words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/matyushinleonid/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**preprocess: do lowercase, drop stop-words, do stemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer \n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = nltk.WordPunctTokenizer()\n",
    "def lowercase_dropstop_stemmatize(doc):\n",
    "    doc = [word.lower() for word in word_tokenizer.tokenize(doc) if \n",
    "           (word.lower() not in string.punctuation and word.lower() not in stop_words)\n",
    "          ]\n",
    "    text_stemmed = list(map(stemmer.stem, doc))\n",
    "    return ' '.join(text_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 105 ms, sys: 250 ms, total: 355 ms\n",
      "Wall time: 6.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with mp.Pool() as pool:\n",
    "    corpus = pool.map(lowercase_dropstop_stemmatize, raw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 28s, sys: 1.76 s, total: 3min 29s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path = get_tmpfile(\"word2vec.model\")\n",
    "model = Word2Vec([doc.split(' ') for doc in corpus], size=300, window=5, min_count=2, iter=10)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**let us compute most relevant contexts for the word \"VADER\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vector representation (first 50 components):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5729108 , -0.22881374,  0.50018376, -0.20409894, -0.55492777,\n",
       "       -0.35430026,  0.07701688,  0.25573292,  0.7422419 ,  0.20204526,\n",
       "       -0.32518914,  0.07587666,  0.821006  ,  0.4427495 ,  0.49637473,\n",
       "        0.2580907 , -0.02924393, -0.17726961, -0.15624647,  0.0852446 ,\n",
       "       -0.6651756 ,  0.08680069, -0.6850511 ,  0.09538114, -0.16225617,\n",
       "       -0.0288447 ,  0.17406179, -0.1850053 , -0.26299593,  0.11451057,\n",
       "        0.3836558 , -0.08444699, -0.05174693, -0.14174159,  0.38017547,\n",
       "        0.14349411, -0.3878912 ,  0.30732694,  0.00203037, -0.24798863,\n",
       "        0.34370682, -0.0265651 ,  0.6500566 , -0.08291614, -0.2405748 ,\n",
       "       -0.12190673,  0.7439231 ,  0.14104372,  0.3179491 ,  0.50621796],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['vader'][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most relevant vectors wrt cosine distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('darth', 0.8780304193496704),\n",
       " ('anakin', 0.7383174300193787),\n",
       " ('vadar', 0.7381731271743774),\n",
       " ('skywalk', 0.7120633125305176),\n",
       " ('leia', 0.630388617515564),\n",
       " ('palpatin', 0.6245428919792175),\n",
       " ('sith', 0.578973114490509),\n",
       " ('obi', 0.5750287771224976),\n",
       " ('jedi', 0.5672026872634888),\n",
       " ('annakin', 0.5638409852981567)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('vader')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model on the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_and_model_to_X(corpus, model):\n",
    "    X = []\n",
    "    for doc in corpus:\n",
    "        x = []\n",
    "        for word in doc.split(' '):\n",
    "            if word in model.wv.vocab.keys():\n",
    "                x.append(model.wv[word])\n",
    "        X.append(np.mean(x, axis=0))\n",
    "    X = np.stack(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.6 s, sys: 220 ms, total: 19.8 s\n",
      "Wall time: 17.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = corpus_and_model_to_X(corpus, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, sentiment, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9476257735520629"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_test, lr.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Trained W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained vectors trained on part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('greenback', 0.746670663356781),\n",
       " ('currency', 0.6809227466583252),\n",
       " ('Dollar', 0.6606971025466919),\n",
       " ('Japanese_Yen', 0.6504158973693848),\n",
       " ('loonie', 0.6266234517097473),\n",
       " ('Swiss_franc', 0.6245606541633606),\n",
       " ('rupee', 0.620018482208252),\n",
       " ('yen.The_Nikkei', 0.6100620031356812),\n",
       " ('euro', 0.6061668395996094),\n",
       " ('USdollar', 0.6032359600067139)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"dollar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Data', 0.7262316942214966),\n",
       " ('datasets', 0.603030264377594),\n",
       " ('dataset', 0.5796632170677185),\n",
       " ('databases', 0.5450121164321899),\n",
       " ('statistics', 0.537885844707489),\n",
       " ('information', 0.5368290543556213),\n",
       " ('database', 0.5325667262077332),\n",
       " ('Data_System_IPEDS', 0.5222617983818054),\n",
       " ('data.The', 0.5189103484153748),\n",
       " ('OpenSpirit_enabled', 0.5174090266227722)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Merkel', 0.7865056991577148),\n",
       " ('Putin', 0.7305814027786255),\n",
       " ('Medvedev', 0.7112522125244141),\n",
       " ('Tymoshenko', 0.6713743209838867),\n",
       " ('Lavrov', 0.6601910591125488),\n",
       " ('Yanukovych', 0.6371150016784668),\n",
       " ('Ms_Tymoshenko', 0.6273255348205566),\n",
       " ('Yanukovich', 0.6268482208251953),\n",
       " ('Kudrin', 0.625721275806427),\n",
       " ('Russia', 0.6246747374534607)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_vector(model.wv['Merkel'] - model.wv['Germany'] + model['Russia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.8449392318725586),\n",
       " ('queen', 0.7300517559051514),\n",
       " ('monarch', 0.6454660892486572),\n",
       " ('princess', 0.6156250834465027),\n",
       " ('crown_prince', 0.5818676352500916),\n",
       " ('prince', 0.577711820602417),\n",
       " ('kings', 0.5613664388656616),\n",
       " ('sultan', 0.5376776456832886),\n",
       " ('Queen_Consort', 0.5344247817993164),\n",
       " ('queens', 0.5289887189865112)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_vector(model.wv['king'] - model.wv['man'] + model['woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 0.7650878429412842),\n",
       " ('cats', 0.7223140001296997),\n",
       " ('dogs', 0.6266546249389648),\n",
       " ('puppies', 0.6123023629188538),\n",
       " ('felines', 0.5935230851173401),\n",
       " ('dog', 0.5880736708641052),\n",
       " ('kitties', 0.5775256752967834),\n",
       " ('kittens', 0.5743311047554016),\n",
       " ('chihuahuas', 0.5673359632492065),\n",
       " ('pup', 0.5663490295410156)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_vector(model.wv['apples'] - model.wv['apple'] + model.wv['cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.4 s, sys: 179 ms, total: 41.6 s\n",
      "Wall time: 39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = corpus_and_model_to_X(corpus, model)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, sentiment, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8948508827277051"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, lr.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "target_adaptation_venv",
   "language": "python",
   "name": "target_adaptation_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
